Using cuda device
Learnable Parameters for MNIST models:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
101632 	 406528 	 407050 	 669706
torch.Size([64, 1, 28, 28])
torch.Size([1, 28, 28])
Forward time for MNIST models:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
0.00010752677917480469 	 9.369850158691406e-05 	 0.00014448165893554688 	 0.00021719932556152344
Forward time for MNIST models FS Timer class:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
8.935809135437012e-05 	 9.00890827178955e-05 	 0.00014763259887695313 	 0.00021589136123657227
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                   model_inference mlp1         5.99%     139.000us        99.05%       2.300ms       2.300ms       0.000us         0.00%      19.000us      19.000us             1  
                                           aten::linear         0.86%      20.000us        86.09%       1.999ms     999.500us       0.000us         0.00%      17.000us       8.500us             2  
                                            aten::addmm        11.15%     259.000us        83.63%       1.942ms     971.000us      17.000us        89.47%      17.000us       8.500us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      15.000us        78.95%      15.000us       7.500us             2  
                                        aten::clamp_min         2.24%      52.000us         7.97%     185.000us      46.250us       2.000us        10.53%       4.000us       1.000us             4  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us        10.53%       2.000us       1.000us             2  
                                             aten::relu         1.55%      36.000us         6.24%     145.000us      72.500us       0.000us         0.00%       2.000us       1.000us             2  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us        10.53%       2.000us       1.000us             2  
                                            aten::zeros         0.30%       7.000us         0.69%      16.000us      16.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         0.95%      22.000us         0.95%      22.000us       5.500us       0.000us         0.00%       0.000us       0.000us             4  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.322ms
Self CUDA time total: 19.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                   model_inference mlp2         6.31%     159.000us        99.25%       2.502ms       2.502ms       0.000us         0.00%      24.000us      24.000us             1  
                                           aten::linear         1.03%      26.000us        84.93%       2.141ms     713.667us       0.000us         0.00%      22.000us       7.333us             3  
                                            aten::addmm        14.04%     354.000us        81.99%       2.067ms     689.000us      22.000us        91.67%      22.000us       7.333us             3  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      22.000us        91.67%      22.000us       7.333us             3  
                                        aten::clamp_min         2.70%      68.000us         9.44%     238.000us      39.667us       2.000us         8.33%       4.000us       0.667us             6  
                                             aten::relu         1.67%      42.000us         7.26%     183.000us      61.000us       0.000us         0.00%       2.000us       0.667us             3  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         8.33%       2.000us       0.667us             3  
                                            aten::zeros         0.32%       8.000us         0.56%      14.000us      14.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         1.11%      28.000us         1.11%      28.000us       5.600us       0.000us         0.00%       0.000us       0.000us             5  
                                            aten::zero_         0.04%       1.000us         0.04%       1.000us       1.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.521ms
Self CUDA time total: 24.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                 model_inference fsp128         5.23%     122.000us        98.80%       2.305ms       2.305ms       0.000us         0.00%       8.000us       8.000us             1  
                                           aten::linear         0.81%      19.000us        90.70%       2.116ms       1.058ms       0.000us         0.00%       7.000us       3.500us             2  
                                           aten::matmul         0.73%      17.000us        88.04%       2.054ms       1.027ms       0.000us         0.00%       7.000us       3.500us             2  
                                               aten::mm        10.24%     239.000us        87.31%       2.037ms       1.018ms       7.000us        87.50%       7.000us       3.500us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us       7.000us        87.50%       7.000us       3.500us             2  
                                              aten::cos         1.71%      40.000us         2.31%      54.000us      54.000us       1.000us        12.50%       1.000us       1.000us             1  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us        12.50%       1.000us       1.000us             1  
                                            aten::zeros         0.30%       7.000us         0.94%      22.000us      22.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         1.33%      31.000us         1.33%      31.000us       7.750us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.13%       3.000us         0.13%       3.000us       3.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.333ms
Self CUDA time total: 8.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                 model_inference fsp512         5.51%     120.000us        98.85%       2.154ms       2.154ms       0.000us         0.00%      15.000us      15.000us             1  
                                           aten::linear         0.83%      18.000us        90.41%       1.970ms     985.000us       0.000us         0.00%      14.000us       7.000us             2  
                                           aten::matmul         0.64%      14.000us        87.75%       1.912ms     956.000us       0.000us         0.00%      14.000us       7.000us             2  
                                               aten::mm        11.20%     244.000us        87.10%       1.898ms     949.000us      14.000us        93.33%      14.000us       7.000us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      14.000us        93.33%      14.000us       7.000us             2  
                                              aten::cos         1.74%      38.000us         2.39%      52.000us      52.000us       1.000us         6.67%       1.000us       1.000us             1  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         6.67%       1.000us       1.000us             1  
                                            aten::zeros         0.32%       7.000us         0.92%      20.000us      20.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         1.33%      29.000us         1.33%      29.000us       7.250us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.14%       3.000us         0.14%       3.000us       3.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.179ms
Self CUDA time total: 15.000us

Optimizer: Adam ,lr: 0.001
Training models with MNIST DATASET :
Fourier Signal Perceptron 128
Epoch 1
-------------------------------
loss: 2.378377  [    0/60000]
loss: 0.460332  [ 6400/60000]
loss: 0.339193  [12800/60000]
loss: 0.349166  [19200/60000]
loss: 0.181355  [25600/60000]
loss: 0.278884  [32000/60000]
loss: 0.111832  [38400/60000]
loss: 0.258115  [44800/60000]
loss: 0.238337  [51200/60000]
loss: 0.225563  [57600/60000]
Test Error: 
 Accuracy: 95.3%, Avg loss: 0.156650 

Epoch 2
-------------------------------
loss: 0.111219  [    0/60000]
loss: 0.133560  [ 6400/60000]
loss: 0.111666  [12800/60000]
loss: 0.129796  [19200/60000]
loss: 0.081914  [25600/60000]
loss: 0.163595  [32000/60000]
loss: 0.056547  [38400/60000]
loss: 0.169670  [44800/60000]
loss: 0.144724  [51200/60000]
loss: 0.119686  [57600/60000]
Test Error: 
 Accuracy: 96.9%, Avg loss: 0.106242 

Epoch 3
-------------------------------
loss: 0.054040  [    0/60000]
loss: 0.085407  [ 6400/60000]
loss: 0.072168  [12800/60000]
loss: 0.086372  [19200/60000]
loss: 0.048832  [25600/60000]
loss: 0.110006  [32000/60000]
loss: 0.035517  [38400/60000]
loss: 0.111481  [44800/60000]
loss: 0.106919  [51200/60000]
loss: 0.055539  [57600/60000]
Test Error: 
 Accuracy: 97.3%, Avg loss: 0.087820 

Epoch 4
-------------------------------
loss: 0.035470  [    0/60000]
loss: 0.056721  [ 6400/60000]
loss: 0.051556  [12800/60000]
loss: 0.065179  [19200/60000]
loss: 0.035749  [25600/60000]
loss: 0.084610  [32000/60000]
loss: 0.019827  [38400/60000]
loss: 0.065405  [44800/60000]
loss: 0.083791  [51200/60000]
loss: 0.033716  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.079013 

Epoch 5
-------------------------------
loss: 0.029595  [    0/60000]
loss: 0.027806  [ 6400/60000]
loss: 0.038682  [12800/60000]
loss: 0.041390  [19200/60000]
loss: 0.024366  [25600/60000]
loss: 0.064330  [32000/60000]
loss: 0.011359  [38400/60000]
loss: 0.047849  [44800/60000]
loss: 0.074557  [51200/60000]
loss: 0.021877  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.075375 

Epoch 6
-------------------------------
loss: 0.023337  [    0/60000]
loss: 0.011201  [ 6400/60000]
loss: 0.031238  [12800/60000]
loss: 0.026281  [19200/60000]
loss: 0.016501  [25600/60000]
loss: 0.039287  [32000/60000]
loss: 0.008782  [38400/60000]
loss: 0.035241  [44800/60000]
loss: 0.062378  [51200/60000]
loss: 0.018443  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.074166 

Epoch 7
-------------------------------
loss: 0.015605  [    0/60000]
loss: 0.005515  [ 6400/60000]
loss: 0.026144  [12800/60000]
loss: 0.020936  [19200/60000]
loss: 0.006783  [25600/60000]
loss: 0.023924  [32000/60000]
loss: 0.007025  [38400/60000]
loss: 0.024147  [44800/60000]
loss: 0.041895  [51200/60000]
loss: 0.012195  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.076578 

Epoch 8
-------------------------------
loss: 0.010554  [    0/60000]
loss: 0.003331  [ 6400/60000]
loss: 0.025491  [12800/60000]
loss: 0.015827  [19200/60000]
loss: 0.004820  [25600/60000]
loss: 0.011805  [32000/60000]
loss: 0.004429  [38400/60000]
loss: 0.017458  [44800/60000]
loss: 0.022063  [51200/60000]
loss: 0.011302  [57600/60000]
Test Error: 
 Accuracy: 97.7%, Avg loss: 0.078385 

Epoch 9
-------------------------------
loss: 0.006959  [    0/60000]
loss: 0.003295  [ 6400/60000]
loss: 0.015754  [12800/60000]
loss: 0.011733  [19200/60000]
loss: 0.004340  [25600/60000]
loss: 0.008845  [32000/60000]
loss: 0.002635  [38400/60000]
loss: 0.011547  [44800/60000]
loss: 0.010174  [51200/60000]
loss: 0.005843  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.078134 

Backprop time:
0.0013271003672083077
Final  epoch:
9 97.57000000000001 0.07813432783610076 tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 97.58, 0.07416560396142037, tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)]
Fourier Signal Perceptron 512
Epoch 1
-------------------------------
loss: 2.396140  [    0/60000]
loss: 0.327379  [ 6400/60000]
loss: 0.238321  [12800/60000]
loss: 0.254050  [19200/60000]
loss: 0.148513  [25600/60000]
loss: 0.286732  [32000/60000]
loss: 0.091632  [38400/60000]
loss: 0.254656  [44800/60000]
loss: 0.275362  [51200/60000]
loss: 0.204449  [57600/60000]
Test Error: 
 Accuracy: 96.1%, Avg loss: 0.121570 

Epoch 2
-------------------------------
loss: 0.112621  [    0/60000]
loss: 0.138860  [ 6400/60000]
loss: 0.089578  [12800/60000]
loss: 0.099506  [19200/60000]
loss: 0.081927  [25600/60000]
loss: 0.152969  [32000/60000]
loss: 0.063788  [38400/60000]
loss: 0.079353  [44800/60000]
loss: 0.126852  [51200/60000]
loss: 0.087969  [57600/60000]
Test Error: 
 Accuracy: 97.3%, Avg loss: 0.084138 

Epoch 3
-------------------------------
loss: 0.050594  [    0/60000]
loss: 0.080893  [ 6400/60000]
loss: 0.079748  [12800/60000]
loss: 0.031357  [19200/60000]
loss: 0.060612  [25600/60000]
loss: 0.093802  [32000/60000]
loss: 0.043631  [38400/60000]
loss: 0.032305  [44800/60000]
loss: 0.096127  [51200/60000]
loss: 0.024883  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.073461 

Epoch 4
-------------------------------
loss: 0.031345  [    0/60000]
loss: 0.035736  [ 6400/60000]
loss: 0.056082  [12800/60000]
loss: 0.015862  [19200/60000]
loss: 0.028906  [25600/60000]
loss: 0.053702  [32000/60000]
loss: 0.018485  [38400/60000]
loss: 0.019806  [44800/60000]
loss: 0.094015  [51200/60000]
loss: 0.010476  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.067340 

Epoch 5
-------------------------------
loss: 0.020254  [    0/60000]
loss: 0.005284  [ 6400/60000]
loss: 0.034349  [12800/60000]
loss: 0.012303  [19200/60000]
loss: 0.017774  [25600/60000]
loss: 0.023081  [32000/60000]
loss: 0.013663  [38400/60000]
loss: 0.013348  [44800/60000]
loss: 0.062523  [51200/60000]
loss: 0.005786  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.067963 

Epoch 6
-------------------------------
loss: 0.009690  [    0/60000]
loss: 0.003888  [ 6400/60000]
loss: 0.032505  [12800/60000]
loss: 0.010972  [19200/60000]
loss: 0.014994  [25600/60000]
loss: 0.007320  [32000/60000]
loss: 0.005216  [38400/60000]
loss: 0.010811  [44800/60000]
loss: 0.017197  [51200/60000]
loss: 0.004041  [57600/60000]
Test Error: 
 Accuracy: 97.7%, Avg loss: 0.080322 

Epoch 7
-------------------------------
loss: 0.017370  [    0/60000]
loss: 0.005736  [ 6400/60000]
loss: 0.038466  [12800/60000]
loss: 0.010828  [19200/60000]
loss: 0.007153  [25600/60000]
loss: 0.009460  [32000/60000]
loss: 0.028514  [38400/60000]
loss: 0.006592  [44800/60000]
loss: 0.026530  [51200/60000]
loss: 0.003495  [57600/60000]
Test Error: 
 Accuracy: 97.7%, Avg loss: 0.077864 

Epoch 8
-------------------------------
loss: 0.002807  [    0/60000]
loss: 0.002882  [ 6400/60000]
loss: 0.004141  [12800/60000]
loss: 0.021297  [19200/60000]
loss: 0.007100  [25600/60000]
loss: 0.024269  [32000/60000]
loss: 0.003539  [38400/60000]
loss: 0.004834  [44800/60000]
loss: 0.015748  [51200/60000]
loss: 0.001562  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.075388 

Epoch 9
-------------------------------
loss: 0.001868  [    0/60000]
loss: 0.004160  [ 6400/60000]
loss: 0.011857  [12800/60000]
loss: 0.010079  [19200/60000]
loss: 0.004077  [25600/60000]
loss: 0.003959  [32000/60000]
loss: 0.017950  [38400/60000]
loss: 0.004389  [44800/60000]
loss: 0.007427  [51200/60000]
loss: 0.056205  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.073139 

Backprop time:
0.0013441848856553848
Final  epoch:
9 97.85000000000001 0.07313891523948085 tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[3, 97.87, 0.06733960436543436]
MLP 1 hidden layer Signal Perceptron
Epoch 1
-------------------------------
loss: 2.294019  [    0/60000]
loss: 1.035769  [ 6400/60000]
loss: 1.434573  [12800/60000]
loss: 1.098860  [19200/60000]
loss: 0.927987  [25600/60000]
loss: 1.083601  [32000/60000]
loss: 0.997195  [38400/60000]
loss: 1.350431  [44800/60000]
loss: 1.094900  [51200/60000]
loss: 1.365071  [57600/60000]
Test Error: 
 Accuracy: 59.2%, Avg loss: 1.017079 

Epoch 2
-------------------------------
loss: 0.738387  [    0/60000]
loss: 0.882015  [ 6400/60000]
loss: 1.289024  [12800/60000]
loss: 1.011065  [19200/60000]
loss: 0.891501  [25600/60000]
loss: 0.992700  [32000/60000]
loss: 0.948493  [38400/60000]
loss: 1.290331  [44800/60000]
loss: 1.072945  [51200/60000]
loss: 1.310873  [57600/60000]
Test Error: 
 Accuracy: 59.5%, Avg loss: 0.981309 

Epoch 3
-------------------------------
loss: 0.696916  [    0/60000]
loss: 0.835008  [ 6400/60000]
loss: 1.213258  [12800/60000]
loss: 1.024906  [19200/60000]
loss: 0.864008  [25600/60000]
loss: 0.956811  [32000/60000]
loss: 0.945356  [38400/60000]
loss: 1.234229  [44800/60000]
loss: 1.019889  [51200/60000]
loss: 1.256900  [57600/60000]
Test Error: 
 Accuracy: 59.8%, Avg loss: 0.962928 

Epoch 4
-------------------------------
loss: 0.689607  [    0/60000]
loss: 0.796381  [ 6400/60000]
loss: 1.195791  [12800/60000]
loss: 1.001781  [19200/60000]
loss: 0.853231  [25600/60000]
loss: 0.934173  [32000/60000]
loss: 0.946947  [38400/60000]
loss: 1.212029  [44800/60000]
loss: 0.993275  [51200/60000]
loss: 1.252940  [57600/60000]
Test Error: 
 Accuracy: 59.9%, Avg loss: 0.958667 

Epoch 5
-------------------------------
loss: 0.681851  [    0/60000]
loss: 0.764116  [ 6400/60000]
loss: 1.178476  [12800/60000]
loss: 0.988844  [19200/60000]
loss: 0.860452  [25600/60000]
loss: 0.896527  [32000/60000]
loss: 0.937663  [38400/60000]
loss: 1.201006  [44800/60000]
loss: 0.992526  [51200/60000]
loss: 1.226277  [57600/60000]
Test Error: 
 Accuracy: 59.9%, Avg loss: 0.958953 

Epoch 6
-------------------------------
loss: 0.666090  [    0/60000]
loss: 0.734679  [ 6400/60000]
loss: 1.176170  [12800/60000]
loss: 0.996643  [19200/60000]
loss: 0.828677  [25600/60000]
loss: 0.898863  [32000/60000]
loss: 0.937117  [38400/60000]
loss: 1.200270  [44800/60000]
loss: 0.992392  [51200/60000]
loss: 1.218635  [57600/60000]
Test Error: 
 Accuracy: 60.0%, Avg loss: 0.957702 

Epoch 7
-------------------------------
loss: 0.657428  [    0/60000]
loss: 0.726838  [ 6400/60000]
loss: 1.184028  [12800/60000]
loss: 0.974241  [19200/60000]
loss: 0.822136  [25600/60000]
loss: 0.884084  [32000/60000]
loss: 0.936802  [38400/60000]
loss: 1.199696  [44800/60000]
loss: 0.980741  [51200/60000]
loss: 1.208927  [57600/60000]
Test Error: 
 Accuracy: 59.8%, Avg loss: 0.959968 

Epoch 8
-------------------------------
loss: 0.678379  [    0/60000]
loss: 0.742631  [ 6400/60000]
loss: 1.163969  [12800/60000]
loss: 0.961907  [19200/60000]
loss: 0.815198  [25600/60000]
loss: 0.869082  [32000/60000]
loss: 0.936059  [38400/60000]
loss: 1.175050  [44800/60000]
loss: 0.991000  [51200/60000]
loss: 1.189102  [57600/60000]
Test Error: 
 Accuracy: 59.9%, Avg loss: 0.962890 

Epoch 9
-------------------------------
loss: 0.652376  [    0/60000]
loss: 0.741197  [ 6400/60000]
loss: 1.161333  [12800/60000]
loss: 0.946865  [19200/60000]
loss: 0.795653  [25600/60000]
loss: 0.864542  [32000/60000]
loss: 0.935714  [38400/60000]
loss: 1.154389  [44800/60000]
loss: 0.976218  [51200/60000]
loss: 1.198494  [57600/60000]
Test Error: 
 Accuracy: 59.8%, Avg loss: 0.963806 

Backprop time:
0.0013276378116006677
Final  epoch:
9 59.830000000000005 0.9638063197682618 tensor(1.0077, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 59.95, 0.9577016082538921, tensor(1.0095, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 2 hidden layer Signal Perceptron
Epoch 1
-------------------------------
loss: 2.302411  [    0/60000]
loss: 0.746146  [ 6400/60000]
loss: 0.684330  [12800/60000]
loss: 0.457033  [19200/60000]
loss: 0.791789  [25600/60000]
loss: 0.950979  [32000/60000]
loss: 0.351614  [38400/60000]
loss: 0.758433  [44800/60000]
loss: 0.638088  [51200/60000]
loss: 0.304387  [57600/60000]
Test Error: 
 Accuracy: 78.4%, Avg loss: 0.573713 

Epoch 2
-------------------------------
loss: 0.508865  [    0/60000]
loss: 0.566025  [ 6400/60000]
loss: 0.589692  [12800/60000]
loss: 0.377854  [19200/60000]
loss: 0.704567  [25600/60000]
loss: 0.737050  [32000/60000]
loss: 0.310724  [38400/60000]
loss: 0.703071  [44800/60000]
loss: 0.554221  [51200/60000]
loss: 0.315073  [57600/60000]
Test Error: 
 Accuracy: 79.3%, Avg loss: 0.521522 

Epoch 3
-------------------------------
loss: 0.485455  [    0/60000]
loss: 0.588943  [ 6400/60000]
loss: 0.531763  [12800/60000]
loss: 0.386288  [19200/60000]
loss: 0.668224  [25600/60000]
loss: 0.716728  [32000/60000]
loss: 0.301695  [38400/60000]
loss: 0.583521  [44800/60000]
loss: 0.486946  [51200/60000]
loss: 0.273541  [57600/60000]
Test Error: 
 Accuracy: 79.5%, Avg loss: 0.512294 

Epoch 4
-------------------------------
loss: 0.480592  [    0/60000]
loss: 0.543628  [ 6400/60000]
loss: 0.503051  [12800/60000]
loss: 0.369359  [19200/60000]
loss: 0.685951  [25600/60000]
loss: 0.692185  [32000/60000]
loss: 0.304300  [38400/60000]
loss: 0.582072  [44800/60000]
loss: 0.485426  [51200/60000]
loss: 0.215205  [57600/60000]
Test Error: 
 Accuracy: 79.4%, Avg loss: 0.514736 

Epoch 5
-------------------------------
loss: 0.445551  [    0/60000]
loss: 0.509168  [ 6400/60000]
loss: 0.494929  [12800/60000]
loss: 0.310872  [19200/60000]
loss: 0.661537  [25600/60000]
loss: 0.683556  [32000/60000]
loss: 0.289211  [38400/60000]
loss: 0.542561  [44800/60000]
loss: 0.473520  [51200/60000]
loss: 0.208308  [57600/60000]
Test Error: 
 Accuracy: 79.6%, Avg loss: 0.515625 

Epoch 6
-------------------------------
loss: 0.462545  [    0/60000]
loss: 0.513308  [ 6400/60000]
loss: 0.470933  [12800/60000]
loss: 0.317531  [19200/60000]
loss: 0.662280  [25600/60000]
loss: 0.669833  [32000/60000]
loss: 0.300824  [38400/60000]
loss: 0.565573  [44800/60000]
loss: 0.477455  [51200/60000]
loss: 0.208144  [57600/60000]
Test Error: 
 Accuracy: 79.9%, Avg loss: 0.507718 

Epoch 7
-------------------------------
loss: 0.435219  [    0/60000]
loss: 0.511890  [ 6400/60000]
loss: 0.507457  [12800/60000]
loss: 0.321770  [19200/60000]
loss: 0.659401  [25600/60000]
loss: 0.665471  [32000/60000]
loss: 0.293503  [38400/60000]
loss: 0.542813  [44800/60000]
loss: 0.473878  [51200/60000]
loss: 0.182306  [57600/60000]
Test Error: 
 Accuracy: 79.8%, Avg loss: 0.507409 

Epoch 8
-------------------------------
loss: 0.446694  [    0/60000]
loss: 0.514503  [ 6400/60000]
loss: 0.470136  [12800/60000]
loss: 0.351258  [19200/60000]
loss: 0.663214  [25600/60000]
loss: 0.648513  [32000/60000]
loss: 0.316583  [38400/60000]
loss: 0.547379  [44800/60000]
loss: 0.482575  [51200/60000]
loss: 0.187991  [57600/60000]
Test Error: 
 Accuracy: 79.8%, Avg loss: 0.523584 

Epoch 9
-------------------------------
loss: 0.510689  [    0/60000]
loss: 0.506264  [ 6400/60000]
loss: 0.468693  [12800/60000]
loss: 0.345663  [19200/60000]
loss: 0.680766  [25600/60000]
loss: 0.682513  [32000/60000]
loss: 0.290326  [38400/60000]
loss: 0.579343  [44800/60000]
loss: 0.484282  [51200/60000]
loss: 0.180653  [57600/60000]
Test Error: 
 Accuracy: 79.8%, Avg loss: 0.523469 

Backprop time:
0.0017846252251168444
Final  epoch:
9 79.80000000000001 0.5234689202847754 tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[6, 79.84, 0.5074088030560001, tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)]
Training models with FashionMNIST DATASET :
Fourier Signal Perceptron 128
Epoch 1
-------------------------------
loss: 2.439080  [    0/60000]
loss: 0.555052  [ 6400/60000]
loss: 0.406390  [12800/60000]
loss: 0.536370  [19200/60000]
loss: 0.475320  [25600/60000]
loss: 0.399198  [32000/60000]
loss: 0.364747  [38400/60000]
loss: 0.491889  [44800/60000]
loss: 0.497347  [51200/60000]
loss: 0.503297  [57600/60000]
Test Error: 
 Accuracy: 85.0%, Avg loss: 0.412456 

Epoch 2
-------------------------------
loss: 0.290695  [    0/60000]
loss: 0.391826  [ 6400/60000]
loss: 0.329127  [12800/60000]
loss: 0.360299  [19200/60000]
loss: 0.346356  [25600/60000]
loss: 0.317646  [32000/60000]
loss: 0.293941  [38400/60000]
loss: 0.453027  [44800/60000]
loss: 0.414778  [51200/60000]
loss: 0.450748  [57600/60000]
Test Error: 
 Accuracy: 85.6%, Avg loss: 0.401623 

Epoch 3
-------------------------------
loss: 0.232265  [    0/60000]
loss: 0.370250  [ 6400/60000]
loss: 0.268338  [12800/60000]
loss: 0.308961  [19200/60000]
loss: 0.318155  [25600/60000]
loss: 0.290643  [32000/60000]
loss: 0.262466  [38400/60000]
loss: 0.439963  [44800/60000]
loss: 0.389062  [51200/60000]
loss: 0.422396  [57600/60000]
Test Error: 
 Accuracy: 86.2%, Avg loss: 0.388129 

Epoch 4
-------------------------------
loss: 0.207204  [    0/60000]
loss: 0.362562  [ 6400/60000]
loss: 0.234167  [12800/60000]
loss: 0.277698  [19200/60000]
loss: 0.312325  [25600/60000]
loss: 0.273905  [32000/60000]
loss: 0.247288  [38400/60000]
loss: 0.386157  [44800/60000]
loss: 0.315049  [51200/60000]
loss: 0.361107  [57600/60000]
Test Error: 
 Accuracy: 86.7%, Avg loss: 0.369571 

Epoch 5
-------------------------------
loss: 0.179901  [    0/60000]
loss: 0.333227  [ 6400/60000]
loss: 0.233460  [12800/60000]
loss: 0.256747  [19200/60000]
loss: 0.339839  [25600/60000]
loss: 0.250654  [32000/60000]
loss: 0.235229  [38400/60000]
loss: 0.330904  [44800/60000]
loss: 0.283829  [51200/60000]
loss: 0.326521  [57600/60000]
Test Error: 
 Accuracy: 86.9%, Avg loss: 0.359590 

Epoch 6
-------------------------------
loss: 0.175405  [    0/60000]
loss: 0.320371  [ 6400/60000]
loss: 0.245809  [12800/60000]
loss: 0.221500  [19200/60000]
loss: 0.292952  [25600/60000]
loss: 0.283337  [32000/60000]
loss: 0.230783  [38400/60000]
loss: 0.301497  [44800/60000]
loss: 0.263452  [51200/60000]
loss: 0.305897  [57600/60000]
Test Error: 
 Accuracy: 87.2%, Avg loss: 0.362293 

Epoch 7
-------------------------------
loss: 0.156354  [    0/60000]
loss: 0.320180  [ 6400/60000]
loss: 0.253454  [12800/60000]
loss: 0.224223  [19200/60000]
loss: 0.300763  [25600/60000]
loss: 0.288827  [32000/60000]
loss: 0.245607  [38400/60000]
loss: 0.302269  [44800/60000]
loss: 0.205673  [51200/60000]
loss: 0.298514  [57600/60000]
Test Error: 
 Accuracy: 86.8%, Avg loss: 0.368329 

Epoch 8
-------------------------------
loss: 0.168962  [    0/60000]
loss: 0.294131  [ 6400/60000]
loss: 0.246826  [12800/60000]
loss: 0.198054  [19200/60000]
loss: 0.277329  [25600/60000]
loss: 0.279901  [32000/60000]
loss: 0.258020  [38400/60000]
loss: 0.296993  [44800/60000]
loss: 0.210357  [51200/60000]
loss: 0.262754  [57600/60000]
Test Error: 
 Accuracy: 86.7%, Avg loss: 0.376072 

Epoch 9
-------------------------------
loss: 0.156642  [    0/60000]
loss: 0.290065  [ 6400/60000]
loss: 0.227333  [12800/60000]
loss: 0.164504  [19200/60000]
loss: 0.242935  [25600/60000]
loss: 0.275957  [32000/60000]
loss: 0.232960  [38400/60000]
loss: 0.284675  [44800/60000]
loss: 0.222469  [51200/60000]
loss: 0.245997  [57600/60000]
Test Error: 
 Accuracy: 86.7%, Avg loss: 0.384243 

Backprop time:
0.0012022998484919009
Final  epoch:
9 86.71 0.38424277875074153 tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[4, 86.9, 0.35958981471266716, tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)]
Fourier Signal Perceptron 512
Epoch 1
-------------------------------
loss: 2.344440  [    0/60000]
loss: 0.555877  [ 6400/60000]
loss: 0.447087  [12800/60000]
loss: 0.558960  [19200/60000]
loss: 0.416550  [25600/60000]
loss: 0.387421  [32000/60000]
loss: 0.348857  [38400/60000]
loss: 0.511854  [44800/60000]
loss: 0.537512  [51200/60000]
loss: 0.515491  [57600/60000]
Test Error: 
 Accuracy: 84.5%, Avg loss: 0.422605 

Epoch 2
-------------------------------
loss: 0.264699  [    0/60000]
loss: 0.401940  [ 6400/60000]
loss: 0.348858  [12800/60000]
loss: 0.402792  [19200/60000]
loss: 0.429164  [25600/60000]
loss: 0.322575  [32000/60000]
loss: 0.292718  [38400/60000]
loss: 0.391689  [44800/60000]
loss: 0.357457  [51200/60000]
loss: 0.442547  [57600/60000]
Test Error: 
 Accuracy: 85.7%, Avg loss: 0.410725 

Epoch 3
-------------------------------
loss: 0.177851  [    0/60000]
loss: 0.321062  [ 6400/60000]
loss: 0.277769  [12800/60000]
loss: 0.337877  [19200/60000]
loss: 0.335791  [25600/60000]
loss: 0.306539  [32000/60000]
loss: 0.279928  [38400/60000]
loss: 0.330724  [44800/60000]
loss: 0.320019  [51200/60000]
loss: 0.422797  [57600/60000]
Test Error: 
 Accuracy: 85.9%, Avg loss: 0.400553 

Epoch 4
-------------------------------
loss: 0.173340  [    0/60000]
loss: 0.303970  [ 6400/60000]
loss: 0.254103  [12800/60000]
loss: 0.270652  [19200/60000]
loss: 0.368490  [25600/60000]
loss: 0.317304  [32000/60000]
loss: 0.230672  [38400/60000]
loss: 0.291740  [44800/60000]
loss: 0.356410  [51200/60000]
loss: 0.369194  [57600/60000]
Test Error: 
 Accuracy: 86.7%, Avg loss: 0.384431 

Epoch 5
-------------------------------
loss: 0.142972  [    0/60000]
loss: 0.282223  [ 6400/60000]
loss: 0.255560  [12800/60000]
loss: 0.245383  [19200/60000]
loss: 0.284594  [25600/60000]
loss: 0.281616  [32000/60000]
loss: 0.220289  [38400/60000]
loss: 0.309224  [44800/60000]
loss: 0.351581  [51200/60000]
loss: 0.388636  [57600/60000]
Test Error: 
 Accuracy: 86.3%, Avg loss: 0.394739 

Epoch 6
-------------------------------
loss: 0.166803  [    0/60000]
loss: 0.241895  [ 6400/60000]
loss: 0.215069  [12800/60000]
loss: 0.243139  [19200/60000]
loss: 0.271375  [25600/60000]
loss: 0.272563  [32000/60000]
loss: 0.200342  [38400/60000]
loss: 0.270335  [44800/60000]
loss: 0.301980  [51200/60000]
loss: 0.337412  [57600/60000]
Test Error: 
 Accuracy: 86.4%, Avg loss: 0.399417 

Epoch 7
-------------------------------
loss: 0.175422  [    0/60000]
loss: 0.240227  [ 6400/60000]
loss: 0.207680  [12800/60000]
loss: 0.220359  [19200/60000]
loss: 0.277558  [25600/60000]
loss: 0.238952  [32000/60000]
loss: 0.213245  [38400/60000]
loss: 0.220824  [44800/60000]
loss: 0.255245  [51200/60000]
loss: 0.276328  [57600/60000]
Test Error: 
 Accuracy: 86.5%, Avg loss: 0.405414 

Epoch 8
-------------------------------
loss: 0.148554  [    0/60000]
loss: 0.240266  [ 6400/60000]
loss: 0.241778  [12800/60000]
loss: 0.172652  [19200/60000]
loss: 0.273195  [25600/60000]
loss: 0.258065  [32000/60000]
loss: 0.196913  [38400/60000]
loss: 0.292639  [44800/60000]
loss: 0.279991  [51200/60000]
loss: 0.279895  [57600/60000]
Test Error: 
 Accuracy: 86.5%, Avg loss: 0.398236 

Epoch 9
-------------------------------
loss: 0.181447  [    0/60000]
loss: 0.214720  [ 6400/60000]
loss: 0.283166  [12800/60000]
loss: 0.214338  [19200/60000]
loss: 0.257819  [25600/60000]
loss: 0.308078  [32000/60000]
loss: 0.258584  [38400/60000]
loss: 0.278558  [44800/60000]
loss: 0.205636  [51200/60000]
loss: 0.266053  [57600/60000]
Test Error: 
 Accuracy: 86.5%, Avg loss: 0.411719 

Backprop time:
0.001304039512305315
Final  epoch:
9 86.47 0.41171891436835 tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[3, 86.66, 0.384431307672695, tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 1 hidden layer
Epoch 1
-------------------------------
loss: 2.312951  [    0/60000]
loss: 1.616098  [ 6400/60000]
loss: 1.341650  [12800/60000]
loss: 1.556557  [19200/60000]
loss: 1.469380  [25600/60000]
loss: 1.473373  [32000/60000]
loss: 1.550899  [38400/60000]
loss: 1.579705  [44800/60000]
loss: 1.465996  [51200/60000]
loss: 1.720395  [57600/60000]
Test Error: 
 Accuracy: 46.8%, Avg loss: 1.492653 

Epoch 2
-------------------------------
loss: 1.359359  [    0/60000]
loss: 1.426010  [ 6400/60000]
loss: 1.299222  [12800/60000]
loss: 1.503039  [19200/60000]
loss: 1.345636  [25600/60000]
loss: 1.483097  [32000/60000]
loss: 1.519984  [38400/60000]
loss: 1.604822  [44800/60000]
loss: 1.449579  [51200/60000]
loss: 1.608408  [57600/60000]
Test Error: 
 Accuracy: 47.7%, Avg loss: 1.336870 

Epoch 3
-------------------------------
loss: 1.138644  [    0/60000]
loss: 1.203923  [ 6400/60000]
loss: 1.148146  [12800/60000]
loss: 1.427895  [19200/60000]
loss: 1.105344  [25600/60000]
loss: 1.368769  [32000/60000]
loss: 1.330143  [38400/60000]
loss: 1.402715  [44800/60000]
loss: 1.263484  [51200/60000]
loss: 1.599870  [57600/60000]
Test Error: 
 Accuracy: 48.0%, Avg loss: 1.316606 

Epoch 4
-------------------------------
loss: 1.101988  [    0/60000]
loss: 1.179390  [ 6400/60000]
loss: 1.147089  [12800/60000]
loss: 1.417074  [19200/60000]
loss: 1.084076  [25600/60000]
loss: 1.394785  [32000/60000]
loss: 1.320230  [38400/60000]
loss: 1.377700  [44800/60000]
loss: 1.254952  [51200/60000]
loss: 1.578490  [57600/60000]
Test Error: 
 Accuracy: 48.2%, Avg loss: 1.309927 

Epoch 5
-------------------------------
loss: 1.075308  [    0/60000]
loss: 1.170866  [ 6400/60000]
loss: 1.144245  [12800/60000]
loss: 1.401128  [19200/60000]
loss: 1.105144  [25600/60000]
loss: 1.385782  [32000/60000]
loss: 1.327117  [38400/60000]
loss: 1.343822  [44800/60000]
loss: 1.240867  [51200/60000]
loss: 1.546490  [57600/60000]
Test Error: 
 Accuracy: 48.1%, Avg loss: 1.304986 

Epoch 6
-------------------------------
loss: 1.058014  [    0/60000]
loss: 1.168315  [ 6400/60000]
loss: 1.144936  [12800/60000]
loss: 1.386309  [19200/60000]
loss: 1.094000  [25600/60000]
loss: 1.399123  [32000/60000]
loss: 1.318725  [38400/60000]
loss: 1.332315  [44800/60000]
loss: 1.225300  [51200/60000]
loss: 1.571231  [57600/60000]
Test Error: 
 Accuracy: 48.2%, Avg loss: 1.303936 

Epoch 7
-------------------------------
loss: 1.039109  [    0/60000]
loss: 1.161050  [ 6400/60000]
loss: 1.139908  [12800/60000]
loss: 1.381006  [19200/60000]
loss: 1.087504  [25600/60000]
loss: 1.381189  [32000/60000]
loss: 1.328722  [38400/60000]
loss: 1.301311  [44800/60000]
loss: 1.224779  [51200/60000]
loss: 1.547524  [57600/60000]
Test Error: 
 Accuracy: 47.9%, Avg loss: 1.305348 

Epoch 8
-------------------------------
loss: 1.034396  [    0/60000]
loss: 1.157045  [ 6400/60000]
loss: 1.126782  [12800/60000]
loss: 1.381071  [19200/60000]
loss: 1.072163  [25600/60000]
loss: 1.382346  [32000/60000]
loss: 1.322228  [38400/60000]
loss: 1.281585  [44800/60000]
loss: 1.223242  [51200/60000]
loss: 1.512952  [57600/60000]
Test Error: 
 Accuracy: 48.2%, Avg loss: 1.303379 

Epoch 9
-------------------------------
loss: 1.030909  [    0/60000]
loss: 1.144623  [ 6400/60000]
loss: 1.121208  [12800/60000]
loss: 1.423699  [19200/60000]
loss: 1.083711  [25600/60000]
loss: 1.356475  [32000/60000]
loss: 1.309854  [38400/60000]
loss: 1.259568  [44800/60000]
loss: 1.224652  [51200/60000]
loss: 1.505459  [57600/60000]
Test Error: 
 Accuracy: 48.2%, Avg loss: 1.303591 

Backprop time:
0.0013289411495545837
Final  epoch:
9 48.230000000000004 1.30359118531464 tensor(1.3936, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[7, 48.18, 1.303378912293987, tensor(1.3800, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 2 hidden layer
Epoch 1
-------------------------------
loss: 2.301091  [    0/60000]
loss: 1.445552  [ 6400/60000]
loss: 1.243576  [12800/60000]
loss: 1.282779  [19200/60000]
loss: 1.523924  [25600/60000]
loss: 1.307136  [32000/60000]
loss: 1.198061  [38400/60000]
loss: 1.433969  [44800/60000]
loss: 1.274951  [51200/60000]
loss: 1.169945  [57600/60000]
Test Error: 
 Accuracy: 52.0%, Avg loss: 1.260826 

Epoch 2
-------------------------------
loss: 1.125304  [    0/60000]
loss: 1.227833  [ 6400/60000]
loss: 1.183622  [12800/60000]
loss: 1.180330  [19200/60000]
loss: 1.484285  [25600/60000]
loss: 1.238560  [32000/60000]
loss: 1.162310  [38400/60000]
loss: 1.359162  [44800/60000]
loss: 1.213257  [51200/60000]
loss: 1.121320  [57600/60000]
Test Error: 
 Accuracy: 52.4%, Avg loss: 1.242646 

Epoch 3
-------------------------------
loss: 1.120976  [    0/60000]
loss: 1.203821  [ 6400/60000]
loss: 1.174351  [12800/60000]
loss: 1.130209  [19200/60000]
loss: 1.456501  [25600/60000]
loss: 1.071850  [32000/60000]
loss: 0.995715  [38400/60000]
loss: 1.196940  [44800/60000]
loss: 1.033815  [51200/60000]
loss: 0.955901  [57600/60000]
Test Error: 
 Accuracy: 62.0%, Avg loss: 1.032050 

Epoch 4
-------------------------------
loss: 1.009304  [    0/60000]
loss: 0.989082  [ 6400/60000]
loss: 0.886532  [12800/60000]
loss: 0.784329  [19200/60000]
loss: 1.230283  [25600/60000]
loss: 1.040098  [32000/60000]
loss: 0.950881  [38400/60000]
loss: 1.186322  [44800/60000]
loss: 0.996299  [51200/60000]
loss: 0.904873  [57600/60000]
Test Error: 
 Accuracy: 61.9%, Avg loss: 1.025340 

Epoch 5
-------------------------------
loss: 0.992633  [    0/60000]
loss: 0.971540  [ 6400/60000]
loss: 0.877270  [12800/60000]
loss: 0.775904  [19200/60000]
loss: 1.184334  [25600/60000]
loss: 1.059494  [32000/60000]
loss: 0.912399  [38400/60000]
loss: 1.156077  [44800/60000]
loss: 0.929771  [51200/60000]
loss: 0.910934  [57600/60000]
Test Error: 
 Accuracy: 62.2%, Avg loss: 1.011395 

Epoch 6
-------------------------------
loss: 0.980226  [    0/60000]
loss: 0.992482  [ 6400/60000]
loss: 0.765254  [12800/60000]
loss: 0.634551  [19200/60000]
loss: 1.082172  [25600/60000]
loss: 0.760614  [32000/60000]
loss: 0.697538  [38400/60000]
loss: 0.784426  [44800/60000]
loss: 0.719610  [51200/60000]
loss: 0.657553  [57600/60000]
Test Error: 
 Accuracy: 68.8%, Avg loss: 0.801147 

Epoch 7
-------------------------------
loss: 0.710980  [    0/60000]
loss: 0.799903  [ 6400/60000]
loss: 0.685532  [12800/60000]
loss: 0.546584  [19200/60000]
loss: 1.101566  [25600/60000]
loss: 0.732935  [32000/60000]
loss: 0.661090  [38400/60000]
loss: 0.712189  [44800/60000]
loss: 0.677004  [51200/60000]
loss: 0.672980  [57600/60000]
Test Error: 
 Accuracy: 68.9%, Avg loss: 0.804635 

Epoch 8
-------------------------------
loss: 0.696154  [    0/60000]
loss: 0.766301  [ 6400/60000]
loss: 0.678234  [12800/60000]
loss: 0.539393  [19200/60000]
loss: 1.105108  [25600/60000]
loss: 0.693707  [32000/60000]
loss: 0.451058  [38400/60000]
loss: 0.452062  [44800/60000]
loss: 0.615725  [51200/60000]
loss: 0.400122  [57600/60000]
Test Error: 
 Accuracy: 78.5%, Avg loss: 0.585572 

Epoch 9
-------------------------------
loss: 0.511393  [    0/60000]
loss: 0.474073  [ 6400/60000]
loss: 0.433024  [12800/60000]
loss: 0.382189  [19200/60000]
loss: 0.674201  [25600/60000]
loss: 0.376961  [32000/60000]
loss: 0.413893  [38400/60000]
loss: 0.415083  [44800/60000]
loss: 0.573695  [51200/60000]
loss: 0.394103  [57600/60000]
Test Error: 
 Accuracy: 79.0%, Avg loss: 0.578397 

Backprop time:
0.0018111641896728428
Final  epoch:
9 78.96 0.5783966529141565 tensor(0.4717, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[8, 78.96, 0.5783966529141565, tensor(0.4717, device='cuda:0', grad_fn=<NllLossBackward>)]
