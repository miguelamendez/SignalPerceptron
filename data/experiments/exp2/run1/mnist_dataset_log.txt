Using cuda device
Learnable Parameters for MNIST models:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
101632 	 406528 	 407050 	 669706
torch.Size([64, 1, 28, 28])
torch.Size([1, 28, 28])
Forward time for MNIST models:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
0.012926101684570312 	 7.176399230957031e-05 	 0.00010657310485839844 	 0.0001590251922607422
Forward time for MNIST models FS Timer class:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
7.002711296081542e-05 	 6.698870658874512e-05 	 0.00010900974273681641 	 0.00015651702880859375
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                   model_inference mlp1         5.79%     134.000us        99.18%       2.295ms       2.295ms       0.000us         0.00%      17.000us      17.000us             1  
                                           aten::linear         0.73%      17.000us        87.64%       2.028ms       1.014ms       0.000us         0.00%      16.000us       8.000us             2  
                                            aten::addmm        10.37%     240.000us        85.83%       1.986ms     993.000us      16.000us        94.12%      16.000us       8.000us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      14.000us        82.35%      14.000us       7.000us             2  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us        11.76%       2.000us       1.000us             2  
                                        aten::clamp_min         1.94%      45.000us         6.70%     155.000us      38.750us       1.000us         5.88%       2.000us       0.500us             4  
                                             aten::relu         1.12%      26.000us         5.06%     117.000us      58.500us       0.000us         0.00%       1.000us       0.500us             2  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         5.88%       1.000us       0.500us             2  
                                            aten::zeros         0.35%       8.000us         0.61%      14.000us      14.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         0.82%      19.000us         0.82%      19.000us       4.750us       0.000us         0.00%       0.000us       0.000us             4  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.314ms
Self CUDA time total: 17.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                   model_inference mlp2         6.18%     151.000us        99.18%       2.425ms       2.425ms       0.000us         0.00%      28.000us      28.000us             1  
                                           aten::linear         1.51%      37.000us        86.18%       2.107ms     702.333us       0.000us         0.00%      26.000us       8.667us             3  
                                            aten::addmm        12.02%     294.000us        83.27%       2.036ms     678.667us      26.000us        92.86%      26.000us       8.667us             3  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      23.000us        82.14%      23.000us       7.667us             3  
                                        aten::clamp_min         2.41%      59.000us         8.26%     202.000us      33.667us       2.000us         7.14%       4.000us       0.667us             6  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us        10.71%       3.000us       1.000us             3  
                                             aten::relu         1.39%      34.000us         6.22%     152.000us      50.667us       0.000us         0.00%       2.000us       0.667us             3  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         7.14%       2.000us       0.667us             3  
                                            aten::zeros         0.29%       7.000us         0.61%      15.000us      15.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         0.90%      22.000us         0.90%      22.000us       4.400us       0.000us         0.00%       0.000us       0.000us             5  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.445ms
Self CUDA time total: 28.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                 model_inference fsp128         5.25%     118.000us        98.98%       2.223ms       2.223ms       0.000us         0.00%       7.000us       7.000us             1  
                                           aten::linear         0.67%      15.000us        90.96%       2.043ms       1.022ms       0.000us         0.00%       6.000us       3.000us             2  
                                           aten::matmul         0.67%      15.000us        88.96%       1.998ms     999.000us       0.000us         0.00%       6.000us       3.000us             2  
                                               aten::mm         9.48%     213.000us        88.29%       1.983ms     991.500us       6.000us        85.71%       6.000us       3.000us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us        85.71%       6.000us       3.000us             2  
                                              aten::cos         1.60%      36.000us         2.18%      49.000us      49.000us       1.000us        14.29%       1.000us       1.000us             1  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us        14.29%       1.000us       1.000us             1  
                                            aten::zeros         0.45%      10.000us         0.80%      18.000us      18.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         0.93%      21.000us         0.93%      21.000us       5.250us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.04%       1.000us         0.04%       1.000us       1.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.246ms
Self CUDA time total: 7.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                 model_inference fsp512         5.15%     113.000us        99.09%       2.176ms       2.176ms       0.000us         0.00%      16.000us      16.000us             1  
                                           aten::linear         0.64%      14.000us        91.12%       2.001ms       1.000ms       0.000us         0.00%      15.000us       7.500us             2  
                                           aten::matmul         0.55%      12.000us        89.16%       1.958ms     979.000us       0.000us         0.00%      15.000us       7.500us             2  
                                               aten::mm         9.38%     206.000us        88.62%       1.946ms     973.000us      15.000us        93.75%      15.000us       7.500us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      15.000us        93.75%      15.000us       7.500us             2  
                                              aten::cos         1.64%      36.000us         2.19%      48.000us      48.000us       1.000us         6.25%       1.000us       1.000us             1  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         6.25%       1.000us       1.000us             1  
                                            aten::zeros         0.41%       9.000us         0.68%      15.000us      15.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         0.91%      20.000us         0.91%      20.000us       5.000us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.05%       1.000us         0.05%       1.000us       1.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.196ms
Self CUDA time total: 16.000us

Optimizer: Adam ,lr: 0.001
Training models with MNIST DATASET :
Fourier Signal Perceptron 128
Epoch 1
-------------------------------
loss: 2.501209  [    0/60000]
loss: 0.445988  [ 6400/60000]
loss: 0.327489  [12800/60000]
loss: 0.332198  [19200/60000]
loss: 0.200429  [25600/60000]
loss: 0.297973  [32000/60000]
loss: 0.117142  [38400/60000]
loss: 0.283112  [44800/60000]
loss: 0.254454  [51200/60000]
loss: 0.243838  [57600/60000]
Test Error: 
 Accuracy: 95.5%, Avg loss: 0.156900 

Epoch 2
-------------------------------
loss: 0.127860  [    0/60000]
loss: 0.150821  [ 6400/60000]
loss: 0.106175  [12800/60000]
loss: 0.102624  [19200/60000]
loss: 0.103059  [25600/60000]
loss: 0.165857  [32000/60000]
loss: 0.050696  [38400/60000]
loss: 0.172145  [44800/60000]
loss: 0.154710  [51200/60000]
loss: 0.141584  [57600/60000]
Test Error: 
 Accuracy: 96.8%, Avg loss: 0.104595 

Epoch 3
-------------------------------
loss: 0.075712  [    0/60000]
loss: 0.106931  [ 6400/60000]
loss: 0.066017  [12800/60000]
loss: 0.047819  [19200/60000]
loss: 0.060818  [25600/60000]
loss: 0.110674  [32000/60000]
loss: 0.032107  [38400/60000]
loss: 0.108182  [44800/60000]
loss: 0.112680  [51200/60000]
loss: 0.075665  [57600/60000]
Test Error: 
 Accuracy: 97.2%, Avg loss: 0.088428 

Epoch 4
-------------------------------
loss: 0.048950  [    0/60000]
loss: 0.086707  [ 6400/60000]
loss: 0.052045  [12800/60000]
loss: 0.032804  [19200/60000]
loss: 0.045979  [25600/60000]
loss: 0.081265  [32000/60000]
loss: 0.021070  [38400/60000]
loss: 0.075030  [44800/60000]
loss: 0.087332  [51200/60000]
loss: 0.034177  [57600/60000]
Test Error: 
 Accuracy: 97.4%, Avg loss: 0.081202 

Epoch 5
-------------------------------
loss: 0.036196  [    0/60000]
loss: 0.070362  [ 6400/60000]
loss: 0.044616  [12800/60000]
loss: 0.026996  [19200/60000]
loss: 0.035744  [25600/60000]
loss: 0.061756  [32000/60000]
loss: 0.015624  [38400/60000]
loss: 0.052760  [44800/60000]
loss: 0.064491  [51200/60000]
loss: 0.019129  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.080343 

Epoch 6
-------------------------------
loss: 0.026063  [    0/60000]
loss: 0.053836  [ 6400/60000]
loss: 0.032739  [12800/60000]
loss: 0.023207  [19200/60000]
loss: 0.025379  [25600/60000]
loss: 0.043148  [32000/60000]
loss: 0.011358  [38400/60000]
loss: 0.033460  [44800/60000]
loss: 0.043711  [51200/60000]
loss: 0.011377  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.080734 

Epoch 7
-------------------------------
loss: 0.015817  [    0/60000]
loss: 0.027827  [ 6400/60000]
loss: 0.022635  [12800/60000]
loss: 0.019250  [19200/60000]
loss: 0.016755  [25600/60000]
loss: 0.030764  [32000/60000]
loss: 0.007293  [38400/60000]
loss: 0.024488  [44800/60000]
loss: 0.020761  [51200/60000]
loss: 0.008311  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.083479 

Epoch 8
-------------------------------
loss: 0.010397  [    0/60000]
loss: 0.011054  [ 6400/60000]
loss: 0.015906  [12800/60000]
loss: 0.015294  [19200/60000]
loss: 0.010843  [25600/60000]
loss: 0.016517  [32000/60000]
loss: 0.004146  [38400/60000]
loss: 0.015399  [44800/60000]
loss: 0.009710  [51200/60000]
loss: 0.007681  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.084031 

Epoch 9
-------------------------------
loss: 0.005694  [    0/60000]
loss: 0.004998  [ 6400/60000]
loss: 0.013919  [12800/60000]
loss: 0.012541  [19200/60000]
loss: 0.006213  [25600/60000]
loss: 0.009592  [32000/60000]
loss: 0.003171  [38400/60000]
loss: 0.013499  [44800/60000]
loss: 0.008079  [51200/60000]
loss: 0.007955  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.087542 

Backprop time:
0.0009784904087756206
Final  epoch:
9 97.52 0.08754180966269623 tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[4, 97.46000000000001, 0.08034266955085739, tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)]
Fourier Signal Perceptron 512
Epoch 1
-------------------------------
loss: 2.371773  [    0/60000]
loss: 0.345417  [ 6400/60000]
loss: 0.250341  [12800/60000]
loss: 0.242105  [19200/60000]
loss: 0.151493  [25600/60000]
loss: 0.296074  [32000/60000]
loss: 0.105529  [38400/60000]
loss: 0.278070  [44800/60000]
loss: 0.245358  [51200/60000]
loss: 0.219940  [57600/60000]
Test Error: 
 Accuracy: 95.9%, Avg loss: 0.129388 

Epoch 2
-------------------------------
loss: 0.094472  [    0/60000]
loss: 0.137030  [ 6400/60000]
loss: 0.083859  [12800/60000]
loss: 0.081993  [19200/60000]
loss: 0.060664  [25600/60000]
loss: 0.124806  [32000/60000]
loss: 0.067336  [38400/60000]
loss: 0.091075  [44800/60000]
loss: 0.097827  [51200/60000]
loss: 0.104894  [57600/60000]
Test Error: 
 Accuracy: 97.4%, Avg loss: 0.081171 

Epoch 3
-------------------------------
loss: 0.039108  [    0/60000]
loss: 0.084344  [ 6400/60000]
loss: 0.055372  [12800/60000]
loss: 0.033241  [19200/60000]
loss: 0.054758  [25600/60000]
loss: 0.087799  [32000/60000]
loss: 0.043045  [38400/60000]
loss: 0.033571  [44800/60000]
loss: 0.067237  [51200/60000]
loss: 0.034338  [57600/60000]
Test Error: 
 Accuracy: 97.7%, Avg loss: 0.072626 

Epoch 4
-------------------------------
loss: 0.036091  [    0/60000]
loss: 0.033510  [ 6400/60000]
loss: 0.037910  [12800/60000]
loss: 0.020145  [19200/60000]
loss: 0.030144  [25600/60000]
loss: 0.045876  [32000/60000]
loss: 0.018672  [38400/60000]
loss: 0.028595  [44800/60000]
loss: 0.052566  [51200/60000]
loss: 0.020606  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.069462 

Epoch 5
-------------------------------
loss: 0.022885  [    0/60000]
loss: 0.009406  [ 6400/60000]
loss: 0.019550  [12800/60000]
loss: 0.012949  [19200/60000]
loss: 0.008990  [25600/60000]
loss: 0.030112  [32000/60000]
loss: 0.005664  [38400/60000]
loss: 0.016762  [44800/60000]
loss: 0.073446  [51200/60000]
loss: 0.012447  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.068227 

Epoch 6
-------------------------------
loss: 0.010519  [    0/60000]
loss: 0.004345  [ 6400/60000]
loss: 0.016938  [12800/60000]
loss: 0.014382  [19200/60000]
loss: 0.006529  [25600/60000]
loss: 0.006650  [32000/60000]
loss: 0.002054  [38400/60000]
loss: 0.017017  [44800/60000]
loss: 0.025846  [51200/60000]
loss: 0.017750  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.072248 

Epoch 7
-------------------------------
loss: 0.012443  [    0/60000]
loss: 0.018424  [ 6400/60000]
loss: 0.011957  [12800/60000]
loss: 0.020154  [19200/60000]
loss: 0.015570  [25600/60000]
loss: 0.025342  [32000/60000]
loss: 0.001485  [38400/60000]
loss: 0.018852  [44800/60000]
loss: 0.001922  [51200/60000]
loss: 0.003125  [57600/60000]
Test Error: 
 Accuracy: 98.0%, Avg loss: 0.072584 

Epoch 8
-------------------------------
loss: 0.003091  [    0/60000]
loss: 0.013213  [ 6400/60000]
loss: 0.027035  [12800/60000]
loss: 0.007408  [19200/60000]
loss: 0.000653  [25600/60000]
loss: 0.000965  [32000/60000]
loss: 0.002940  [38400/60000]
loss: 0.003168  [44800/60000]
loss: 0.038628  [51200/60000]
loss: 0.000693  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.075581 

Epoch 9
-------------------------------
loss: 0.001354  [    0/60000]
loss: 0.000579  [ 6400/60000]
loss: 0.003335  [12800/60000]
loss: 0.009512  [19200/60000]
loss: 0.006782  [25600/60000]
loss: 0.004959  [32000/60000]
loss: 0.022405  [38400/60000]
loss: 0.012157  [44800/60000]
loss: 0.005323  [51200/60000]
loss: 0.012726  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.086254 

Backprop time:
0.0009174908656301274
Final  epoch:
9 97.56 0.08625428546874436 tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[4, 97.89, 0.06822652585313359]
MLP 1 hidden layer Signal Perceptron
Epoch 1
-------------------------------
loss: 2.289313  [    0/60000]
loss: 0.614716  [ 6400/60000]
loss: 0.804799  [12800/60000]
loss: 0.735826  [19200/60000]
loss: 0.595491  [25600/60000]
loss: 0.745209  [32000/60000]
loss: 0.739118  [38400/60000]
loss: 0.589664  [44800/60000]
loss: 0.790142  [51200/60000]
loss: 0.960638  [57600/60000]
Test Error: 
 Accuracy: 77.3%, Avg loss: 0.586284 

Epoch 2
-------------------------------
loss: 0.420731  [    0/60000]
loss: 0.392106  [ 6400/60000]
loss: 0.652308  [12800/60000]
loss: 0.637517  [19200/60000]
loss: 0.526700  [25600/60000]
loss: 0.573162  [32000/60000]
loss: 0.672888  [38400/60000]
loss: 0.503617  [44800/60000]
loss: 0.657336  [51200/60000]
loss: 0.878089  [57600/60000]
Test Error: 
 Accuracy: 77.7%, Avg loss: 0.558136 

Epoch 3
-------------------------------
loss: 0.376968  [    0/60000]
loss: 0.347969  [ 6400/60000]
loss: 0.627815  [12800/60000]
loss: 0.627942  [19200/60000]
loss: 0.489735  [25600/60000]
loss: 0.529118  [32000/60000]
loss: 0.662574  [38400/60000]
loss: 0.477014  [44800/60000]
loss: 0.609366  [51200/60000]
loss: 0.837945  [57600/60000]
Test Error: 
 Accuracy: 78.1%, Avg loss: 0.540536 

Epoch 4
-------------------------------
loss: 0.350385  [    0/60000]
loss: 0.328190  [ 6400/60000]
loss: 0.587438  [12800/60000]
loss: 0.628342  [19200/60000]
loss: 0.478961  [25600/60000]
loss: 0.508361  [32000/60000]
loss: 0.642379  [38400/60000]
loss: 0.462909  [44800/60000]
loss: 0.592585  [51200/60000]
loss: 0.817412  [57600/60000]
Test Error: 
 Accuracy: 78.4%, Avg loss: 0.536516 

Epoch 5
-------------------------------
loss: 0.334082  [    0/60000]
loss: 0.312644  [ 6400/60000]
loss: 0.579011  [12800/60000]
loss: 0.588732  [19200/60000]
loss: 0.475077  [25600/60000]
loss: 0.510348  [32000/60000]
loss: 0.634752  [38400/60000]
loss: 0.445945  [44800/60000]
loss: 0.588187  [51200/60000]
loss: 0.809621  [57600/60000]
Test Error: 
 Accuracy: 78.4%, Avg loss: 0.538820 

Epoch 6
-------------------------------
loss: 0.334486  [    0/60000]
loss: 0.297584  [ 6400/60000]
loss: 0.558369  [12800/60000]
loss: 0.551467  [19200/60000]
loss: 0.456372  [25600/60000]
loss: 0.490117  [32000/60000]
loss: 0.622438  [38400/60000]
loss: 0.442033  [44800/60000]
loss: 0.572359  [51200/60000]
loss: 0.810951  [57600/60000]
Test Error: 
 Accuracy: 78.6%, Avg loss: 0.529143 

Epoch 7
-------------------------------
loss: 0.335431  [    0/60000]
loss: 0.292694  [ 6400/60000]
loss: 0.573780  [12800/60000]
loss: 0.538671  [19200/60000]
loss: 0.438583  [25600/60000]
loss: 0.485570  [32000/60000]
loss: 0.615722  [38400/60000]
loss: 0.418177  [44800/60000]
loss: 0.559941  [51200/60000]
loss: 0.796052  [57600/60000]
Test Error: 
 Accuracy: 78.5%, Avg loss: 0.532801 

Epoch 8
-------------------------------
loss: 0.337383  [    0/60000]
loss: 0.298053  [ 6400/60000]
loss: 0.546305  [12800/60000]
loss: 0.565873  [19200/60000]
loss: 0.437280  [25600/60000]
loss: 0.476765  [32000/60000]
loss: 0.614407  [38400/60000]
loss: 0.431156  [44800/60000]
loss: 0.548183  [51200/60000]
loss: 0.797272  [57600/60000]
Test Error: 
 Accuracy: 78.5%, Avg loss: 0.537289 

Epoch 9
-------------------------------
loss: 0.347458  [    0/60000]
loss: 0.308562  [ 6400/60000]
loss: 0.563101  [12800/60000]
loss: 0.541591  [19200/60000]
loss: 0.440676  [25600/60000]
loss: 0.477247  [32000/60000]
loss: 0.615414  [38400/60000]
loss: 0.420857  [44800/60000]
loss: 0.638588  [51200/60000]
loss: 0.799783  [57600/60000]
Test Error: 
 Accuracy: 78.6%, Avg loss: 0.533081 

Backprop time:
0.0012682663606428135
Final  epoch:
9 78.61 0.5330812591276352 tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 78.60000000000001, 0.5291434723860139, tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 2 hidden layer Signal Perceptron
Epoch 1
-------------------------------
loss: 2.304431  [    0/60000]
loss: 1.186033  [ 6400/60000]
loss: 1.400185  [12800/60000]
loss: 0.882029  [19200/60000]
loss: 0.863003  [25600/60000]
loss: 1.282814  [32000/60000]
loss: 1.275943  [38400/60000]
loss: 1.091485  [44800/60000]
loss: 1.102624  [51200/60000]
loss: 1.361578  [57600/60000]
Test Error: 
 Accuracy: 55.9%, Avg loss: 1.084853 

Epoch 2
-------------------------------
loss: 0.864169  [    0/60000]
loss: 1.001030  [ 6400/60000]
loss: 1.261678  [12800/60000]
loss: 0.804733  [19200/60000]
loss: 0.770749  [25600/60000]
loss: 1.102830  [32000/60000]
loss: 1.207482  [38400/60000]
loss: 1.039029  [44800/60000]
loss: 1.055252  [51200/60000]
loss: 1.286337  [57600/60000]
Test Error: 
 Accuracy: 57.7%, Avg loss: 1.016727 

Epoch 3
-------------------------------
loss: 0.817603  [    0/60000]
loss: 0.959582  [ 6400/60000]
loss: 1.215701  [12800/60000]
loss: 0.783494  [19200/60000]
loss: 0.849071  [25600/60000]
loss: 1.083025  [32000/60000]
loss: 1.194084  [38400/60000]
loss: 1.027997  [44800/60000]
loss: 1.007442  [51200/60000]
loss: 1.240081  [57600/60000]
Test Error: 
 Accuracy: 56.8%, Avg loss: 1.038070 

Epoch 4
-------------------------------
loss: 0.829987  [    0/60000]
loss: 1.000781  [ 6400/60000]
loss: 1.195006  [12800/60000]
loss: 0.775267  [19200/60000]
loss: 0.798876  [25600/60000]
loss: 1.055813  [32000/60000]
loss: 1.190359  [38400/60000]
loss: 1.037187  [44800/60000]
loss: 1.030769  [51200/60000]
loss: 1.230649  [57600/60000]
Test Error: 
 Accuracy: 58.2%, Avg loss: 1.005387 

Epoch 5
-------------------------------
loss: 0.849011  [    0/60000]
loss: 0.907238  [ 6400/60000]
loss: 1.167901  [12800/60000]
loss: 0.756646  [19200/60000]
loss: 0.773564  [25600/60000]
loss: 1.051641  [32000/60000]
loss: 0.868842  [38400/60000]
loss: 0.877862  [44800/60000]
loss: 0.723988  [51200/60000]
loss: 0.787520  [57600/60000]
Test Error: 
 Accuracy: 68.6%, Avg loss: 0.768426 

Epoch 6
-------------------------------
loss: 0.667581  [    0/60000]
loss: 0.767876  [ 6400/60000]
loss: 0.816306  [12800/60000]
loss: 0.574511  [19200/60000]
loss: 0.535602  [25600/60000]
loss: 0.846090  [32000/60000]
loss: 0.888284  [38400/60000]
loss: 0.845848  [44800/60000]
loss: 0.827365  [51200/60000]
loss: 0.705623  [57600/60000]
Test Error: 
 Accuracy: 68.5%, Avg loss: 0.766558 

Epoch 7
-------------------------------
loss: 0.617694  [    0/60000]
loss: 0.756580  [ 6400/60000]
loss: 0.750219  [12800/60000]
loss: 0.542179  [19200/60000]
loss: 0.521368  [25600/60000]
loss: 0.832115  [32000/60000]
loss: 0.872360  [38400/60000]
loss: 0.913172  [44800/60000]
loss: 0.797547  [51200/60000]
loss: 0.691575  [57600/60000]
Test Error: 
 Accuracy: 68.5%, Avg loss: 0.763947 

Epoch 8
-------------------------------
loss: 0.622152  [    0/60000]
loss: 0.769451  [ 6400/60000]
loss: 0.745939  [12800/60000]
loss: 0.539904  [19200/60000]
loss: 0.622863  [25600/60000]
loss: 0.867616  [32000/60000]
loss: 0.868765  [38400/60000]
loss: 0.851235  [44800/60000]
loss: 0.845883  [51200/60000]
loss: 0.702736  [57600/60000]
Test Error: 
 Accuracy: 68.6%, Avg loss: 0.769494 

Epoch 9
-------------------------------
loss: 0.617019  [    0/60000]
loss: 0.785262  [ 6400/60000]
loss: 0.731889  [12800/60000]
loss: 0.541897  [19200/60000]
loss: 0.503789  [25600/60000]
loss: 0.845377  [32000/60000]
loss: 0.863721  [38400/60000]
loss: 0.829844  [44800/60000]
loss: 0.736662  [51200/60000]
loss: 0.692202  [57600/60000]
Test Error: 
 Accuracy: 69.0%, Avg loss: 0.765873 

Backprop time:
0.0017228833986836795
Final  epoch:
9 68.95 0.7658725010741289 tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[6, 68.53, 0.7639468813398081, tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward>)]
Training models with FashionMNIST DATASET :
Fourier Signal Perceptron 128
Epoch 1
-------------------------------
loss: 2.560635  [    0/60000]
loss: 0.608317  [ 6400/60000]
loss: 0.406134  [12800/60000]
loss: 0.585037  [19200/60000]
loss: 0.441815  [25600/60000]
loss: 0.389762  [32000/60000]
loss: 0.409197  [38400/60000]
loss: 0.456128  [44800/60000]
loss: 0.513922  [51200/60000]
loss: 0.475270  [57600/60000]
Test Error: 
 Accuracy: 84.8%, Avg loss: 0.422905 

Epoch 2
-------------------------------
loss: 0.324120  [    0/60000]
loss: 0.390348  [ 6400/60000]
loss: 0.311018  [12800/60000]
loss: 0.436491  [19200/60000]
loss: 0.338776  [25600/60000]
loss: 0.331051  [32000/60000]
loss: 0.309693  [38400/60000]
loss: 0.432915  [44800/60000]
loss: 0.431881  [51200/60000]
loss: 0.419731  [57600/60000]
Test Error: 
 Accuracy: 85.5%, Avg loss: 0.397492 

Epoch 3
-------------------------------
loss: 0.248911  [    0/60000]
loss: 0.310995  [ 6400/60000]
loss: 0.267540  [12800/60000]
loss: 0.338671  [19200/60000]
loss: 0.246779  [25600/60000]
loss: 0.316369  [32000/60000]
loss: 0.323703  [38400/60000]
loss: 0.323587  [44800/60000]
loss: 0.368681  [51200/60000]
loss: 0.411692  [57600/60000]
Test Error: 
 Accuracy: 86.2%, Avg loss: 0.385037 

Epoch 4
-------------------------------
loss: 0.204667  [    0/60000]
loss: 0.276088  [ 6400/60000]
loss: 0.254628  [12800/60000]
loss: 0.301465  [19200/60000]
loss: 0.233991  [25600/60000]
loss: 0.299338  [32000/60000]
loss: 0.268260  [38400/60000]
loss: 0.300733  [44800/60000]
loss: 0.361391  [51200/60000]
loss: 0.396942  [57600/60000]
Test Error: 
 Accuracy: 86.0%, Avg loss: 0.389503 

Epoch 5
-------------------------------
loss: 0.192796  [    0/60000]
loss: 0.262683  [ 6400/60000]
loss: 0.273499  [12800/60000]
loss: 0.253844  [19200/60000]
loss: 0.220839  [25600/60000]
loss: 0.269427  [32000/60000]
loss: 0.279862  [38400/60000]
loss: 0.272196  [44800/60000]
loss: 0.350553  [51200/60000]
loss: 0.369978  [57600/60000]
Test Error: 
 Accuracy: 86.3%, Avg loss: 0.389149 

Epoch 6
-------------------------------
loss: 0.155006  [    0/60000]
loss: 0.274280  [ 6400/60000]
loss: 0.257254  [12800/60000]
loss: 0.224273  [19200/60000]
loss: 0.207913  [25600/60000]
loss: 0.244965  [32000/60000]
loss: 0.271032  [38400/60000]
loss: 0.250932  [44800/60000]
loss: 0.336548  [51200/60000]
loss: 0.387117  [57600/60000]
Test Error: 
 Accuracy: 86.3%, Avg loss: 0.394543 

Epoch 7
-------------------------------
loss: 0.150394  [    0/60000]
loss: 0.259539  [ 6400/60000]
loss: 0.234424  [12800/60000]
loss: 0.218438  [19200/60000]
loss: 0.215077  [25600/60000]
loss: 0.256942  [32000/60000]
loss: 0.253033  [38400/60000]
loss: 0.242821  [44800/60000]
loss: 0.302083  [51200/60000]
loss: 0.365593  [57600/60000]
Test Error: 
 Accuracy: 86.5%, Avg loss: 0.396773 

Epoch 8
-------------------------------
loss: 0.152388  [    0/60000]
loss: 0.263673  [ 6400/60000]
loss: 0.257564  [12800/60000]
loss: 0.189475  [19200/60000]
loss: 0.192799  [25600/60000]
loss: 0.250495  [32000/60000]
loss: 0.242384  [38400/60000]
loss: 0.264397  [44800/60000]
loss: 0.266723  [51200/60000]
loss: 0.317369  [57600/60000]
Test Error: 
 Accuracy: 86.6%, Avg loss: 0.396931 

Epoch 9
-------------------------------
loss: 0.147342  [    0/60000]
loss: 0.252090  [ 6400/60000]
loss: 0.278817  [12800/60000]
loss: 0.178243  [19200/60000]
loss: 0.205903  [25600/60000]
loss: 0.266285  [32000/60000]
loss: 0.253723  [38400/60000]
loss: 0.270028  [44800/60000]
loss: 0.273517  [51200/60000]
loss: 0.313877  [57600/60000]
Test Error: 
 Accuracy: 86.6%, Avg loss: 0.398763 

Backprop time:
0.0009192233400812536
Final  epoch:
9 86.57000000000001 0.39876303219111864 tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[2, 86.2, 0.3850373470099868, tensor(0.1761, device='cuda:0', grad_fn=<NllLossBackward>)]
Fourier Signal Perceptron 512
Epoch 1
-------------------------------
loss: 2.423352  [    0/60000]
loss: 0.565502  [ 6400/60000]
loss: 0.431770  [12800/60000]
loss: 0.553118  [19200/60000]
loss: 0.416793  [25600/60000]
loss: 0.383994  [32000/60000]
loss: 0.383634  [38400/60000]
loss: 0.498305  [44800/60000]
loss: 0.503759  [51200/60000]
loss: 0.553832  [57600/60000]
Test Error: 
 Accuracy: 84.3%, Avg loss: 0.428006 

Epoch 2
-------------------------------
loss: 0.239793  [    0/60000]
loss: 0.379049  [ 6400/60000]
loss: 0.303961  [12800/60000]
loss: 0.393202  [19200/60000]
loss: 0.367775  [25600/60000]
loss: 0.324932  [32000/60000]
loss: 0.270138  [38400/60000]
loss: 0.381316  [44800/60000]
loss: 0.379887  [51200/60000]
loss: 0.451404  [57600/60000]
Test Error: 
 Accuracy: 85.7%, Avg loss: 0.400389 

Epoch 3
-------------------------------
loss: 0.209159  [    0/60000]
loss: 0.328589  [ 6400/60000]
loss: 0.262220  [12800/60000]
loss: 0.320646  [19200/60000]
loss: 0.375879  [25600/60000]
loss: 0.333762  [32000/60000]
loss: 0.226830  [38400/60000]
loss: 0.329184  [44800/60000]
loss: 0.338786  [51200/60000]
loss: 0.408391  [57600/60000]
Test Error: 
 Accuracy: 86.0%, Avg loss: 0.397449 

Epoch 4
-------------------------------
loss: 0.196641  [    0/60000]
loss: 0.268989  [ 6400/60000]
loss: 0.221599  [12800/60000]
loss: 0.247412  [19200/60000]
loss: 0.360865  [25600/60000]
loss: 0.317427  [32000/60000]
loss: 0.227929  [38400/60000]
loss: 0.345714  [44800/60000]
loss: 0.377412  [51200/60000]
loss: 0.346617  [57600/60000]
Test Error: 
 Accuracy: 86.2%, Avg loss: 0.400412 

Epoch 5
-------------------------------
loss: 0.172416  [    0/60000]
loss: 0.263991  [ 6400/60000]
loss: 0.203545  [12800/60000]
loss: 0.217480  [19200/60000]
loss: 0.327609  [25600/60000]
loss: 0.337083  [32000/60000]
loss: 0.211065  [38400/60000]
loss: 0.311486  [44800/60000]
loss: 0.314826  [51200/60000]
loss: 0.372327  [57600/60000]
Test Error: 
 Accuracy: 86.2%, Avg loss: 0.400504 

Epoch 6
-------------------------------
loss: 0.165543  [    0/60000]
loss: 0.267313  [ 6400/60000]
loss: 0.202974  [12800/60000]
loss: 0.223119  [19200/60000]
loss: 0.245381  [25600/60000]
loss: 0.297047  [32000/60000]
loss: 0.227382  [38400/60000]
loss: 0.326368  [44800/60000]
loss: 0.266195  [51200/60000]
loss: 0.324668  [57600/60000]
Test Error: 
 Accuracy: 86.4%, Avg loss: 0.403716 

Epoch 7
-------------------------------
loss: 0.163661  [    0/60000]
loss: 0.208043  [ 6400/60000]
loss: 0.253397  [12800/60000]
loss: 0.169061  [19200/60000]
loss: 0.279891  [25600/60000]
loss: 0.314846  [32000/60000]
loss: 0.214462  [38400/60000]
loss: 0.317211  [44800/60000]
loss: 0.273167  [51200/60000]
loss: 0.327563  [57600/60000]
Test Error: 
 Accuracy: 86.1%, Avg loss: 0.417174 

Epoch 8
-------------------------------
loss: 0.138747  [    0/60000]
loss: 0.216525  [ 6400/60000]
loss: 0.211011  [12800/60000]
loss: 0.137918  [19200/60000]
loss: 0.248306  [25600/60000]
loss: 0.319569  [32000/60000]
loss: 0.265008  [38400/60000]
loss: 0.236287  [44800/60000]
loss: 0.234719  [51200/60000]
loss: 0.306028  [57600/60000]
Test Error: 
 Accuracy: 86.1%, Avg loss: 0.419749 

Epoch 9
-------------------------------
loss: 0.167796  [    0/60000]
loss: 0.241282  [ 6400/60000]
loss: 0.295242  [12800/60000]
loss: 0.188366  [19200/60000]
loss: 0.268445  [25600/60000]
loss: 0.305234  [32000/60000]
loss: 0.251065  [38400/60000]
loss: 0.239376  [44800/60000]
loss: 0.225952  [51200/60000]
loss: 0.333876  [57600/60000]
Test Error: 
 Accuracy: 86.1%, Avg loss: 0.431492 

Backprop time:
0.0009160936231778093
Final  epoch:
9 86.08 0.4314917127607734 tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[2, 85.96000000000001, 0.397448780620174, tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 1 hidden layer
Epoch 1
-------------------------------
loss: 2.322956  [    0/60000]
loss: 1.053821  [ 6400/60000]
loss: 0.893440  [12800/60000]
loss: 1.342631  [19200/60000]
loss: 1.179129  [25600/60000]
loss: 0.900927  [32000/60000]
loss: 1.227339  [38400/60000]
loss: 1.067469  [44800/60000]
loss: 1.011452  [51200/60000]
loss: 1.328890  [57600/60000]
Test Error: 
 Accuracy: 57.9%, Avg loss: 1.074929 

Epoch 2
-------------------------------
loss: 0.632814  [    0/60000]
loss: 0.789986  [ 6400/60000]
loss: 0.814254  [12800/60000]
loss: 1.262047  [19200/60000]
loss: 1.068061  [25600/60000]
loss: 0.836415  [32000/60000]
loss: 1.169593  [38400/60000]
loss: 0.982326  [44800/60000]
loss: 0.931722  [51200/60000]
loss: 1.224307  [57600/60000]
Test Error: 
 Accuracy: 58.8%, Avg loss: 1.038641 

Epoch 3
-------------------------------
loss: 0.595745  [    0/60000]
loss: 0.764148  [ 6400/60000]
loss: 0.774256  [12800/60000]
loss: 1.195456  [19200/60000]
loss: 1.016373  [25600/60000]
loss: 0.666044  [32000/60000]
loss: 0.889141  [38400/60000]
loss: 0.756052  [44800/60000]
loss: 0.726776  [51200/60000]
loss: 0.911071  [57600/60000]
Test Error: 
 Accuracy: 68.3%, Avg loss: 0.807588 

Epoch 4
-------------------------------
loss: 0.489031  [    0/60000]
loss: 0.636478  [ 6400/60000]
loss: 0.570937  [12800/60000]
loss: 0.865147  [19200/60000]
loss: 0.825311  [25600/60000]
loss: 0.657688  [32000/60000]
loss: 0.848231  [38400/60000]
loss: 0.716992  [44800/60000]
loss: 0.707349  [51200/60000]
loss: 0.944156  [57600/60000]
Test Error: 
 Accuracy: 68.8%, Avg loss: 0.797099 

Epoch 5
-------------------------------
loss: 0.496327  [    0/60000]
loss: 0.630544  [ 6400/60000]
loss: 0.564968  [12800/60000]
loss: 0.850968  [19200/60000]
loss: 0.817178  [25600/60000]
loss: 0.651464  [32000/60000]
loss: 0.856674  [38400/60000]
loss: 0.708753  [44800/60000]
loss: 0.677688  [51200/60000]
loss: 0.892618  [57600/60000]
Test Error: 
 Accuracy: 68.4%, Avg loss: 0.801470 

Epoch 6
-------------------------------
loss: 0.492445  [    0/60000]
loss: 0.633528  [ 6400/60000]
loss: 0.573458  [12800/60000]
loss: 0.814156  [19200/60000]
loss: 0.827816  [25600/60000]
loss: 0.626110  [32000/60000]
loss: 0.839317  [38400/60000]
loss: 0.704192  [44800/60000]
loss: 0.666438  [51200/60000]
loss: 0.856897  [57600/60000]
Test Error: 
 Accuracy: 68.6%, Avg loss: 0.799958 

Epoch 7
-------------------------------
loss: 0.465995  [    0/60000]
loss: 0.605127  [ 6400/60000]
loss: 0.564642  [12800/60000]
loss: 0.818678  [19200/60000]
loss: 0.851085  [25600/60000]
loss: 0.608611  [32000/60000]
loss: 0.834848  [38400/60000]
loss: 0.664405  [44800/60000]
loss: 0.659670  [51200/60000]
loss: 0.826322  [57600/60000]
Test Error: 
 Accuracy: 68.5%, Avg loss: 0.804130 

Epoch 8
-------------------------------
loss: 0.478695  [    0/60000]
loss: 0.601138  [ 6400/60000]
loss: 0.582896  [12800/60000]
loss: 0.812860  [19200/60000]
loss: 0.812106  [25600/60000]
loss: 0.610165  [32000/60000]
loss: 0.832647  [38400/60000]
loss: 0.677191  [44800/60000]
loss: 0.628150  [51200/60000]
loss: 0.820341  [57600/60000]
Test Error: 
 Accuracy: 68.6%, Avg loss: 0.811426 

Epoch 9
-------------------------------
loss: 0.460995  [    0/60000]
loss: 0.601984  [ 6400/60000]
loss: 0.560832  [12800/60000]
loss: 0.800288  [19200/60000]
loss: 0.806862  [25600/60000]
loss: 0.599911  [32000/60000]
loss: 0.820800  [38400/60000]
loss: 0.631779  [44800/60000]
loss: 0.620626  [51200/60000]
loss: 0.801211  [57600/60000]
Test Error: 
 Accuracy: 68.6%, Avg loss: 0.814225 

Backprop time:
0.0012736206398109666
Final  epoch:
9 68.57 0.8142246214827155 tensor(0.3987, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[3, 68.76, 0.7970988106955389, tensor(0.4058, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 2 hidden layer
Epoch 1
-------------------------------
loss: 2.305735  [    0/60000]
loss: 1.312259  [ 6400/60000]
loss: 1.417898  [12800/60000]
loss: 1.693990  [19200/60000]
loss: 1.244769  [25600/60000]
loss: 1.522446  [32000/60000]
loss: 1.233102  [38400/60000]
loss: 1.369033  [44800/60000]
loss: 1.397799  [51200/60000]
loss: 1.445906  [57600/60000]
Test Error: 
 Accuracy: 44.5%, Avg loss: 1.376826 

Epoch 2
-------------------------------
loss: 1.174492  [    0/60000]
loss: 1.133644  [ 6400/60000]
loss: 1.368971  [12800/60000]
loss: 1.642391  [19200/60000]
loss: 1.156671  [25600/60000]
loss: 1.472426  [32000/60000]
loss: 1.206676  [38400/60000]
loss: 1.338058  [44800/60000]
loss: 1.371603  [51200/60000]
loss: 1.373133  [57600/60000]
Test Error: 
 Accuracy: 45.0%, Avg loss: 1.354389 

Epoch 3
-------------------------------
loss: 1.138164  [    0/60000]
loss: 1.131155  [ 6400/60000]
loss: 1.347564  [12800/60000]
loss: 1.570860  [19200/60000]
loss: 1.141675  [25600/60000]
loss: 1.450028  [32000/60000]
loss: 1.184288  [38400/60000]
loss: 1.332490  [44800/60000]
loss: 1.357061  [51200/60000]
loss: 1.356804  [57600/60000]
Test Error: 
 Accuracy: 45.2%, Avg loss: 1.344754 

Epoch 4
-------------------------------
loss: 1.130572  [    0/60000]
loss: 1.120623  [ 6400/60000]
loss: 1.309696  [12800/60000]
loss: 1.532525  [19200/60000]
loss: 1.126212  [25600/60000]
loss: 1.452251  [32000/60000]
loss: 1.170863  [38400/60000]
loss: 1.309528  [44800/60000]
loss: 1.331819  [51200/60000]
loss: 1.310728  [57600/60000]
Test Error: 
 Accuracy: 45.4%, Avg loss: 1.340506 

Epoch 5
-------------------------------
loss: 1.118900  [    0/60000]
loss: 1.122281  [ 6400/60000]
loss: 1.287794  [12800/60000]
loss: 1.532632  [19200/60000]
loss: 1.173885  [25600/60000]
loss: 1.437769  [32000/60000]
loss: 1.181263  [38400/60000]
loss: 1.312473  [44800/60000]
loss: 1.338369  [51200/60000]
loss: 1.291699  [57600/60000]
Test Error: 
 Accuracy: 45.2%, Avg loss: 1.350948 

Epoch 6
-------------------------------
loss: 1.118053  [    0/60000]
loss: 1.115525  [ 6400/60000]
loss: 1.314582  [12800/60000]
loss: 1.507964  [19200/60000]
loss: 1.101263  [25600/60000]
loss: 1.428557  [32000/60000]
loss: 1.169894  [38400/60000]
loss: 1.310637  [44800/60000]
loss: 1.322332  [51200/60000]
loss: 1.280512  [57600/60000]
Test Error: 
 Accuracy: 45.5%, Avg loss: 1.346497 

Epoch 7
-------------------------------
loss: 1.106819  [    0/60000]
loss: 1.119849  [ 6400/60000]
loss: 1.321888  [12800/60000]
loss: 1.497591  [19200/60000]
loss: 1.157744  [25600/60000]
loss: 1.406290  [32000/60000]
loss: 1.171696  [38400/60000]
loss: 1.313307  [44800/60000]
loss: 1.307970  [51200/60000]
loss: 1.255801  [57600/60000]
Test Error: 
 Accuracy: 45.3%, Avg loss: 1.345135 

Epoch 8
-------------------------------
loss: 1.095388  [    0/60000]
loss: 1.118382  [ 6400/60000]
loss: 1.334425  [12800/60000]
loss: 1.498676  [19200/60000]
loss: 1.158833  [25600/60000]
loss: 1.398130  [32000/60000]
loss: 1.166963  [38400/60000]
loss: 1.297817  [44800/60000]
loss: 1.280392  [51200/60000]
loss: 1.228500  [57600/60000]
Test Error: 
 Accuracy: 45.3%, Avg loss: 1.348750 

Epoch 9
-------------------------------
loss: 1.099462  [    0/60000]
loss: 1.120960  [ 6400/60000]
loss: 1.325197  [12800/60000]
loss: 1.484994  [19200/60000]
loss: 1.191666  [25600/60000]
loss: 1.411378  [32000/60000]
loss: 1.202407  [38400/60000]
loss: 1.306337  [44800/60000]
loss: 1.288354  [51200/60000]
loss: 1.203905  [57600/60000]
Test Error: 
 Accuracy: 45.6%, Avg loss: 1.349951 

Backprop time:
0.0017560476715659057
Final  epoch:
9 45.61 1.3499505474309252 tensor(1.2840, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[3, 45.410000000000004, 1.3405055092398528, tensor(1.3267, device='cuda:0', grad_fn=<NllLossBackward>)]
