Using cuda device
Learnable Parameters for MNIST models:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
101632 	 406528 	 407050 	 669706
torch.Size([64, 1, 28, 28])
torch.Size([1, 28, 28])
Forward time for MNIST models:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
0.0001246929168701172 	 7.152557373046875e-05 	 0.00010466575622558594 	 0.0001575946807861328
Forward time for MNIST models FS Timer class:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
6.830120086669922e-05 	 6.919503211975098e-05 	 0.00010959744453430176 	 0.00015752935409545898
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                   model_inference mlp1         5.74%     130.000us        99.16%       2.247ms       2.247ms       0.000us         0.00%      18.000us      18.000us             1  
                                           aten::linear         0.79%      18.000us        87.42%       1.981ms     990.500us       0.000us         0.00%      16.000us       8.000us             2  
                                            aten::addmm        11.65%     264.000us        85.48%       1.937ms     968.500us      16.000us        88.89%      16.000us       8.000us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      15.000us        83.33%      15.000us       7.500us             2  
                                        aten::clamp_min         1.99%      45.000us         6.80%     154.000us      38.500us       2.000us        11.11%       4.000us       1.000us             4  
                                             aten::relu         1.28%      29.000us         5.30%     120.000us      60.000us       0.000us         0.00%       2.000us       1.000us             2  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us        11.11%       2.000us       1.000us             2  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         5.56%       1.000us       0.500us             2  
                                            aten::zeros         0.31%       7.000us         0.62%      14.000us      14.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         0.88%      20.000us         0.88%      20.000us       5.000us       0.000us         0.00%       0.000us       0.000us             4  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.266ms
Self CUDA time total: 18.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                   model_inference mlp2         6.15%     150.000us        99.18%       2.418ms       2.418ms       0.000us         0.00%      27.000us      27.000us             1  
                                           aten::linear         1.48%      36.000us        85.85%       2.093ms     697.667us       0.000us         0.00%      25.000us       8.333us             3  
                                            aten::addmm        11.40%     278.000us        82.94%       2.022ms     674.000us      25.000us        92.59%      25.000us       8.333us             3  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      23.000us        85.19%      23.000us       7.667us             3  
                                        aten::clamp_min         2.46%      60.000us         8.65%     211.000us      35.167us       2.000us         7.41%       4.000us       0.667us             6  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         7.41%       2.000us       0.667us             3  
                                             aten::relu         1.44%      35.000us         6.48%     158.000us      52.667us       0.000us         0.00%       2.000us       0.667us             3  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         7.41%       2.000us       0.667us             3  
                                            aten::zeros         0.29%       7.000us         0.62%      15.000us      15.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         0.98%      24.000us         0.98%      24.000us       4.800us       0.000us         0.00%       0.000us       0.000us             5  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.438ms
Self CUDA time total: 27.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                 model_inference fsp128         5.18%     115.000us        99.10%       2.202ms       2.202ms       0.000us         0.00%       7.000us       7.000us             1  
                                           aten::linear         0.72%      16.000us        90.86%       2.019ms       1.010ms       0.000us         0.00%       6.000us       3.000us             2  
                                           aten::matmul         0.68%      15.000us        88.79%       1.973ms     986.500us       0.000us         0.00%       6.000us       3.000us             2  
                                               aten::mm        10.13%     225.000us        88.12%       1.958ms     979.000us       6.000us        85.71%       6.000us       3.000us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us        85.71%       6.000us       3.000us             2  
                                              aten::cos         1.67%      37.000us         2.34%      52.000us      52.000us       1.000us        14.29%       1.000us       1.000us             1  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us        14.29%       1.000us       1.000us             1  
                                            aten::zeros         0.32%       7.000us         0.68%      15.000us      15.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         0.99%      22.000us         0.99%      22.000us       5.500us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.09%       2.000us         0.09%       2.000us       2.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.222ms
Self CUDA time total: 7.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                 model_inference fsp512         4.99%     114.000us        99.08%       2.265ms       2.265ms       0.000us         0.00%      14.000us      14.000us             1  
                                           aten::linear         0.70%      16.000us        91.16%       2.084ms       1.042ms       0.000us         0.00%      13.000us       6.500us             2  
                                           aten::matmul         0.52%      12.000us        89.20%       2.039ms       1.020ms       0.000us         0.00%      13.000us       6.500us             2  
                                               aten::mm         9.89%     226.000us        88.67%       2.027ms       1.014ms      13.000us        92.86%      13.000us       6.500us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      13.000us        92.86%      13.000us       6.500us             2  
                                              aten::cos         1.66%      38.000us         2.27%      52.000us      52.000us       1.000us         7.14%       1.000us       1.000us             1  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         7.14%       1.000us       1.000us             1  
                                            aten::zeros         0.39%       9.000us         0.70%      16.000us      16.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         0.96%      22.000us         0.96%      22.000us       5.500us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.04%       1.000us         0.04%       1.000us       1.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.286ms
Self CUDA time total: 14.000us

Optimizer: Adam ,lr: 0.001
Training models with MNIST DATASET :
Fourier Signal Perceptron 128
Epoch 1
-------------------------------
loss: 2.410483  [    0/60000]
loss: 0.458883  [ 6400/60000]
loss: 0.333943  [12800/60000]
loss: 0.354864  [19200/60000]
loss: 0.210382  [25600/60000]
loss: 0.278579  [32000/60000]
loss: 0.121176  [38400/60000]
loss: 0.264160  [44800/60000]
loss: 0.210281  [51200/60000]
loss: 0.233101  [57600/60000]
Test Error: 
 Accuracy: 95.2%, Avg loss: 0.161177 

Epoch 2
-------------------------------
loss: 0.123039  [    0/60000]
loss: 0.154953  [ 6400/60000]
loss: 0.128360  [12800/60000]
loss: 0.141205  [19200/60000]
loss: 0.127972  [25600/60000]
loss: 0.185592  [32000/60000]
loss: 0.078101  [38400/60000]
loss: 0.150843  [44800/60000]
loss: 0.127950  [51200/60000]
loss: 0.132550  [57600/60000]
Test Error: 
 Accuracy: 96.8%, Avg loss: 0.105415 

Epoch 3
-------------------------------
loss: 0.070902  [    0/60000]
loss: 0.112520  [ 6400/60000]
loss: 0.087156  [12800/60000]
loss: 0.065632  [19200/60000]
loss: 0.079088  [25600/60000]
loss: 0.120550  [32000/60000]
loss: 0.070196  [38400/60000]
loss: 0.085469  [44800/60000]
loss: 0.097580  [51200/60000]
loss: 0.059864  [57600/60000]
Test Error: 
 Accuracy: 97.2%, Avg loss: 0.086319 

Epoch 4
-------------------------------
loss: 0.049376  [    0/60000]
loss: 0.082471  [ 6400/60000]
loss: 0.062954  [12800/60000]
loss: 0.039017  [19200/60000]
loss: 0.052844  [25600/60000]
loss: 0.088731  [32000/60000]
loss: 0.054941  [38400/60000]
loss: 0.038636  [44800/60000]
loss: 0.075299  [51200/60000]
loss: 0.022576  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.077338 

Epoch 5
-------------------------------
loss: 0.035617  [    0/60000]
loss: 0.047108  [ 6400/60000]
loss: 0.047403  [12800/60000]
loss: 0.029577  [19200/60000]
loss: 0.039621  [25600/60000]
loss: 0.067685  [32000/60000]
loss: 0.036027  [38400/60000]
loss: 0.021392  [44800/60000]
loss: 0.058537  [51200/60000]
loss: 0.014319  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.073241 

Epoch 6
-------------------------------
loss: 0.030744  [    0/60000]
loss: 0.019818  [ 6400/60000]
loss: 0.037660  [12800/60000]
loss: 0.022381  [19200/60000]
loss: 0.024909  [25600/60000]
loss: 0.044787  [32000/60000]
loss: 0.016367  [38400/60000]
loss: 0.014137  [44800/60000]
loss: 0.047072  [51200/60000]
loss: 0.010599  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.072375 

Epoch 7
-------------------------------
loss: 0.021319  [    0/60000]
loss: 0.010064  [ 6400/60000]
loss: 0.029157  [12800/60000]
loss: 0.017997  [19200/60000]
loss: 0.016275  [25600/60000]
loss: 0.028915  [32000/60000]
loss: 0.008486  [38400/60000]
loss: 0.009534  [44800/60000]
loss: 0.033250  [51200/60000]
loss: 0.007374  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.073595 

Epoch 8
-------------------------------
loss: 0.014713  [    0/60000]
loss: 0.009212  [ 6400/60000]
loss: 0.024646  [12800/60000]
loss: 0.016890  [19200/60000]
loss: 0.010526  [25600/60000]
loss: 0.019316  [32000/60000]
loss: 0.004172  [38400/60000]
loss: 0.006429  [44800/60000]
loss: 0.023891  [51200/60000]
loss: 0.005006  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.075704 

Epoch 9
-------------------------------
loss: 0.014436  [    0/60000]
loss: 0.008255  [ 6400/60000]
loss: 0.017703  [12800/60000]
loss: 0.016009  [19200/60000]
loss: 0.005702  [25600/60000]
loss: 0.011194  [32000/60000]
loss: 0.003342  [38400/60000]
loss: 0.004344  [44800/60000]
loss: 0.024617  [51200/60000]
loss: 0.014311  [57600/60000]
Test Error: 
 Accuracy: 95.1%, Avg loss: 0.157749 

Backprop time:
0.0009695291519165039
Final  epoch:
9 95.14 0.15774924468410728 tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 97.64, 0.07237504243258992, tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)]
Fourier Signal Perceptron 512
Epoch 1
-------------------------------
loss: 2.399588  [    0/60000]
loss: 0.347952  [ 6400/60000]
loss: 0.236509  [12800/60000]
loss: 0.245024  [19200/60000]
loss: 0.152491  [25600/60000]
loss: 0.281798  [32000/60000]
loss: 0.100930  [38400/60000]
loss: 0.258571  [44800/60000]
loss: 0.221645  [51200/60000]
loss: 0.201907  [57600/60000]
Test Error: 
 Accuracy: 95.8%, Avg loss: 0.131226 

Epoch 2
-------------------------------
loss: 0.097855  [    0/60000]
loss: 0.137808  [ 6400/60000]
loss: 0.081503  [12800/60000]
loss: 0.078028  [19200/60000]
loss: 0.078817  [25600/60000]
loss: 0.130607  [32000/60000]
loss: 0.069604  [38400/60000]
loss: 0.068182  [44800/60000]
loss: 0.103143  [51200/60000]
loss: 0.093343  [57600/60000]
Test Error: 
 Accuracy: 97.1%, Avg loss: 0.083302 

Epoch 3
-------------------------------
loss: 0.041211  [    0/60000]
loss: 0.061063  [ 6400/60000]
loss: 0.053812  [12800/60000]
loss: 0.036908  [19200/60000]
loss: 0.062903  [25600/60000]
loss: 0.075234  [32000/60000]
loss: 0.053485  [38400/60000]
loss: 0.032399  [44800/60000]
loss: 0.074211  [51200/60000]
loss: 0.027602  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.072630 

Epoch 4
-------------------------------
loss: 0.031828  [    0/60000]
loss: 0.026384  [ 6400/60000]
loss: 0.029108  [12800/60000]
loss: 0.024575  [19200/60000]
loss: 0.028809  [25600/60000]
loss: 0.044636  [32000/60000]
loss: 0.012134  [38400/60000]
loss: 0.027388  [44800/60000]
loss: 0.073955  [51200/60000]
loss: 0.008733  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.068213 

Epoch 5
-------------------------------
loss: 0.024045  [    0/60000]
loss: 0.006146  [ 6400/60000]
loss: 0.021511  [12800/60000]
loss: 0.010157  [19200/60000]
loss: 0.010504  [25600/60000]
loss: 0.028446  [32000/60000]
loss: 0.010330  [38400/60000]
loss: 0.019763  [44800/60000]
loss: 0.052508  [51200/60000]
loss: 0.006172  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.067972 

Epoch 6
-------------------------------
loss: 0.009825  [    0/60000]
loss: 0.009147  [ 6400/60000]
loss: 0.014572  [12800/60000]
loss: 0.016516  [19200/60000]
loss: 0.006603  [25600/60000]
loss: 0.005678  [32000/60000]
loss: 0.002590  [38400/60000]
loss: 0.010561  [44800/60000]
loss: 0.012998  [51200/60000]
loss: 0.002464  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.068988 

Epoch 7
-------------------------------
loss: 0.005134  [    0/60000]
loss: 0.002991  [ 6400/60000]
loss: 0.023260  [12800/60000]
loss: 0.006882  [19200/60000]
loss: 0.003267  [25600/60000]
loss: 0.003053  [32000/60000]
loss: 0.001103  [38400/60000]
loss: 0.006464  [44800/60000]
loss: 0.077125  [51200/60000]
loss: 0.002782  [57600/60000]
Test Error: 
 Accuracy: 97.7%, Avg loss: 0.069469 

Epoch 8
-------------------------------
loss: 0.014759  [    0/60000]
loss: 0.042920  [ 6400/60000]
loss: 0.011804  [12800/60000]
loss: 0.073920  [19200/60000]
loss: 0.015237  [25600/60000]
loss: 0.007207  [32000/60000]
loss: 0.009155  [38400/60000]
loss: 0.013102  [44800/60000]
loss: 0.008741  [51200/60000]
loss: 0.005836  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.077396 

Epoch 9
-------------------------------
loss: 0.003894  [    0/60000]
loss: 0.003110  [ 6400/60000]
loss: 0.013928  [12800/60000]
loss: 0.002482  [19200/60000]
loss: 0.002060  [25600/60000]
loss: 0.048154  [32000/60000]
loss: 0.011714  [38400/60000]
loss: 0.003085  [44800/60000]
loss: 0.012275  [51200/60000]
loss: 0.004109  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.073927 

Backprop time:
0.0008964749541618168
Final  epoch:
9 97.82 0.07392717749734733 tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[4, 97.85000000000001, 0.06797248808557507]
MLP 1 hidden layer Signal Perceptron
Epoch 1
-------------------------------
loss: 2.305820  [    0/60000]
loss: 1.275836  [ 6400/60000]
loss: 0.835302  [12800/60000]
loss: 1.286151  [19200/60000]
loss: 1.167093  [25600/60000]
loss: 1.197740  [32000/60000]
loss: 0.875625  [38400/60000]
loss: 0.948190  [44800/60000]
loss: 1.134341  [51200/60000]
loss: 0.857512  [57600/60000]
Test Error: 
 Accuracy: 67.4%, Avg loss: 1.030906 

Epoch 2
-------------------------------
loss: 1.140709  [    0/60000]
loss: 1.197955  [ 6400/60000]
loss: 0.702153  [12800/60000]
loss: 1.153240  [19200/60000]
loss: 1.112618  [25600/60000]
loss: 1.140592  [32000/60000]
loss: 0.843279  [38400/60000]
loss: 0.896339  [44800/60000]
loss: 1.062511  [51200/60000]
loss: 0.832695  [57600/60000]
Test Error: 
 Accuracy: 68.1%, Avg loss: 0.996350 

Epoch 3
-------------------------------
loss: 1.110621  [    0/60000]
loss: 1.187633  [ 6400/60000]
loss: 0.690257  [12800/60000]
loss: 1.150692  [19200/60000]
loss: 1.076099  [25600/60000]
loss: 1.060484  [32000/60000]
loss: 0.839028  [38400/60000]
loss: 0.841946  [44800/60000]
loss: 1.030329  [51200/60000]
loss: 0.782162  [57600/60000]
Test Error: 
 Accuracy: 68.8%, Avg loss: 0.975398 

Epoch 4
-------------------------------
loss: 1.081157  [    0/60000]
loss: 1.173432  [ 6400/60000]
loss: 0.698076  [12800/60000]
loss: 1.148260  [19200/60000]
loss: 1.051781  [25600/60000]
loss: 1.018054  [32000/60000]
loss: 0.832585  [38400/60000]
loss: 0.807626  [44800/60000]
loss: 1.012697  [51200/60000]
loss: 0.747439  [57600/60000]
Test Error: 
 Accuracy: 69.0%, Avg loss: 0.970134 

Epoch 5
-------------------------------
loss: 1.069040  [    0/60000]
loss: 1.160542  [ 6400/60000]
loss: 0.714173  [12800/60000]
loss: 1.176263  [19200/60000]
loss: 1.051126  [25600/60000]
loss: 0.988288  [32000/60000]
loss: 0.831767  [38400/60000]
loss: 0.796630  [44800/60000]
loss: 0.993941  [51200/60000]
loss: 0.730751  [57600/60000]
Test Error: 
 Accuracy: 68.9%, Avg loss: 0.975392 

Epoch 6
-------------------------------
loss: 1.064942  [    0/60000]
loss: 1.156009  [ 6400/60000]
loss: 0.710738  [12800/60000]
loss: 1.119728  [19200/60000]
loss: 1.082253  [25600/60000]
loss: 0.984964  [32000/60000]
loss: 0.829506  [38400/60000]
loss: 0.771352  [44800/60000]
loss: 0.995072  [51200/60000]
loss: 0.695889  [57600/60000]
Test Error: 
 Accuracy: 68.9%, Avg loss: 0.974724 

Epoch 7
-------------------------------
loss: 1.054313  [    0/60000]
loss: 1.187147  [ 6400/60000]
loss: 0.700188  [12800/60000]
loss: 1.104578  [19200/60000]
loss: 1.015065  [25600/60000]
loss: 0.951622  [32000/60000]
loss: 0.828864  [38400/60000]
loss: 0.785649  [44800/60000]
loss: 0.983484  [51200/60000]
loss: 0.694955  [57600/60000]
Test Error: 
 Accuracy: 69.0%, Avg loss: 0.972007 

Epoch 8
-------------------------------
loss: 1.049006  [    0/60000]
loss: 1.156854  [ 6400/60000]
loss: 0.706748  [12800/60000]
loss: 1.110097  [19200/60000]
loss: 1.010435  [25600/60000]
loss: 0.954873  [32000/60000]
loss: 0.828446  [38400/60000]
loss: 0.766833  [44800/60000]
loss: 0.982645  [51200/60000]
loss: 0.687713  [57600/60000]
Test Error: 
 Accuracy: 69.1%, Avg loss: 0.976008 

Epoch 9
-------------------------------
loss: 1.057390  [    0/60000]
loss: 1.153566  [ 6400/60000]
loss: 0.666421  [12800/60000]
loss: 1.067712  [19200/60000]
loss: 1.014854  [25600/60000]
loss: 0.942381  [32000/60000]
loss: 0.828228  [38400/60000]
loss: 0.763920  [44800/60000]
loss: 0.978275  [51200/60000]
loss: 0.688640  [57600/60000]
Test Error: 
 Accuracy: 69.2%, Avg loss: 0.971257 

Backprop time:
0.0012588761649801675
Final  epoch:
9 69.25 0.9712569580715933 tensor(0.7918, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[3, 68.99, 0.9701344015871644, tensor(0.7962, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 2 hidden layer Signal Perceptron
Epoch 1
-------------------------------
loss: 2.300357  [    0/60000]
loss: 1.186061  [ 6400/60000]
loss: 1.278369  [12800/60000]
loss: 1.044726  [19200/60000]
loss: 1.228873  [25600/60000]
loss: 1.295399  [32000/60000]
loss: 0.846187  [38400/60000]
loss: 0.884172  [44800/60000]
loss: 1.166019  [51200/60000]
loss: 1.028163  [57600/60000]
Test Error: 
 Accuracy: 58.9%, Avg loss: 1.003779 

Epoch 2
-------------------------------
loss: 0.956333  [    0/60000]
loss: 1.091197  [ 6400/60000]
loss: 1.231206  [12800/60000]
loss: 1.007403  [19200/60000]
loss: 1.181214  [25600/60000]
loss: 1.225055  [32000/60000]
loss: 0.781023  [38400/60000]
loss: 0.842614  [44800/60000]
loss: 1.083400  [51200/60000]
loss: 0.991015  [57600/60000]
Test Error: 
 Accuracy: 59.3%, Avg loss: 0.984448 

Epoch 3
-------------------------------
loss: 0.913652  [    0/60000]
loss: 1.076699  [ 6400/60000]
loss: 1.209949  [12800/60000]
loss: 1.039453  [19200/60000]
loss: 1.124079  [25600/60000]
loss: 1.185571  [32000/60000]
loss: 0.774412  [38400/60000]
loss: 0.820299  [44800/60000]
loss: 1.054272  [51200/60000]
loss: 0.976464  [57600/60000]
Test Error: 
 Accuracy: 59.7%, Avg loss: 0.969850 

Epoch 4
-------------------------------
loss: 0.909721  [    0/60000]
loss: 1.070183  [ 6400/60000]
loss: 1.155211  [12800/60000]
loss: 1.011088  [19200/60000]
loss: 1.137374  [25600/60000]
loss: 1.118265  [32000/60000]
loss: 0.765985  [38400/60000]
loss: 0.803300  [44800/60000]
loss: 1.068680  [51200/60000]
loss: 0.956185  [57600/60000]
Test Error: 
 Accuracy: 59.8%, Avg loss: 0.965869 

Epoch 5
-------------------------------
loss: 0.907495  [    0/60000]
loss: 1.049534  [ 6400/60000]
loss: 1.174508  [12800/60000]
loss: 1.007319  [19200/60000]
loss: 1.134160  [25600/60000]
loss: 1.106634  [32000/60000]
loss: 0.758322  [38400/60000]
loss: 0.800594  [44800/60000]
loss: 1.045354  [51200/60000]
loss: 0.912039  [57600/60000]
Test Error: 
 Accuracy: 59.6%, Avg loss: 0.970750 

Epoch 6
-------------------------------
loss: 0.879782  [    0/60000]
loss: 1.061604  [ 6400/60000]
loss: 1.188425  [12800/60000]
loss: 0.983366  [19200/60000]
loss: 1.144300  [25600/60000]
loss: 1.145186  [32000/60000]
loss: 0.785575  [38400/60000]
loss: 0.837931  [44800/60000]
loss: 1.044682  [51200/60000]
loss: 0.960143  [57600/60000]
Test Error: 
 Accuracy: 59.9%, Avg loss: 0.962406 

Epoch 7
-------------------------------
loss: 0.905364  [    0/60000]
loss: 1.071775  [ 6400/60000]
loss: 1.133445  [12800/60000]
loss: 1.025966  [19200/60000]
loss: 1.125077  [25600/60000]
loss: 1.125990  [32000/60000]
loss: 0.823875  [38400/60000]
loss: 0.801162  [44800/60000]
loss: 1.050473  [51200/60000]
loss: 0.912823  [57600/60000]
Test Error: 
 Accuracy: 59.7%, Avg loss: 0.966042 

Epoch 8
-------------------------------
loss: 0.877356  [    0/60000]
loss: 1.044258  [ 6400/60000]
loss: 1.135687  [12800/60000]
loss: 0.994530  [19200/60000]
loss: 1.138083  [25600/60000]
loss: 1.100299  [32000/60000]
loss: 0.756066  [38400/60000]
loss: 0.796508  [44800/60000]
loss: 1.043622  [51200/60000]
loss: 0.903687  [57600/60000]
Test Error: 
 Accuracy: 59.9%, Avg loss: 0.961761 

Epoch 9
-------------------------------
loss: 0.867639  [    0/60000]
loss: 1.046237  [ 6400/60000]
loss: 1.185949  [12800/60000]
loss: 1.008507  [19200/60000]
loss: 1.152696  [25600/60000]
loss: 1.114902  [32000/60000]
loss: 0.777076  [38400/60000]
loss: 0.791929  [44800/60000]
loss: 1.049091  [51200/60000]
loss: 0.902436  [57600/60000]
Test Error: 
 Accuracy: 59.8%, Avg loss: 0.970166 

Backprop time:
0.0017201961484799252
Final  epoch:
9 59.830000000000005 0.9701663164576148 tensor(0.9354, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[7, 59.91, 0.9617613770399883, tensor(0.9354, device='cuda:0', grad_fn=<NllLossBackward>)]
Training models with FashionMNIST DATASET :
Fourier Signal Perceptron 128
Epoch 1
-------------------------------
loss: 2.415693  [    0/60000]
loss: 0.573598  [ 6400/60000]
loss: 0.391411  [12800/60000]
loss: 0.617523  [19200/60000]
loss: 0.484682  [25600/60000]
loss: 0.420475  [32000/60000]
loss: 0.337656  [38400/60000]
loss: 0.492222  [44800/60000]
loss: 0.458788  [51200/60000]
loss: 0.483250  [57600/60000]
Test Error: 
 Accuracy: 84.3%, Avg loss: 0.425828 

Epoch 2
-------------------------------
loss: 0.327832  [    0/60000]
loss: 0.383847  [ 6400/60000]
loss: 0.294450  [12800/60000]
loss: 0.406987  [19200/60000]
loss: 0.328868  [25600/60000]
loss: 0.370997  [32000/60000]
loss: 0.279019  [38400/60000]
loss: 0.374719  [44800/60000]
loss: 0.440544  [51200/60000]
loss: 0.415007  [57600/60000]
Test Error: 
 Accuracy: 85.3%, Avg loss: 0.403178 

Epoch 3
-------------------------------
loss: 0.251144  [    0/60000]
loss: 0.353550  [ 6400/60000]
loss: 0.301929  [12800/60000]
loss: 0.332072  [19200/60000]
loss: 0.306418  [25600/60000]
loss: 0.316294  [32000/60000]
loss: 0.261540  [38400/60000]
loss: 0.318121  [44800/60000]
loss: 0.402259  [51200/60000]
loss: 0.400783  [57600/60000]
Test Error: 
 Accuracy: 85.9%, Avg loss: 0.385938 

Epoch 4
-------------------------------
loss: 0.193046  [    0/60000]
loss: 0.314670  [ 6400/60000]
loss: 0.264389  [12800/60000]
loss: 0.304694  [19200/60000]
loss: 0.292455  [25600/60000]
loss: 0.276750  [32000/60000]
loss: 0.227325  [38400/60000]
loss: 0.257760  [44800/60000]
loss: 0.384405  [51200/60000]
loss: 0.343613  [57600/60000]
Test Error: 
 Accuracy: 86.4%, Avg loss: 0.384750 

Epoch 5
-------------------------------
loss: 0.187014  [    0/60000]
loss: 0.299554  [ 6400/60000]
loss: 0.290821  [12800/60000]
loss: 0.277534  [19200/60000]
loss: 0.288016  [25600/60000]
loss: 0.286630  [32000/60000]
loss: 0.207883  [38400/60000]
loss: 0.232497  [44800/60000]
loss: 0.336433  [51200/60000]
loss: 0.330746  [57600/60000]
Test Error: 
 Accuracy: 86.2%, Avg loss: 0.388546 

Epoch 6
-------------------------------
loss: 0.175509  [    0/60000]
loss: 0.277880  [ 6400/60000]
loss: 0.261925  [12800/60000]
loss: 0.267149  [19200/60000]
loss: 0.306084  [25600/60000]
loss: 0.281280  [32000/60000]
loss: 0.223679  [38400/60000]
loss: 0.211553  [44800/60000]
loss: 0.294228  [51200/60000]
loss: 0.307651  [57600/60000]
Test Error: 
 Accuracy: 86.1%, Avg loss: 0.398311 

Epoch 7
-------------------------------
loss: 0.169812  [    0/60000]
loss: 0.282330  [ 6400/60000]
loss: 0.235538  [12800/60000]
loss: 0.258179  [19200/60000]
loss: 0.288332  [25600/60000]
loss: 0.324969  [32000/60000]
loss: 0.185401  [38400/60000]
loss: 0.204819  [44800/60000]
loss: 0.304641  [51200/60000]
loss: 0.290315  [57600/60000]
Test Error: 
 Accuracy: 86.4%, Avg loss: 0.398092 

Epoch 8
-------------------------------
loss: 0.142739  [    0/60000]
loss: 0.272686  [ 6400/60000]
loss: 0.245253  [12800/60000]
loss: 0.239807  [19200/60000]
loss: 0.308413  [25600/60000]
loss: 0.324921  [32000/60000]
loss: 0.167927  [38400/60000]
loss: 0.203511  [44800/60000]
loss: 0.284787  [51200/60000]
loss: 0.274242  [57600/60000]
Test Error: 
 Accuracy: 86.4%, Avg loss: 0.407106 

Epoch 9
-------------------------------
loss: 0.147093  [    0/60000]
loss: 0.264574  [ 6400/60000]
loss: 0.220734  [12800/60000]
loss: 0.218222  [19200/60000]
loss: 0.319015  [25600/60000]
loss: 0.304030  [32000/60000]
loss: 0.183204  [38400/60000]
loss: 0.176054  [44800/60000]
loss: 0.260883  [51200/60000]
loss: 0.262554  [57600/60000]
Test Error: 
 Accuracy: 86.1%, Avg loss: 0.415950 

Backprop time:
0.0009974396305043561
Final  epoch:
9 86.06 0.41594993968488303 tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[3, 86.36, 0.3847496597345468, tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)]
Fourier Signal Perceptron 512
Epoch 1
-------------------------------
loss: 2.364362  [    0/60000]
loss: 0.571706  [ 6400/60000]
loss: 0.401653  [12800/60000]
loss: 0.563346  [19200/60000]
loss: 0.466581  [25600/60000]
loss: 0.380253  [32000/60000]
loss: 0.386153  [38400/60000]
loss: 0.508844  [44800/60000]
loss: 0.571538  [51200/60000]
loss: 0.525473  [57600/60000]
Test Error: 
 Accuracy: 84.4%, Avg loss: 0.429312 

Epoch 2
-------------------------------
loss: 0.241984  [    0/60000]
loss: 0.334616  [ 6400/60000]
loss: 0.319857  [12800/60000]
loss: 0.381659  [19200/60000]
loss: 0.329183  [25600/60000]
loss: 0.324602  [32000/60000]
loss: 0.271478  [38400/60000]
loss: 0.425224  [44800/60000]
loss: 0.389730  [51200/60000]
loss: 0.445370  [57600/60000]
Test Error: 
 Accuracy: 85.0%, Avg loss: 0.417457 

Epoch 3
-------------------------------
loss: 0.196427  [    0/60000]
loss: 0.334653  [ 6400/60000]
loss: 0.257585  [12800/60000]
loss: 0.307552  [19200/60000]
loss: 0.325910  [25600/60000]
loss: 0.298567  [32000/60000]
loss: 0.254681  [38400/60000]
loss: 0.320598  [44800/60000]
loss: 0.333374  [51200/60000]
loss: 0.455253  [57600/60000]
Test Error: 
 Accuracy: 85.7%, Avg loss: 0.402469 

Epoch 4
-------------------------------
loss: 0.165200  [    0/60000]
loss: 0.311662  [ 6400/60000]
loss: 0.246068  [12800/60000]
loss: 0.230352  [19200/60000]
loss: 0.227439  [25600/60000]
loss: 0.292776  [32000/60000]
loss: 0.206025  [38400/60000]
loss: 0.263795  [44800/60000]
loss: 0.306527  [51200/60000]
loss: 0.397175  [57600/60000]
Test Error: 
 Accuracy: 85.5%, Avg loss: 0.408016 

Epoch 5
-------------------------------
loss: 0.162888  [    0/60000]
loss: 0.289495  [ 6400/60000]
loss: 0.227042  [12800/60000]
loss: 0.207786  [19200/60000]
loss: 0.313268  [25600/60000]
loss: 0.296980  [32000/60000]
loss: 0.201632  [38400/60000]
loss: 0.280107  [44800/60000]
loss: 0.255191  [51200/60000]
loss: 0.378228  [57600/60000]
Test Error: 
 Accuracy: 85.8%, Avg loss: 0.401490 

Epoch 6
-------------------------------
loss: 0.166779  [    0/60000]
loss: 0.262986  [ 6400/60000]
loss: 0.221544  [12800/60000]
loss: 0.185249  [19200/60000]
loss: 0.257665  [25600/60000]
loss: 0.260820  [32000/60000]
loss: 0.260605  [38400/60000]
loss: 0.269989  [44800/60000]
loss: 0.276919  [51200/60000]
loss: 0.395559  [57600/60000]
Test Error: 
 Accuracy: 85.8%, Avg loss: 0.407398 

Epoch 7
-------------------------------
loss: 0.163877  [    0/60000]
loss: 0.277629  [ 6400/60000]
loss: 0.235403  [12800/60000]
loss: 0.172487  [19200/60000]
loss: 0.242952  [25600/60000]
loss: 0.225281  [32000/60000]
loss: 0.206839  [38400/60000]
loss: 0.266808  [44800/60000]
loss: 0.246430  [51200/60000]
loss: 0.299013  [57600/60000]
Test Error: 
 Accuracy: 85.9%, Avg loss: 0.407109 

Epoch 8
-------------------------------
loss: 0.166588  [    0/60000]
loss: 0.222150  [ 6400/60000]
loss: 0.194410  [12800/60000]
loss: 0.181339  [19200/60000]
loss: 0.192126  [25600/60000]
loss: 0.259601  [32000/60000]
loss: 0.205777  [38400/60000]
loss: 0.262073  [44800/60000]
loss: 0.247614  [51200/60000]
loss: 0.295190  [57600/60000]
Test Error: 
 Accuracy: 86.3%, Avg loss: 0.403184 

Epoch 9
-------------------------------
loss: 0.164721  [    0/60000]
loss: 0.248990  [ 6400/60000]
loss: 0.236248  [12800/60000]
loss: 0.193723  [19200/60000]
loss: 0.271198  [25600/60000]
loss: 0.282500  [32000/60000]
loss: 0.262230  [38400/60000]
loss: 0.292661  [44800/60000]
loss: 0.191639  [51200/60000]
loss: 0.318562  [57600/60000]
Test Error: 
 Accuracy: 86.6%, Avg loss: 0.413175 

Backprop time:
0.001003357929743966
Final  epoch:
9 86.57000000000001 0.41317548674003335 tensor(0.1971, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[4, 85.81, 0.40148957467572705, tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 1 hidden layer
Epoch 1
-------------------------------
loss: 2.308640  [    0/60000]
loss: 1.213248  [ 6400/60000]
loss: 1.060584  [12800/60000]
loss: 1.123521  [19200/60000]
loss: 0.984478  [25600/60000]
loss: 1.070548  [32000/60000]
loss: 0.951138  [38400/60000]
loss: 1.145413  [44800/60000]
loss: 1.098077  [51200/60000]
loss: 0.985394  [57600/60000]
Test Error: 
 Accuracy: 65.7%, Avg loss: 1.030912 

Epoch 2
-------------------------------
loss: 0.984128  [    0/60000]
loss: 1.020602  [ 6400/60000]
loss: 0.962676  [12800/60000]
loss: 0.987513  [19200/60000]
loss: 0.931509  [25600/60000]
loss: 0.992328  [32000/60000]
loss: 0.899844  [38400/60000]
loss: 1.095364  [44800/60000]
loss: 1.030131  [51200/60000]
loss: 0.923942  [57600/60000]
Test Error: 
 Accuracy: 66.4%, Avg loss: 0.994397 

Epoch 3
-------------------------------
loss: 0.963323  [    0/60000]
loss: 0.987765  [ 6400/60000]
loss: 0.949014  [12800/60000]
loss: 0.938143  [19200/60000]
loss: 0.891124  [25600/60000]
loss: 0.980078  [32000/60000]
loss: 0.884919  [38400/60000]
loss: 1.075256  [44800/60000]
loss: 1.007371  [51200/60000]
loss: 0.885549  [57600/60000]
Test Error: 
 Accuracy: 67.5%, Avg loss: 0.975618 

Epoch 4
-------------------------------
loss: 0.964667  [    0/60000]
loss: 0.975263  [ 6400/60000]
loss: 0.939872  [12800/60000]
loss: 0.900894  [19200/60000]
loss: 0.888416  [25600/60000]
loss: 0.957930  [32000/60000]
loss: 0.882427  [38400/60000]
loss: 1.046635  [44800/60000]
loss: 0.981936  [51200/60000]
loss: 0.835662  [57600/60000]
Test Error: 
 Accuracy: 67.7%, Avg loss: 0.968902 

Epoch 5
-------------------------------
loss: 0.951819  [    0/60000]
loss: 0.973632  [ 6400/60000]
loss: 0.920367  [12800/60000]
loss: 0.884315  [19200/60000]
loss: 0.869027  [25600/60000]
loss: 0.952277  [32000/60000]
loss: 0.874418  [38400/60000]
loss: 1.027842  [44800/60000]
loss: 0.961687  [51200/60000]
loss: 0.818718  [57600/60000]
Test Error: 
 Accuracy: 67.1%, Avg loss: 0.966361 

Epoch 6
-------------------------------
loss: 0.944603  [    0/60000]
loss: 0.960583  [ 6400/60000]
loss: 0.909303  [12800/60000]
loss: 0.882054  [19200/60000]
loss: 0.858443  [25600/60000]
loss: 0.929452  [32000/60000]
loss: 0.852539  [38400/60000]
loss: 1.011276  [44800/60000]
loss: 0.949384  [51200/60000]
loss: 0.801830  [57600/60000]
Test Error: 
 Accuracy: 68.0%, Avg loss: 0.957870 

Epoch 7
-------------------------------
loss: 0.935496  [    0/60000]
loss: 0.938106  [ 6400/60000]
loss: 0.894790  [12800/60000]
loss: 0.856591  [19200/60000]
loss: 0.860340  [25600/60000]
loss: 0.937372  [32000/60000]
loss: 0.869490  [38400/60000]
loss: 0.999616  [44800/60000]
loss: 0.934594  [51200/60000]
loss: 0.770210  [57600/60000]
Test Error: 
 Accuracy: 68.6%, Avg loss: 0.956795 

Epoch 8
-------------------------------
loss: 0.938007  [    0/60000]
loss: 0.933780  [ 6400/60000]
loss: 0.899552  [12800/60000]
loss: 0.837793  [19200/60000]
loss: 0.843665  [25600/60000]
loss: 0.915005  [32000/60000]
loss: 0.849312  [38400/60000]
loss: 0.987477  [44800/60000]
loss: 0.940347  [51200/60000]
loss: 0.771315  [57600/60000]
Test Error: 
 Accuracy: 68.5%, Avg loss: 0.955252 

Epoch 9
-------------------------------
loss: 0.932202  [    0/60000]
loss: 0.915816  [ 6400/60000]
loss: 0.903808  [12800/60000]
loss: 0.827608  [19200/60000]
loss: 0.831852  [25600/60000]
loss: 0.920967  [32000/60000]
loss: 0.831364  [38400/60000]
loss: 1.009760  [44800/60000]
loss: 0.940022  [51200/60000]
loss: 0.773397  [57600/60000]
Test Error: 
 Accuracy: 69.0%, Avg loss: 0.958245 

Backprop time:
0.0013171725577015754
Final  epoch:
9 68.97999999999999 0.9582445507596253 tensor(0.8117, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[7, 68.47999999999999, 0.9552516717060356, tensor(0.8236, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 2 hidden layer
Epoch 1
-------------------------------
loss: 2.316023  [    0/60000]
loss: 1.913176  [ 6400/60000]
loss: 1.956661  [12800/60000]
loss: 1.939992  [19200/60000]
loss: 1.909279  [25600/60000]
loss: 1.983760  [32000/60000]
loss: 1.705983  [38400/60000]
loss: 1.910814  [44800/60000]
loss: 1.777940  [51200/60000]
loss: 1.869573  [57600/60000]
Test Error: 
 Accuracy: 29.0%, Avg loss: 1.900955 

Epoch 2
-------------------------------
loss: 1.802901  [    0/60000]
loss: 1.891905  [ 6400/60000]
loss: 1.921932  [12800/60000]
loss: 1.889496  [19200/60000]
loss: 1.810551  [25600/60000]
loss: 1.979143  [32000/60000]
loss: 1.701337  [38400/60000]
loss: 1.909340  [44800/60000]
loss: 1.744649  [51200/60000]
loss: 1.882316  [57600/60000]
Test Error: 
 Accuracy: 29.2%, Avg loss: 1.888227 

Epoch 3
-------------------------------
loss: 1.765002  [    0/60000]
loss: 1.847298  [ 6400/60000]
loss: 1.913712  [12800/60000]
loss: 1.884833  [19200/60000]
loss: 1.778782  [25600/60000]
loss: 1.979249  [32000/60000]
loss: 1.708188  [38400/60000]
loss: 1.908002  [44800/60000]
loss: 1.757563  [51200/60000]
loss: 1.832370  [57600/60000]
Test Error: 
 Accuracy: 29.3%, Avg loss: 1.886596 

Epoch 4
-------------------------------
loss: 1.764392  [    0/60000]
loss: 1.849261  [ 6400/60000]
loss: 1.914266  [12800/60000]
loss: 1.879791  [19200/60000]
loss: 1.788945  [25600/60000]
loss: 1.979077  [32000/60000]
loss: 1.701150  [38400/60000]
loss: 1.907948  [44800/60000]
loss: 1.736390  [51200/60000]
loss: 1.781988  [57600/60000]
Test Error: 
 Accuracy: 29.4%, Avg loss: 1.887656 

Epoch 5
-------------------------------
loss: 1.764791  [    0/60000]
loss: 1.864765  [ 6400/60000]
loss: 1.912653  [12800/60000]
loss: 1.889495  [19200/60000]
loss: 1.762958  [25600/60000]
loss: 1.978992  [32000/60000]
loss: 1.699606  [38400/60000]
loss: 1.909026  [44800/60000]
loss: 1.733149  [51200/60000]
loss: 1.826359  [57600/60000]
Test Error: 
 Accuracy: 29.4%, Avg loss: 1.875778 

Epoch 6
-------------------------------
loss: 1.766990  [    0/60000]
loss: 1.855063  [ 6400/60000]
loss: 1.923317  [12800/60000]
loss: 1.884815  [19200/60000]
loss: 1.804008  [25600/60000]
loss: 1.978873  [32000/60000]
loss: 1.698477  [38400/60000]
loss: 1.907311  [44800/60000]
loss: 1.729734  [51200/60000]
loss: 1.810896  [57600/60000]
Test Error: 
 Accuracy: 29.4%, Avg loss: 1.878334 

Epoch 7
-------------------------------
loss: 1.765801  [    0/60000]
loss: 1.855951  [ 6400/60000]
loss: 1.911790  [12800/60000]
loss: 1.871436  [19200/60000]
loss: 1.761083  [25600/60000]
loss: 1.978796  [32000/60000]
loss: 1.700136  [38400/60000]
loss: 1.907235  [44800/60000]
loss: 1.729500  [51200/60000]
loss: 1.791805  [57600/60000]
Test Error: 
 Accuracy: 29.4%, Avg loss: 1.878277 

Epoch 8
-------------------------------
loss: 1.771847  [    0/60000]
loss: 1.858469  [ 6400/60000]
loss: 1.907952  [12800/60000]
loss: 1.897168  [19200/60000]
loss: 1.788613  [25600/60000]
loss: 1.978803  [32000/60000]
loss: 1.692124  [38400/60000]
loss: 1.907035  [44800/60000]
loss: 1.732491  [51200/60000]
loss: 1.796635  [57600/60000]
Test Error: 
 Accuracy: 29.4%, Avg loss: 1.883357 

Epoch 9
-------------------------------
loss: 1.763896  [    0/60000]
loss: 1.864421  [ 6400/60000]
loss: 1.907125  [12800/60000]
loss: 1.872045  [19200/60000]
loss: 1.793390  [25600/60000]
loss: 1.978812  [32000/60000]
loss: 1.701493  [38400/60000]
loss: 1.907248  [44800/60000]
loss: 1.728225  [51200/60000]
loss: 1.794442  [57600/60000]
Test Error: 
 Accuracy: 29.4%, Avg loss: 1.878223 

Backprop time:
0.001750049249901644
Final  epoch:
9 29.42 1.8782231093971593 tensor(1.8786, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[4, 29.409999999999997, 1.8757779279332252, tensor(1.8752, device='cuda:0', grad_fn=<NllLossBackward>)]
