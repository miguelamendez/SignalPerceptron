Using cuda device
Learnable Parameters for MNIST models:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
101632 	 406528 	 407050 	 669706
torch.Size([64, 1, 28, 28])
torch.Size([1, 28, 28])
Forward time for MNIST models:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
0.00011587142944335938 	 0.00010132789611816406 	 0.00014400482177734375 	 0.0002167224884033203
Forward time for MNIST models FS Timer class:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
9.793972969055175e-05 	 9.59630012512207e-05 	 0.00014683675765991212 	 0.0002176365852355957
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                   model_inference mlp1         5.64%     136.000us        99.17%       2.393ms       2.393ms       0.000us         0.00%      18.000us      18.000us             1  
                                           aten::linear         0.79%      19.000us        86.95%       2.098ms       1.049ms       0.000us         0.00%      16.000us       8.000us             2  
                                            aten::addmm        10.40%     251.000us        84.62%       2.042ms       1.021ms      16.000us        88.89%      16.000us       8.000us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      14.000us        77.78%      14.000us       7.000us             2  
                                        aten::clamp_min         2.15%      52.000us         7.25%     175.000us      43.750us       2.000us        11.11%       4.000us       1.000us             4  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us        11.11%       2.000us       1.000us             2  
                                             aten::relu         1.41%      34.000us         5.76%     139.000us      69.500us       0.000us         0.00%       2.000us       1.000us             2  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us        11.11%       2.000us       1.000us             2  
                                            aten::zeros         0.33%       8.000us         0.62%      15.000us      15.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         0.95%      23.000us         0.95%      23.000us       5.750us       0.000us         0.00%       0.000us       0.000us             4  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.413ms
Self CUDA time total: 18.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                   model_inference mlp2         6.30%     160.000us        99.09%       2.517ms       2.517ms       0.000us         0.00%      26.000us      26.000us             1  
                                           aten::linear         1.18%      30.000us        84.72%       2.152ms     717.333us       0.000us         0.00%      25.000us       8.333us             3  
                                            aten::addmm        11.14%     283.000us        81.65%       2.074ms     691.333us      25.000us        96.15%      25.000us       8.333us             3  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      22.000us        84.62%      22.000us       7.333us             3  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us        11.54%       3.000us       1.000us             3  
                                        aten::clamp_min         2.76%      70.000us         9.61%     244.000us      40.667us       1.000us         3.85%       2.000us       0.333us             6  
                                             aten::relu         1.61%      41.000us         7.28%     185.000us      61.667us       0.000us         0.00%       1.000us       0.333us             3  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         3.85%       1.000us       0.333us             3  
                                            aten::zeros         0.35%       9.000us         0.67%      17.000us      17.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         1.34%      34.000us         1.34%      34.000us       6.800us       0.000us         0.00%       0.000us       0.000us             5  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.540ms
Self CUDA time total: 26.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                 model_inference fsp128         5.60%     122.000us        98.67%       2.149ms       2.149ms       0.000us         0.00%       6.000us       6.000us             1  
                                           aten::linear         0.87%      19.000us        90.08%       1.962ms     981.000us       0.000us         0.00%       6.000us       3.000us             2  
                                           aten::matmul         0.78%      17.000us        87.28%       1.901ms     950.500us       0.000us         0.00%       6.000us       3.000us             2  
                                               aten::mm        10.74%     234.000us        86.50%       1.884ms     942.000us       6.000us       100.00%       6.000us       3.000us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us       100.00%       6.000us       3.000us             2  
                                            aten::zeros         0.37%       8.000us         1.10%      24.000us      24.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         1.56%      34.000us         1.56%      34.000us       8.500us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.14%       3.000us         0.14%       3.000us       3.000us       0.000us         0.00%       0.000us       0.000us             1  
                                          aten::flatten         0.28%       6.000us         0.51%      11.000us      11.000us       0.000us         0.00%       0.000us       0.000us             1  
                                             aten::view         0.23%       5.000us         0.23%       5.000us       5.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.178ms
Self CUDA time total: 6.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                 model_inference fsp512         5.38%     119.000us        99.01%       2.190ms       2.190ms       0.000us         0.00%      14.000us      14.000us             1  
                                           aten::linear         0.99%      22.000us        90.46%       2.001ms       1.000ms       0.000us         0.00%      13.000us       6.500us             2  
                                           aten::matmul         0.68%      15.000us        87.61%       1.938ms     969.000us       0.000us         0.00%      13.000us       6.500us             2  
                                               aten::mm        10.94%     242.000us        86.93%       1.923ms     961.500us      13.000us        92.86%      13.000us       6.500us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      13.000us        92.86%      13.000us       6.500us             2  
                                              aten::cos         1.76%      39.000us         2.35%      52.000us      52.000us       1.000us         7.14%       1.000us       1.000us             1  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         7.14%       1.000us       1.000us             1  
                                            aten::zeros         0.36%       8.000us         0.77%      17.000us      17.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         1.36%      30.000us         1.36%      30.000us       7.500us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.14%       3.000us         0.14%       3.000us       3.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.212ms
Self CUDA time total: 14.000us

Optimizer: Adam ,lr: 0.001
Training models with MNIST DATASET :
Fourier Signal Perceptron 128
Epoch 1
-------------------------------
loss: 2.462337  [    0/60000]
loss: 0.447057  [ 6400/60000]
loss: 0.322980  [12800/60000]
loss: 0.331306  [19200/60000]
loss: 0.220022  [25600/60000]
loss: 0.300983  [32000/60000]
loss: 0.118441  [38400/60000]
loss: 0.276614  [44800/60000]
loss: 0.223917  [51200/60000]
loss: 0.242018  [57600/60000]
Test Error: 
 Accuracy: 95.4%, Avg loss: 0.160192 

Epoch 2
-------------------------------
loss: 0.109365  [    0/60000]
loss: 0.156730  [ 6400/60000]
loss: 0.106273  [12800/60000]
loss: 0.129563  [19200/60000]
loss: 0.097184  [25600/60000]
loss: 0.203418  [32000/60000]
loss: 0.076802  [38400/60000]
loss: 0.154092  [44800/60000]
loss: 0.135633  [51200/60000]
loss: 0.159414  [57600/60000]
Test Error: 
 Accuracy: 96.7%, Avg loss: 0.106268 

Epoch 3
-------------------------------
loss: 0.058247  [    0/60000]
loss: 0.123747  [ 6400/60000]
loss: 0.075954  [12800/60000]
loss: 0.073698  [19200/60000]
loss: 0.049835  [25600/60000]
loss: 0.146760  [32000/60000]
loss: 0.071702  [38400/60000]
loss: 0.081803  [44800/60000]
loss: 0.096241  [51200/60000]
loss: 0.089637  [57600/60000]
Test Error: 
 Accuracy: 97.1%, Avg loss: 0.091099 

Epoch 4
-------------------------------
loss: 0.047807  [    0/60000]
loss: 0.101787  [ 6400/60000]
loss: 0.063008  [12800/60000]
loss: 0.039303  [19200/60000]
loss: 0.029312  [25600/60000]
loss: 0.113988  [32000/60000]
loss: 0.049590  [38400/60000]
loss: 0.042866  [44800/60000]
loss: 0.085563  [51200/60000]
loss: 0.045073  [57600/60000]
Test Error: 
 Accuracy: 97.4%, Avg loss: 0.084824 

Epoch 5
-------------------------------
loss: 0.037539  [    0/60000]
loss: 0.085899  [ 6400/60000]
loss: 0.040814  [12800/60000]
loss: 0.023944  [19200/60000]
loss: 0.020312  [25600/60000]
loss: 0.081662  [32000/60000]
loss: 0.029345  [38400/60000]
loss: 0.028740  [44800/60000]
loss: 0.073416  [51200/60000]
loss: 0.024418  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.082463 

Epoch 6
-------------------------------
loss: 0.029719  [    0/60000]
loss: 0.054817  [ 6400/60000]
loss: 0.028364  [12800/60000]
loss: 0.016707  [19200/60000]
loss: 0.015328  [25600/60000]
loss: 0.046821  [32000/60000]
loss: 0.016720  [38400/60000]
loss: 0.021887  [44800/60000]
loss: 0.051984  [51200/60000]
loss: 0.015686  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.081957 

Epoch 7
-------------------------------
loss: 0.024100  [    0/60000]
loss: 0.019704  [ 6400/60000]
loss: 0.022668  [12800/60000]
loss: 0.011850  [19200/60000]
loss: 0.011280  [25600/60000]
loss: 0.020429  [32000/60000]
loss: 0.011057  [38400/60000]
loss: 0.016074  [44800/60000]
loss: 0.032011  [51200/60000]
loss: 0.010316  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.081660 

Epoch 8
-------------------------------
loss: 0.018252  [    0/60000]
loss: 0.009337  [ 6400/60000]
loss: 0.016621  [12800/60000]
loss: 0.009619  [19200/60000]
loss: 0.008293  [25600/60000]
loss: 0.013702  [32000/60000]
loss: 0.005535  [38400/60000]
loss: 0.010122  [44800/60000]
loss: 0.021845  [51200/60000]
loss: 0.008472  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.082475 

Epoch 9
-------------------------------
loss: 0.010052  [    0/60000]
loss: 0.004651  [ 6400/60000]
loss: 0.013241  [12800/60000]
loss: 0.009497  [19200/60000]
loss: 0.005711  [25600/60000]
loss: 0.010520  [32000/60000]
loss: 0.003085  [38400/60000]
loss: 0.006705  [44800/60000]
loss: 0.013732  [51200/60000]
loss: 0.004878  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.089065 

Backprop time:
0.0011500812090846832
Final  epoch:
9 97.49 0.08906454481115894 tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[6, 97.6, 0.08166031186913057, tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)]
Fourier Signal Perceptron 512
Epoch 1
-------------------------------
loss: 2.382298  [    0/60000]
loss: 0.321130  [ 6400/60000]
loss: 0.239843  [12800/60000]
loss: 0.253352  [19200/60000]
loss: 0.155250  [25600/60000]
loss: 0.273355  [32000/60000]
loss: 0.095244  [38400/60000]
loss: 0.276894  [44800/60000]
loss: 0.225054  [51200/60000]
loss: 0.201664  [57600/60000]
Test Error: 
 Accuracy: 96.1%, Avg loss: 0.125569 

Epoch 2
-------------------------------
loss: 0.098176  [    0/60000]
loss: 0.159256  [ 6400/60000]
loss: 0.076728  [12800/60000]
loss: 0.090882  [19200/60000]
loss: 0.059453  [25600/60000]
loss: 0.143403  [32000/60000]
loss: 0.068874  [38400/60000]
loss: 0.087972  [44800/60000]
loss: 0.123118  [51200/60000]
loss: 0.070676  [57600/60000]
Test Error: 
 Accuracy: 97.4%, Avg loss: 0.081597 

Epoch 3
-------------------------------
loss: 0.040596  [    0/60000]
loss: 0.091992  [ 6400/60000]
loss: 0.047071  [12800/60000]
loss: 0.040216  [19200/60000]
loss: 0.028753  [25600/60000]
loss: 0.088977  [32000/60000]
loss: 0.039886  [38400/60000]
loss: 0.032406  [44800/60000]
loss: 0.085498  [51200/60000]
loss: 0.023407  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.068986 

Epoch 4
-------------------------------
loss: 0.024988  [    0/60000]
loss: 0.037118  [ 6400/60000]
loss: 0.029669  [12800/60000]
loss: 0.027372  [19200/60000]
loss: 0.011787  [25600/60000]
loss: 0.049771  [32000/60000]
loss: 0.013021  [38400/60000]
loss: 0.019620  [44800/60000]
loss: 0.067012  [51200/60000]
loss: 0.011552  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.065118 

Epoch 5
-------------------------------
loss: 0.016255  [    0/60000]
loss: 0.009514  [ 6400/60000]
loss: 0.022155  [12800/60000]
loss: 0.015952  [19200/60000]
loss: 0.006047  [25600/60000]
loss: 0.022053  [32000/60000]
loss: 0.004532  [38400/60000]
loss: 0.015679  [44800/60000]
loss: 0.053788  [51200/60000]
loss: 0.004731  [57600/60000]
Test Error: 
 Accuracy: 98.0%, Avg loss: 0.067828 

Epoch 6
-------------------------------
loss: 0.009017  [    0/60000]
loss: 0.005662  [ 6400/60000]
loss: 0.013529  [12800/60000]
loss: 0.016384  [19200/60000]
loss: 0.004195  [25600/60000]
loss: 0.008303  [32000/60000]
loss: 0.001579  [38400/60000]
loss: 0.011183  [44800/60000]
loss: 0.017059  [51200/60000]
loss: 0.002647  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.074663 

Epoch 7
-------------------------------
loss: 0.004845  [    0/60000]
loss: 0.015971  [ 6400/60000]
loss: 0.018540  [12800/60000]
loss: 0.006913  [19200/60000]
loss: 0.002930  [25600/60000]
loss: 0.007163  [32000/60000]
loss: 0.003128  [38400/60000]
loss: 0.006495  [44800/60000]
loss: 0.055520  [51200/60000]
loss: 0.002994  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.078096 

Epoch 8
-------------------------------
loss: 0.008587  [    0/60000]
loss: 0.021371  [ 6400/60000]
loss: 0.024554  [12800/60000]
loss: 0.012851  [19200/60000]
loss: 0.007920  [25600/60000]
loss: 0.004140  [32000/60000]
loss: 0.013863  [38400/60000]
loss: 0.010014  [44800/60000]
loss: 0.026618  [51200/60000]
loss: 0.002406  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.076579 

Epoch 9
-------------------------------
loss: 0.003497  [    0/60000]
loss: 0.002064  [ 6400/60000]
loss: 0.003702  [12800/60000]
loss: 0.010904  [19200/60000]
loss: 0.005909  [25600/60000]
loss: 0.001421  [32000/60000]
loss: 0.001259  [38400/60000]
loss: 0.004725  [44800/60000]
loss: 0.035960  [51200/60000]
loss: 0.014422  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.078867 

Backprop time:
0.0010814903133205258
Final  epoch:
9 97.85000000000001 0.07886694641772132 tensor(8.1401e-05, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[3, 97.94, 0.06511849289279688]
MLP 1 hidden layer Signal Perceptron
Epoch 1
-------------------------------
loss: 2.305743  [    0/60000]
loss: 1.614555  [ 6400/60000]
loss: 1.708073  [12800/60000]
loss: 1.341879  [19200/60000]
loss: 1.666402  [25600/60000]
loss: 1.743271  [32000/60000]
loss: 1.397721  [38400/60000]
loss: 1.345938  [44800/60000]
loss: 1.489000  [51200/60000]
loss: 1.573079  [57600/60000]
Test Error: 
 Accuracy: 38.3%, Avg loss: 1.474087 

Epoch 2
-------------------------------
loss: 1.517460  [    0/60000]
loss: 1.558046  [ 6400/60000]
loss: 1.647118  [12800/60000]
loss: 1.309766  [19200/60000]
loss: 1.669335  [25600/60000]
loss: 1.737705  [32000/60000]
loss: 1.380903  [38400/60000]
loss: 1.276569  [44800/60000]
loss: 1.425283  [51200/60000]
loss: 1.536302  [57600/60000]
Test Error: 
 Accuracy: 38.6%, Avg loss: 1.450453 

Epoch 3
-------------------------------
loss: 1.502916  [    0/60000]
loss: 1.548663  [ 6400/60000]
loss: 1.633925  [12800/60000]
loss: 1.317097  [19200/60000]
loss: 1.645188  [25600/60000]
loss: 1.686339  [32000/60000]
loss: 1.360992  [38400/60000]
loss: 1.244338  [44800/60000]
loss: 1.409873  [51200/60000]
loss: 1.484720  [57600/60000]
Test Error: 
 Accuracy: 38.7%, Avg loss: 1.442727 

Epoch 4
-------------------------------
loss: 1.498279  [    0/60000]
loss: 1.537514  [ 6400/60000]
loss: 1.631053  [12800/60000]
loss: 1.299957  [19200/60000]
loss: 1.636561  [25600/60000]
loss: 1.652469  [32000/60000]
loss: 1.353956  [38400/60000]
loss: 1.233632  [44800/60000]
loss: 1.405738  [51200/60000]
loss: 1.407217  [57600/60000]
Test Error: 
 Accuracy: 38.8%, Avg loss: 1.440253 

Epoch 5
-------------------------------
loss: 1.487892  [    0/60000]
loss: 1.525463  [ 6400/60000]
loss: 1.625437  [12800/60000]
loss: 1.301515  [19200/60000]
loss: 1.630117  [25600/60000]
loss: 1.634342  [32000/60000]
loss: 1.342757  [38400/60000]
loss: 1.226387  [44800/60000]
loss: 1.406505  [51200/60000]
loss: 1.375687  [57600/60000]
Test Error: 
 Accuracy: 38.7%, Avg loss: 1.440987 

Epoch 6
-------------------------------
loss: 1.478578  [    0/60000]
loss: 1.520754  [ 6400/60000]
loss: 1.626701  [12800/60000]
loss: 1.306206  [19200/60000]
loss: 1.624916  [25600/60000]
loss: 1.632678  [32000/60000]
loss: 1.342574  [38400/60000]
loss: 1.225885  [44800/60000]
loss: 1.405086  [51200/60000]
loss: 1.369304  [57600/60000]
Test Error: 
 Accuracy: 38.6%, Avg loss: 1.442332 

Epoch 7
-------------------------------
loss: 1.477140  [    0/60000]
loss: 1.512309  [ 6400/60000]
loss: 1.621172  [12800/60000]
loss: 1.303515  [19200/60000]
loss: 1.651392  [25600/60000]
loss: 1.622414  [32000/60000]
loss: 1.337943  [38400/60000]
loss: 1.228750  [44800/60000]
loss: 1.403786  [51200/60000]
loss: 1.374216  [57600/60000]
Test Error: 
 Accuracy: 38.6%, Avg loss: 1.442305 

Epoch 8
-------------------------------
loss: 1.478484  [    0/60000]
loss: 1.518112  [ 6400/60000]
loss: 1.619234  [12800/60000]
loss: 1.297536  [19200/60000]
loss: 1.619739  [25600/60000]
loss: 1.629928  [32000/60000]
loss: 1.333846  [38400/60000]
loss: 1.225879  [44800/60000]
loss: 1.404444  [51200/60000]
loss: 1.371855  [57600/60000]
Test Error: 
 Accuracy: 38.6%, Avg loss: 1.439915 

Epoch 9
-------------------------------
loss: 1.476731  [    0/60000]
loss: 1.515610  [ 6400/60000]
loss: 1.619091  [12800/60000]
loss: 1.297543  [19200/60000]
loss: 1.619064  [25600/60000]
loss: 1.622722  [32000/60000]
loss: 1.334159  [38400/60000]
loss: 1.226372  [44800/60000]
loss: 1.406723  [51200/60000]
loss: 1.373706  [57600/60000]
Test Error: 
 Accuracy: 38.6%, Avg loss: 1.446216 

Backprop time:
0.0013099203062294728
Final  epoch:
9 38.64 1.446215628059047 tensor(1.4391, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[7, 38.629999999999995, 1.4399149569736165, tensor(1.4393, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 2 hidden layer Signal Perceptron
Epoch 1
-------------------------------
loss: 2.301730  [    0/60000]
loss: 0.612888  [ 6400/60000]
loss: 0.554948  [12800/60000]
loss: 0.283904  [19200/60000]
loss: 0.386604  [25600/60000]
loss: 0.595295  [32000/60000]
loss: 0.269322  [38400/60000]
loss: 0.639707  [44800/60000]
loss: 0.465841  [51200/60000]
loss: 0.326314  [57600/60000]
Test Error: 
 Accuracy: 87.4%, Avg loss: 0.343134 

Epoch 2
-------------------------------
loss: 0.237053  [    0/60000]
loss: 0.378502  [ 6400/60000]
loss: 0.103931  [12800/60000]
loss: 0.164335  [19200/60000]
loss: 0.037763  [25600/60000]
loss: 0.162499  [32000/60000]
loss: 0.038580  [38400/60000]
loss: 0.139994  [44800/60000]
loss: 0.160402  [51200/60000]
loss: 0.118933  [57600/60000]
Test Error: 
 Accuracy: 97.0%, Avg loss: 0.095788 

Epoch 3
-------------------------------
loss: 0.054778  [    0/60000]
loss: 0.079220  [ 6400/60000]
loss: 0.031812  [12800/60000]
loss: 0.134386  [19200/60000]
loss: 0.041589  [25600/60000]
loss: 0.098842  [32000/60000]
loss: 0.020616  [38400/60000]
loss: 0.094244  [44800/60000]
loss: 0.111694  [51200/60000]
loss: 0.073419  [57600/60000]
Test Error: 
 Accuracy: 97.0%, Avg loss: 0.099983 

Epoch 4
-------------------------------
loss: 0.072536  [    0/60000]
loss: 0.012284  [ 6400/60000]
loss: 0.030217  [12800/60000]
loss: 0.083624  [19200/60000]
loss: 0.016022  [25600/60000]
loss: 0.064597  [32000/60000]
loss: 0.039265  [38400/60000]
loss: 0.043231  [44800/60000]
loss: 0.126424  [51200/60000]
loss: 0.036286  [57600/60000]
Test Error: 
 Accuracy: 97.3%, Avg loss: 0.096041 

Epoch 5
-------------------------------
loss: 0.012935  [    0/60000]
loss: 0.028283  [ 6400/60000]
loss: 0.008603  [12800/60000]
loss: 0.017151  [19200/60000]
loss: 0.017270  [25600/60000]
loss: 0.029958  [32000/60000]
loss: 0.005058  [38400/60000]
loss: 0.015182  [44800/60000]
loss: 0.085294  [51200/60000]
loss: 0.015180  [57600/60000]
Test Error: 
 Accuracy: 96.9%, Avg loss: 0.122989 

Epoch 6
-------------------------------
loss: 0.055402  [    0/60000]
loss: 0.014496  [ 6400/60000]
loss: 0.009246  [12800/60000]
loss: 0.062834  [19200/60000]
loss: 0.086431  [25600/60000]
loss: 0.034139  [32000/60000]
loss: 0.010901  [38400/60000]
loss: 0.035950  [44800/60000]
loss: 0.076897  [51200/60000]
loss: 0.017883  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.085658 

Epoch 7
-------------------------------
loss: 0.023303  [    0/60000]
loss: 0.012937  [ 6400/60000]
loss: 0.114404  [12800/60000]
loss: 0.001702  [19200/60000]
loss: 0.147491  [25600/60000]
loss: 0.027823  [32000/60000]
loss: 0.004935  [38400/60000]
loss: 0.050359  [44800/60000]
loss: 0.029822  [51200/60000]
loss: 0.032092  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.102043 

Epoch 8
-------------------------------
loss: 0.007000  [    0/60000]
loss: 0.078217  [ 6400/60000]
loss: 0.047364  [12800/60000]
loss: 0.002016  [19200/60000]
loss: 0.001314  [25600/60000]
loss: 0.012845  [32000/60000]
loss: 0.001015  [38400/60000]
loss: 0.005919  [44800/60000]
loss: 0.110156  [51200/60000]
loss: 0.052586  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.114206 

Epoch 9
-------------------------------
loss: 0.053738  [    0/60000]
loss: 0.007393  [ 6400/60000]
loss: 0.121729  [12800/60000]
loss: 0.000915  [19200/60000]
loss: 0.005448  [25600/60000]
loss: 0.003885  [32000/60000]
loss: 0.026522  [38400/60000]
loss: 0.011695  [44800/60000]
loss: 0.020850  [51200/60000]
loss: 0.005242  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.095445 

Backprop time:
0.0017994354472175025
Final  epoch:
9 97.88 0.09544526293965484 tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 97.8, 0.08565806949324034, tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)]
Training models with FashionMNIST DATASET :
Fourier Signal Perceptron 128
Epoch 1
-------------------------------
loss: 2.400416  [    0/60000]
loss: 0.572160  [ 6400/60000]
loss: 0.436564  [12800/60000]
loss: 0.583185  [19200/60000]
loss: 0.460881  [25600/60000]
loss: 0.416601  [32000/60000]
loss: 0.363459  [38400/60000]
loss: 0.509931  [44800/60000]
loss: 0.463957  [51200/60000]
loss: 0.459839  [57600/60000]
Test Error: 
 Accuracy: 85.1%, Avg loss: 0.417795 

Epoch 2
-------------------------------
loss: 0.292380  [    0/60000]
loss: 0.381343  [ 6400/60000]
loss: 0.344086  [12800/60000]
loss: 0.383225  [19200/60000]
loss: 0.365687  [25600/60000]
loss: 0.331454  [32000/60000]
loss: 0.316218  [38400/60000]
loss: 0.432818  [44800/60000]
loss: 0.416276  [51200/60000]
loss: 0.408843  [57600/60000]
Test Error: 
 Accuracy: 86.0%, Avg loss: 0.391069 

Epoch 3
-------------------------------
loss: 0.226136  [    0/60000]
loss: 0.365053  [ 6400/60000]
loss: 0.295597  [12800/60000]
loss: 0.349099  [19200/60000]
loss: 0.304112  [25600/60000]
loss: 0.300715  [32000/60000]
loss: 0.321007  [38400/60000]
loss: 0.386156  [44800/60000]
loss: 0.390905  [51200/60000]
loss: 0.382463  [57600/60000]
Test Error: 
 Accuracy: 86.2%, Avg loss: 0.376645 

Epoch 4
-------------------------------
loss: 0.208803  [    0/60000]
loss: 0.357585  [ 6400/60000]
loss: 0.284608  [12800/60000]
loss: 0.325022  [19200/60000]
loss: 0.314896  [25600/60000]
loss: 0.279577  [32000/60000]
loss: 0.286266  [38400/60000]
loss: 0.347555  [44800/60000]
loss: 0.359869  [51200/60000]
loss: 0.356931  [57600/60000]
Test Error: 
 Accuracy: 86.7%, Avg loss: 0.368177 

Epoch 5
-------------------------------
loss: 0.178230  [    0/60000]
loss: 0.336060  [ 6400/60000]
loss: 0.250405  [12800/60000]
loss: 0.288135  [19200/60000]
loss: 0.331640  [25600/60000]
loss: 0.273978  [32000/60000]
loss: 0.280665  [38400/60000]
loss: 0.327397  [44800/60000]
loss: 0.332171  [51200/60000]
loss: 0.315691  [57600/60000]
Test Error: 
 Accuracy: 86.6%, Avg loss: 0.371738 

Epoch 6
-------------------------------
loss: 0.176647  [    0/60000]
loss: 0.304183  [ 6400/60000]
loss: 0.248932  [12800/60000]
loss: 0.248566  [19200/60000]
loss: 0.317299  [25600/60000]
loss: 0.257223  [32000/60000]
loss: 0.251304  [38400/60000]
loss: 0.291900  [44800/60000]
loss: 0.337738  [51200/60000]
loss: 0.317880  [57600/60000]
Test Error: 
 Accuracy: 86.7%, Avg loss: 0.378263 

Epoch 7
-------------------------------
loss: 0.184839  [    0/60000]
loss: 0.309191  [ 6400/60000]
loss: 0.230827  [12800/60000]
loss: 0.230613  [19200/60000]
loss: 0.329242  [25600/60000]
loss: 0.274082  [32000/60000]
loss: 0.270300  [38400/60000]
loss: 0.269014  [44800/60000]
loss: 0.319978  [51200/60000]
loss: 0.313484  [57600/60000]
Test Error: 
 Accuracy: 87.0%, Avg loss: 0.374393 

Epoch 8
-------------------------------
loss: 0.156101  [    0/60000]
loss: 0.295511  [ 6400/60000]
loss: 0.197257  [12800/60000]
loss: 0.221259  [19200/60000]
loss: 0.331832  [25600/60000]
loss: 0.285443  [32000/60000]
loss: 0.266160  [38400/60000]
loss: 0.246190  [44800/60000]
loss: 0.299383  [51200/60000]
loss: 0.287935  [57600/60000]
Test Error: 
 Accuracy: 87.0%, Avg loss: 0.378526 

Epoch 9
-------------------------------
loss: 0.184625  [    0/60000]
loss: 0.291682  [ 6400/60000]
loss: 0.225699  [12800/60000]
loss: 0.222840  [19200/60000]
loss: 0.254317  [25600/60000]
loss: 0.288274  [32000/60000]
loss: 0.257805  [38400/60000]
loss: 0.249499  [44800/60000]
loss: 0.290486  [51200/60000]
loss: 0.273249  [57600/60000]
Test Error: 
 Accuracy: 87.1%, Avg loss: 0.384510 

Backprop time:
0.0010523633125466042
Final  epoch:
9 87.1 0.3845099192231324 tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[3, 86.7, 0.368176911287247, tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)]
Fourier Signal Perceptron 512
Epoch 1
-------------------------------
loss: 2.376786  [    0/60000]
loss: 0.577417  [ 6400/60000]
loss: 0.429664  [12800/60000]
loss: 0.530518  [19200/60000]
loss: 0.482863  [25600/60000]
loss: 0.363538  [32000/60000]
loss: 0.338232  [38400/60000]
loss: 0.453266  [44800/60000]
loss: 0.529172  [51200/60000]
loss: 0.532813  [57600/60000]
Test Error: 
 Accuracy: 84.1%, Avg loss: 0.438644 

Epoch 2
-------------------------------
loss: 0.296163  [    0/60000]
loss: 0.345630  [ 6400/60000]
loss: 0.296553  [12800/60000]
loss: 0.392345  [19200/60000]
loss: 0.402953  [25600/60000]
loss: 0.380310  [32000/60000]
loss: 0.278312  [38400/60000]
loss: 0.403389  [44800/60000]
loss: 0.409945  [51200/60000]
loss: 0.477164  [57600/60000]
Test Error: 
 Accuracy: 85.5%, Avg loss: 0.406399 

Epoch 3
-------------------------------
loss: 0.221178  [    0/60000]
loss: 0.295695  [ 6400/60000]
loss: 0.300968  [12800/60000]
loss: 0.349323  [19200/60000]
loss: 0.378725  [25600/60000]
loss: 0.331092  [32000/60000]
loss: 0.248568  [38400/60000]
loss: 0.409642  [44800/60000]
loss: 0.361644  [51200/60000]
loss: 0.422759  [57600/60000]
Test Error: 
 Accuracy: 85.2%, Avg loss: 0.409953 

Epoch 4
-------------------------------
loss: 0.192378  [    0/60000]
loss: 0.288375  [ 6400/60000]
loss: 0.256504  [12800/60000]
loss: 0.250674  [19200/60000]
loss: 0.305432  [25600/60000]
loss: 0.337254  [32000/60000]
loss: 0.268634  [38400/60000]
loss: 0.337771  [44800/60000]
loss: 0.349949  [51200/60000]
loss: 0.430969  [57600/60000]
Test Error: 
 Accuracy: 85.9%, Avg loss: 0.395158 

Epoch 5
-------------------------------
loss: 0.172856  [    0/60000]
loss: 0.277218  [ 6400/60000]
loss: 0.250910  [12800/60000]
loss: 0.222588  [19200/60000]
loss: 0.313436  [25600/60000]
loss: 0.325216  [32000/60000]
loss: 0.239071  [38400/60000]
loss: 0.312244  [44800/60000]
loss: 0.325860  [51200/60000]
loss: 0.390458  [57600/60000]
Test Error: 
 Accuracy: 85.9%, Avg loss: 0.398553 

Epoch 6
-------------------------------
loss: 0.129433  [    0/60000]
loss: 0.242675  [ 6400/60000]
loss: 0.247606  [12800/60000]
loss: 0.165399  [19200/60000]
loss: 0.277494  [25600/60000]
loss: 0.310942  [32000/60000]
loss: 0.212958  [38400/60000]
loss: 0.281187  [44800/60000]
loss: 0.322337  [51200/60000]
loss: 0.386867  [57600/60000]
Test Error: 
 Accuracy: 85.9%, Avg loss: 0.409530 

Epoch 7
-------------------------------
loss: 0.163912  [    0/60000]
loss: 0.221744  [ 6400/60000]
loss: 0.241441  [12800/60000]
loss: 0.190925  [19200/60000]
loss: 0.285513  [25600/60000]
loss: 0.316963  [32000/60000]
loss: 0.239646  [38400/60000]
loss: 0.231999  [44800/60000]
loss: 0.270488  [51200/60000]
loss: 0.288654  [57600/60000]
Test Error: 
 Accuracy: 86.5%, Avg loss: 0.403109 

Epoch 8
-------------------------------
loss: 0.165308  [    0/60000]
loss: 0.202905  [ 6400/60000]
loss: 0.225641  [12800/60000]
loss: 0.161279  [19200/60000]
loss: 0.258511  [25600/60000]
loss: 0.315345  [32000/60000]
loss: 0.213975  [38400/60000]
loss: 0.233213  [44800/60000]
loss: 0.255940  [51200/60000]
loss: 0.360053  [57600/60000]
Test Error: 
 Accuracy: 86.3%, Avg loss: 0.409273 

Epoch 9
-------------------------------
loss: 0.140153  [    0/60000]
loss: 0.248697  [ 6400/60000]
loss: 0.197071  [12800/60000]
loss: 0.161611  [19200/60000]
loss: 0.385703  [25600/60000]
loss: 0.300302  [32000/60000]
loss: 0.224737  [38400/60000]
loss: 0.218385  [44800/60000]
loss: 0.273326  [51200/60000]
loss: 0.277421  [57600/60000]
Test Error: 
 Accuracy: 86.6%, Avg loss: 0.400548 

Backprop time:
0.0010699263344271944
Final  epoch:
9 86.64 0.400547533040973 tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[3, 85.86, 0.395158062220379, tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 1 hidden layer
Epoch 1
-------------------------------
loss: 2.314481  [    0/60000]
loss: 1.108094  [ 6400/60000]
loss: 1.193111  [12800/60000]
loss: 1.368923  [19200/60000]
loss: 1.279527  [25600/60000]
loss: 1.264657  [32000/60000]
loss: 1.171286  [38400/60000]
loss: 1.393799  [44800/60000]
loss: 1.313390  [51200/60000]
loss: 1.288828  [57600/60000]
Test Error: 
 Accuracy: 53.0%, Avg loss: 1.210627 

Epoch 2
-------------------------------
loss: 1.050259  [    0/60000]
loss: 0.977606  [ 6400/60000]
loss: 1.059233  [12800/60000]
loss: 1.248046  [19200/60000]
loss: 1.120776  [25600/60000]
loss: 1.228293  [32000/60000]
loss: 1.136870  [38400/60000]
loss: 1.353289  [44800/60000]
loss: 1.275675  [51200/60000]
loss: 1.249980  [57600/60000]
Test Error: 
 Accuracy: 53.9%, Avg loss: 1.189894 

Epoch 3
-------------------------------
loss: 1.010659  [    0/60000]
loss: 0.977894  [ 6400/60000]
loss: 1.022105  [12800/60000]
loss: 1.199773  [19200/60000]
loss: 1.117690  [25600/60000]
loss: 1.223705  [32000/60000]
loss: 1.125748  [38400/60000]
loss: 1.329241  [44800/60000]
loss: 1.256879  [51200/60000]
loss: 1.185475  [57600/60000]
Test Error: 
 Accuracy: 53.9%, Avg loss: 1.180680 

Epoch 4
-------------------------------
loss: 0.990335  [    0/60000]
loss: 0.958898  [ 6400/60000]
loss: 1.025310  [12800/60000]
loss: 1.155800  [19200/60000]
loss: 1.111765  [25600/60000]
loss: 1.210964  [32000/60000]
loss: 1.108678  [38400/60000]
loss: 1.274636  [44800/60000]
loss: 1.241664  [51200/60000]
loss: 1.181757  [57600/60000]
Test Error: 
 Accuracy: 54.3%, Avg loss: 1.175242 

Epoch 5
-------------------------------
loss: 0.981170  [    0/60000]
loss: 0.941456  [ 6400/60000]
loss: 1.017251  [12800/60000]
loss: 1.134154  [19200/60000]
loss: 1.106524  [25600/60000]
loss: 1.196356  [32000/60000]
loss: 1.092508  [38400/60000]
loss: 1.247574  [44800/60000]
loss: 1.232867  [51200/60000]
loss: 1.161320  [57600/60000]
Test Error: 
 Accuracy: 54.5%, Avg loss: 1.178576 

Epoch 6
-------------------------------
loss: 0.970265  [    0/60000]
loss: 0.929109  [ 6400/60000]
loss: 1.005831  [12800/60000]
loss: 1.113116  [19200/60000]
loss: 1.115600  [25600/60000]
loss: 1.184870  [32000/60000]
loss: 1.085443  [38400/60000]
loss: 1.215506  [44800/60000]
loss: 1.227055  [51200/60000]
loss: 1.120191  [57600/60000]
Test Error: 
 Accuracy: 54.6%, Avg loss: 1.168937 

Epoch 7
-------------------------------
loss: 0.959499  [    0/60000]
loss: 0.929069  [ 6400/60000]
loss: 0.995267  [12800/60000]
loss: 1.097506  [19200/60000]
loss: 1.116511  [25600/60000]
loss: 1.154412  [32000/60000]
loss: 1.065773  [38400/60000]
loss: 1.193386  [44800/60000]
loss: 1.200926  [51200/60000]
loss: 1.118768  [57600/60000]
Test Error: 
 Accuracy: 54.6%, Avg loss: 1.169831 

Epoch 8
-------------------------------
loss: 0.964326  [    0/60000]
loss: 0.920598  [ 6400/60000]
loss: 1.000463  [12800/60000]
loss: 1.086309  [19200/60000]
loss: 1.113180  [25600/60000]
loss: 1.182548  [32000/60000]
loss: 1.064896  [38400/60000]
loss: 1.182607  [44800/60000]
loss: 1.201964  [51200/60000]
loss: 1.089624  [57600/60000]
Test Error: 
 Accuracy: 54.8%, Avg loss: 1.172234 

Epoch 9
-------------------------------
loss: 0.971382  [    0/60000]
loss: 0.906741  [ 6400/60000]
loss: 0.987887  [12800/60000]
loss: 1.082226  [19200/60000]
loss: 1.122160  [25600/60000]
loss: 1.162827  [32000/60000]
loss: 1.043317  [38400/60000]
loss: 1.154088  [44800/60000]
loss: 1.210048  [51200/60000]
loss: 1.083094  [57600/60000]
Test Error: 
 Accuracy: 54.6%, Avg loss: 1.173154 

Backprop time:
0.0013144279763303754
Final  epoch:
9 54.63 1.173153524945496 tensor(1.2832, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 54.559999999999995, 1.1689369636736098, tensor(1.3007, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 2 hidden layer
Epoch 1
-------------------------------
loss: 2.308322  [    0/60000]
loss: 1.071247  [ 6400/60000]
loss: 1.234899  [12800/60000]
loss: 1.392769  [19200/60000]
loss: 1.056738  [25600/60000]
loss: 1.344226  [32000/60000]
loss: 1.051069  [38400/60000]
loss: 1.259828  [44800/60000]
loss: 1.272572  [51200/60000]
loss: 1.320603  [57600/60000]
Test Error: 
 Accuracy: 54.0%, Avg loss: 1.163333 

Epoch 2
-------------------------------
loss: 1.073407  [    0/60000]
loss: 0.912156  [ 6400/60000]
loss: 1.129611  [12800/60000]
loss: 1.291712  [19200/60000]
loss: 0.898531  [25600/60000]
loss: 1.315491  [32000/60000]
loss: 1.008980  [38400/60000]
loss: 1.230796  [44800/60000]
loss: 1.243286  [51200/60000]
loss: 1.250836  [57600/60000]
Test Error: 
 Accuracy: 54.7%, Avg loss: 1.135484 

Epoch 3
-------------------------------
loss: 1.039275  [    0/60000]
loss: 0.900382  [ 6400/60000]
loss: 1.102139  [12800/60000]
loss: 1.211592  [19200/60000]
loss: 0.873911  [25600/60000]
loss: 1.299799  [32000/60000]
loss: 0.982711  [38400/60000]
loss: 1.209472  [44800/60000]
loss: 1.218250  [51200/60000]
loss: 1.206532  [57600/60000]
Test Error: 
 Accuracy: 54.6%, Avg loss: 1.131183 

Epoch 4
-------------------------------
loss: 1.030889  [    0/60000]
loss: 0.904565  [ 6400/60000]
loss: 1.084457  [12800/60000]
loss: 1.178624  [19200/60000]
loss: 0.871255  [25600/60000]
loss: 1.308342  [32000/60000]
loss: 0.985764  [38400/60000]
loss: 1.219096  [44800/60000]
loss: 1.203935  [51200/60000]
loss: 1.200244  [57600/60000]
Test Error: 
 Accuracy: 55.0%, Avg loss: 1.126008 

Epoch 5
-------------------------------
loss: 1.025622  [    0/60000]
loss: 0.893306  [ 6400/60000]
loss: 1.082741  [12800/60000]
loss: 1.154557  [19200/60000]
loss: 0.830209  [25600/60000]
loss: 1.285519  [32000/60000]
loss: 0.990683  [38400/60000]
loss: 1.222177  [44800/60000]
loss: 1.195674  [51200/60000]
loss: 1.195932  [57600/60000]
Test Error: 
 Accuracy: 55.0%, Avg loss: 1.119130 

Epoch 6
-------------------------------
loss: 1.005102  [    0/60000]
loss: 0.881946  [ 6400/60000]
loss: 1.079887  [12800/60000]
loss: 1.147483  [19200/60000]
loss: 0.856519  [25600/60000]
loss: 1.083128  [32000/60000]
loss: 0.876886  [38400/60000]
loss: 1.101677  [44800/60000]
loss: 0.892227  [51200/60000]
loss: 1.078984  [57600/60000]
Test Error: 
 Accuracy: 63.8%, Avg loss: 0.946461 

Epoch 7
-------------------------------
loss: 0.653417  [    0/60000]
loss: 0.635236  [ 6400/60000]
loss: 0.751407  [12800/60000]
loss: 0.926205  [19200/60000]
loss: 0.831772  [25600/60000]
loss: 1.044376  [32000/60000]
loss: 0.857347  [38400/60000]
loss: 1.078095  [44800/60000]
loss: 0.862161  [51200/60000]
loss: 1.034440  [57600/60000]
Test Error: 
 Accuracy: 63.9%, Avg loss: 0.936397 

Epoch 8
-------------------------------
loss: 0.647183  [    0/60000]
loss: 0.590467  [ 6400/60000]
loss: 0.729534  [12800/60000]
loss: 0.928936  [19200/60000]
loss: 0.823056  [25600/60000]
loss: 1.029864  [32000/60000]
loss: 0.840156  [38400/60000]
loss: 1.043663  [44800/60000]
loss: 0.835420  [51200/60000]
loss: 1.007412  [57600/60000]
Test Error: 
 Accuracy: 63.5%, Avg loss: 0.953997 

Epoch 9
-------------------------------
loss: 0.647475  [    0/60000]
loss: 0.610003  [ 6400/60000]
loss: 0.730859  [12800/60000]
loss: 0.880740  [19200/60000]
loss: 0.812713  [25600/60000]
loss: 1.015499  [32000/60000]
loss: 0.817621  [38400/60000]
loss: 0.998546  [44800/60000]
loss: 0.866710  [51200/60000]
loss: 0.959563  [57600/60000]
Test Error: 
 Accuracy: 63.5%, Avg loss: 0.952225 

Backprop time:
0.00179971857281043
Final  epoch:
9 63.53 0.9522246562751235 tensor(1.0175, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[6, 63.85999999999999, 0.9363969131639809, tensor(1.0439, device='cuda:0', grad_fn=<NllLossBackward>)]
