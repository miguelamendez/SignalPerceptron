Using cuda device
Learnable Parameters for MNIST models:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
101632 	 406528 	 407050 	 669706
torch.Size([64, 1, 28, 28])
torch.Size([1, 28, 28])
Forward time for MNIST models:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
0.00010633468627929688 	 9.417533874511719e-05 	 0.00014257431030273438 	 0.00021409988403320312
Forward time for MNIST models FS Timer class:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
8.891868591308594e-05 	 8.93096923828125e-05 	 0.0001473996639251709 	 0.00021479487419128417
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                   model_inference mlp1         5.59%     134.000us        99.12%       2.377ms       2.377ms       0.000us         0.00%      18.000us      18.000us             1  
                                           aten::linear         0.88%      21.000us        86.99%       2.086ms       1.043ms       0.000us         0.00%      16.000us       8.000us             2  
                                            aten::addmm        10.72%     257.000us        84.65%       2.030ms       1.015ms      16.000us        88.89%      16.000us       8.000us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      15.000us        83.33%      15.000us       7.500us             2  
                                        aten::clamp_min         2.17%      52.000us         7.38%     177.000us      44.250us       2.000us        11.11%       4.000us       1.000us             4  
                                             aten::relu         1.46%      35.000us         5.84%     140.000us      70.000us       0.000us         0.00%       2.000us       1.000us             2  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us        11.11%       2.000us       1.000us             2  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         5.56%       1.000us       0.500us             2  
                                            aten::zeros         0.29%       7.000us         0.67%      16.000us      16.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         0.92%      22.000us         0.92%      22.000us       5.500us       0.000us         0.00%       0.000us       0.000us             4  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.398ms
Self CUDA time total: 18.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                   model_inference mlp2         8.94%     226.000us        99.09%       2.504ms       2.504ms       0.000us         0.00%      25.000us      25.000us             1  
                                           aten::linear         1.07%      27.000us        82.23%       2.078ms     692.667us       0.000us         0.00%      24.000us       8.000us             3  
                                            aten::addmm        11.36%     287.000us        79.18%       2.001ms     667.000us      24.000us        96.00%      24.000us       8.000us             3  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      22.000us        88.00%      22.000us       7.333us             3  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         8.00%       2.000us       0.667us             3  
                                        aten::clamp_min         2.69%      68.000us         9.50%     240.000us      40.000us       1.000us         4.00%       2.000us       0.333us             6  
                                             aten::relu         1.62%      41.000us         7.24%     183.000us      61.000us       0.000us         0.00%       1.000us       0.333us             3  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         4.00%       1.000us       0.333us             3  
                                            aten::zeros         0.32%       8.000us         0.67%      17.000us      17.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         1.07%      27.000us         1.07%      27.000us       5.400us       0.000us         0.00%       0.000us       0.000us             5  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.527ms
Self CUDA time total: 25.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                 model_inference fsp128         5.39%     118.000us        98.72%       2.162ms       2.162ms       0.000us         0.00%       7.000us       7.000us             1  
                                           aten::linear         0.96%      21.000us        90.32%       1.978ms     989.000us       0.000us         0.00%       6.000us       3.000us             2  
                                           aten::matmul         0.68%      15.000us        87.49%       1.916ms     958.000us       0.000us         0.00%       6.000us       3.000us             2  
                                               aten::mm        10.59%     232.000us        86.80%       1.901ms     950.500us       6.000us        85.71%       6.000us       3.000us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us        85.71%       6.000us       3.000us             2  
                                              aten::cos         1.74%      38.000us         2.37%      52.000us      52.000us       1.000us        14.29%       1.000us       1.000us             1  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us        14.29%       1.000us       1.000us             1  
                                            aten::zeros         0.41%       9.000us         1.05%      23.000us      23.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         1.42%      31.000us         1.42%      31.000us       7.750us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.09%       2.000us         0.09%       2.000us       2.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.190ms
Self CUDA time total: 7.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                 model_inference fsp512         5.43%     119.000us        99.00%       2.168ms       2.168ms       0.000us         0.00%      17.000us      17.000us             1  
                                           aten::linear         0.87%      19.000us        90.41%       1.980ms     990.000us       0.000us         0.00%      15.000us       7.500us             2  
                                           aten::matmul         0.68%      15.000us        87.72%       1.921ms     960.500us       0.000us         0.00%      15.000us       7.500us             2  
                                               aten::mm        10.87%     238.000us        87.03%       1.906ms     953.000us      15.000us        88.24%      15.000us       7.500us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      15.000us        88.24%      15.000us       7.500us             2  
                                              aten::cos         1.74%      38.000us         2.37%      52.000us      52.000us       2.000us        11.76%       2.000us       2.000us             1  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us        11.76%       2.000us       2.000us             1  
                                            aten::zeros         0.37%       8.000us         0.78%      17.000us      17.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         1.37%      30.000us         1.37%      30.000us       7.500us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.14%       3.000us         0.14%       3.000us       3.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.190ms
Self CUDA time total: 17.000us

Optimizer: Adam ,lr: 0.001
Training models with MNIST DATASET :
Fourier Signal Perceptron 128
Epoch 1
-------------------------------
loss: 2.558245  [    0/60000]
loss: 0.461586  [ 6400/60000]
loss: 0.332462  [12800/60000]
loss: 0.346834  [19200/60000]
loss: 0.195911  [25600/60000]
loss: 0.315231  [32000/60000]
loss: 0.135820  [38400/60000]
loss: 0.287566  [44800/60000]
loss: 0.242051  [51200/60000]
loss: 0.227813  [57600/60000]
Test Error: 
 Accuracy: 95.3%, Avg loss: 0.161258 

Epoch 2
-------------------------------
loss: 0.119518  [    0/60000]
loss: 0.163257  [ 6400/60000]
loss: 0.099408  [12800/60000]
loss: 0.104098  [19200/60000]
loss: 0.110365  [25600/60000]
loss: 0.206933  [32000/60000]
loss: 0.075969  [38400/60000]
loss: 0.173576  [44800/60000]
loss: 0.154157  [51200/60000]
loss: 0.109981  [57600/60000]
Test Error: 
 Accuracy: 96.5%, Avg loss: 0.110465 

Epoch 3
-------------------------------
loss: 0.067427  [    0/60000]
loss: 0.121239  [ 6400/60000]
loss: 0.076082  [12800/60000]
loss: 0.039564  [19200/60000]
loss: 0.074752  [25600/60000]
loss: 0.112343  [32000/60000]
loss: 0.068131  [38400/60000]
loss: 0.111873  [44800/60000]
loss: 0.130066  [51200/60000]
loss: 0.050707  [57600/60000]
Test Error: 
 Accuracy: 97.1%, Avg loss: 0.091941 

Epoch 4
-------------------------------
loss: 0.049470  [    0/60000]
loss: 0.099954  [ 6400/60000]
loss: 0.065717  [12800/60000]
loss: 0.023146  [19200/60000]
loss: 0.045713  [25600/60000]
loss: 0.062461  [32000/60000]
loss: 0.049119  [38400/60000]
loss: 0.070224  [44800/60000]
loss: 0.115443  [51200/60000]
loss: 0.026551  [57600/60000]
Test Error: 
 Accuracy: 97.3%, Avg loss: 0.086404 

Epoch 5
-------------------------------
loss: 0.031397  [    0/60000]
loss: 0.082806  [ 6400/60000]
loss: 0.061829  [12800/60000]
loss: 0.016373  [19200/60000]
loss: 0.033085  [25600/60000]
loss: 0.039096  [32000/60000]
loss: 0.028244  [38400/60000]
loss: 0.044296  [44800/60000]
loss: 0.092726  [51200/60000]
loss: 0.018086  [57600/60000]
Test Error: 
 Accuracy: 97.4%, Avg loss: 0.084318 

Epoch 6
-------------------------------
loss: 0.017783  [    0/60000]
loss: 0.052187  [ 6400/60000]
loss: 0.062152  [12800/60000]
loss: 0.013261  [19200/60000]
loss: 0.024704  [25600/60000]
loss: 0.024022  [32000/60000]
loss: 0.018287  [38400/60000]
loss: 0.030269  [44800/60000]
loss: 0.075183  [51200/60000]
loss: 0.016647  [57600/60000]
Test Error: 
 Accuracy: 97.4%, Avg loss: 0.083779 

Epoch 7
-------------------------------
loss: 0.013814  [    0/60000]
loss: 0.023911  [ 6400/60000]
loss: 0.052565  [12800/60000]
loss: 0.008888  [19200/60000]
loss: 0.017738  [25600/60000]
loss: 0.013678  [32000/60000]
loss: 0.010757  [38400/60000]
loss: 0.018457  [44800/60000]
loss: 0.065457  [51200/60000]
loss: 0.012493  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.083794 

Epoch 8
-------------------------------
loss: 0.011201  [    0/60000]
loss: 0.010201  [ 6400/60000]
loss: 0.045288  [12800/60000]
loss: 0.005665  [19200/60000]
loss: 0.011959  [25600/60000]
loss: 0.009109  [32000/60000]
loss: 0.007062  [38400/60000]
loss: 0.012579  [44800/60000]
loss: 0.057739  [51200/60000]
loss: 0.010302  [57600/60000]
Test Error: 
 Accuracy: 97.4%, Avg loss: 0.086001 

Epoch 9
-------------------------------
loss: 0.005538  [    0/60000]
loss: 0.006936  [ 6400/60000]
loss: 0.035345  [12800/60000]
loss: 0.004576  [19200/60000]
loss: 0.008325  [25600/60000]
loss: 0.005563  [32000/60000]
loss: 0.003248  [38400/60000]
loss: 0.008407  [44800/60000]
loss: 0.029456  [51200/60000]
loss: 0.007551  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.086124 

Backprop time:
0.0010980813318563
Final  epoch:
9 97.46000000000001 0.08612351536245276 tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 97.44, 0.08377909860877723, tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)]
Fourier Signal Perceptron 512
Epoch 1
-------------------------------
loss: 2.452194  [    0/60000]
loss: 0.324756  [ 6400/60000]
loss: 0.246967  [12800/60000]
loss: 0.237041  [19200/60000]
loss: 0.138141  [25600/60000]
loss: 0.270578  [32000/60000]
loss: 0.103596  [38400/60000]
loss: 0.270244  [44800/60000]
loss: 0.218691  [51200/60000]
loss: 0.226003  [57600/60000]
Test Error: 
 Accuracy: 96.2%, Avg loss: 0.121034 

Epoch 2
-------------------------------
loss: 0.104916  [    0/60000]
loss: 0.144750  [ 6400/60000]
loss: 0.090127  [12800/60000]
loss: 0.067735  [19200/60000]
loss: 0.060105  [25600/60000]
loss: 0.140508  [32000/60000]
loss: 0.073137  [38400/60000]
loss: 0.086981  [44800/60000]
loss: 0.096249  [51200/60000]
loss: 0.099031  [57600/60000]
Test Error: 
 Accuracy: 97.2%, Avg loss: 0.085000 

Epoch 3
-------------------------------
loss: 0.070248  [    0/60000]
loss: 0.074258  [ 6400/60000]
loss: 0.062572  [12800/60000]
loss: 0.027604  [19200/60000]
loss: 0.045175  [25600/60000]
loss: 0.090732  [32000/60000]
loss: 0.036118  [38400/60000]
loss: 0.031389  [44800/60000]
loss: 0.077383  [51200/60000]
loss: 0.020409  [57600/60000]
Test Error: 
 Accuracy: 97.7%, Avg loss: 0.069765 

Epoch 4
-------------------------------
loss: 0.037340  [    0/60000]
loss: 0.035949  [ 6400/60000]
loss: 0.036376  [12800/60000]
loss: 0.027188  [19200/60000]
loss: 0.029446  [25600/60000]
loss: 0.042227  [32000/60000]
loss: 0.013921  [38400/60000]
loss: 0.018313  [44800/60000]
loss: 0.062127  [51200/60000]
loss: 0.011806  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.066721 

Epoch 5
-------------------------------
loss: 0.020378  [    0/60000]
loss: 0.011562  [ 6400/60000]
loss: 0.024157  [12800/60000]
loss: 0.012925  [19200/60000]
loss: 0.009593  [25600/60000]
loss: 0.017694  [32000/60000]
loss: 0.010689  [38400/60000]
loss: 0.015732  [44800/60000]
loss: 0.047267  [51200/60000]
loss: 0.005403  [57600/60000]
Test Error: 
 Accuracy: 98.0%, Avg loss: 0.064978 

Epoch 6
-------------------------------
loss: 0.008369  [    0/60000]
loss: 0.003496  [ 6400/60000]
loss: 0.015736  [12800/60000]
loss: 0.009100  [19200/60000]
loss: 0.004253  [25600/60000]
loss: 0.016031  [32000/60000]
loss: 0.001996  [38400/60000]
loss: 0.010043  [44800/60000]
loss: 0.012566  [51200/60000]
loss: 0.001888  [57600/60000]
Test Error: 
 Accuracy: 98.1%, Avg loss: 0.064560 

Epoch 7
-------------------------------
loss: 0.009226  [    0/60000]
loss: 0.003691  [ 6400/60000]
loss: 0.007594  [12800/60000]
loss: 0.007931  [19200/60000]
loss: 0.009649  [25600/60000]
loss: 0.001772  [32000/60000]
loss: 0.002816  [38400/60000]
loss: 0.008570  [44800/60000]
loss: 0.013674  [51200/60000]
loss: 0.003347  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.082045 

Epoch 8
-------------------------------
loss: 0.002265  [    0/60000]
loss: 0.048216  [ 6400/60000]
loss: 0.035921  [12800/60000]
loss: 0.005593  [19200/60000]
loss: 0.003151  [25600/60000]
loss: 0.001971  [32000/60000]
loss: 0.001529  [38400/60000]
loss: 0.007481  [44800/60000]
loss: 0.022655  [51200/60000]
loss: 0.014868  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.071772 

Epoch 9
-------------------------------
loss: 0.009802  [    0/60000]
loss: 0.001044  [ 6400/60000]
loss: 0.009927  [12800/60000]
loss: 0.014082  [19200/60000]
loss: 0.005972  [25600/60000]
loss: 0.002476  [32000/60000]
loss: 0.000535  [38400/60000]
loss: 0.006643  [44800/60000]
loss: 0.001398  [51200/60000]
loss: 0.000777  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.071222 

Backprop time:
0.0009751515601997945
Final  epoch:
9 97.89999999999999 0.07122223414092384 tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 98.1, 0.06455959779880217]
MLP 1 hidden layer Signal Perceptron
Epoch 1
-------------------------------
loss: 2.295707  [    0/60000]
loss: 0.626232  [ 6400/60000]
loss: 0.883552  [12800/60000]
loss: 0.556804  [19200/60000]
loss: 0.621569  [25600/60000]
loss: 0.748065  [32000/60000]
loss: 0.749596  [38400/60000]
loss: 0.540450  [44800/60000]
loss: 0.728856  [51200/60000]
loss: 0.316914  [57600/60000]
Test Error: 
 Accuracy: 85.7%, Avg loss: 0.398087 

Epoch 2
-------------------------------
loss: 0.337447  [    0/60000]
loss: 0.308035  [ 6400/60000]
loss: 0.323382  [12800/60000]
loss: 0.301523  [19200/60000]
loss: 0.295392  [25600/60000]
loss: 0.371950  [32000/60000]
loss: 0.334906  [38400/60000]
loss: 0.388162  [44800/60000]
loss: 0.377032  [51200/60000]
loss: 0.263633  [57600/60000]
Test Error: 
 Accuracy: 87.6%, Avg loss: 0.321260 

Epoch 3
-------------------------------
loss: 0.272682  [    0/60000]
loss: 0.240127  [ 6400/60000]
loss: 0.295560  [12800/60000]
loss: 0.274453  [19200/60000]
loss: 0.275161  [25600/60000]
loss: 0.322972  [32000/60000]
loss: 0.312073  [38400/60000]
loss: 0.329331  [44800/60000]
loss: 0.295839  [51200/60000]
loss: 0.220150  [57600/60000]
Test Error: 
 Accuracy: 88.0%, Avg loss: 0.304709 

Epoch 4
-------------------------------
loss: 0.246777  [    0/60000]
loss: 0.232568  [ 6400/60000]
loss: 0.284961  [12800/60000]
loss: 0.209972  [19200/60000]
loss: 0.263834  [25600/60000]
loss: 0.303445  [32000/60000]
loss: 0.304532  [38400/60000]
loss: 0.286343  [44800/60000]
loss: 0.279182  [51200/60000]
loss: 0.183097  [57600/60000]
Test Error: 
 Accuracy: 88.4%, Avg loss: 0.296226 

Epoch 5
-------------------------------
loss: 0.232608  [    0/60000]
loss: 0.225875  [ 6400/60000]
loss: 0.271559  [12800/60000]
loss: 0.181418  [19200/60000]
loss: 0.246290  [25600/60000]
loss: 0.286589  [32000/60000]
loss: 0.299057  [38400/60000]
loss: 0.248968  [44800/60000]
loss: 0.260892  [51200/60000]
loss: 0.166819  [57600/60000]
Test Error: 
 Accuracy: 88.1%, Avg loss: 0.306055 

Epoch 6
-------------------------------
loss: 0.225971  [    0/60000]
loss: 0.226144  [ 6400/60000]
loss: 0.256192  [12800/60000]
loss: 0.182679  [19200/60000]
loss: 0.235565  [25600/60000]
loss: 0.250174  [32000/60000]
loss: 0.293906  [38400/60000]
loss: 0.246022  [44800/60000]
loss: 0.251008  [51200/60000]
loss: 0.155398  [57600/60000]
Test Error: 
 Accuracy: 88.5%, Avg loss: 0.292817 

Epoch 7
-------------------------------
loss: 0.222343  [    0/60000]
loss: 0.221508  [ 6400/60000]
loss: 0.246052  [12800/60000]
loss: 0.161761  [19200/60000]
loss: 0.222159  [25600/60000]
loss: 0.231927  [32000/60000]
loss: 0.291055  [38400/60000]
loss: 0.226101  [44800/60000]
loss: 0.236862  [51200/60000]
loss: 0.152791  [57600/60000]
Test Error: 
 Accuracy: 88.6%, Avg loss: 0.293216 

Epoch 8
-------------------------------
loss: 0.238312  [    0/60000]
loss: 0.224491  [ 6400/60000]
loss: 0.241499  [12800/60000]
loss: 0.149994  [19200/60000]
loss: 0.220050  [25600/60000]
loss: 0.226932  [32000/60000]
loss: 0.309967  [38400/60000]
loss: 0.223171  [44800/60000]
loss: 0.224792  [51200/60000]
loss: 0.145823  [57600/60000]
Test Error: 
 Accuracy: 88.8%, Avg loss: 0.291448 

Epoch 9
-------------------------------
loss: 0.221033  [    0/60000]
loss: 0.219037  [ 6400/60000]
loss: 0.250545  [12800/60000]
loss: 0.158927  [19200/60000]
loss: 0.223216  [25600/60000]
loss: 0.222971  [32000/60000]
loss: 0.297734  [38400/60000]
loss: 0.231096  [44800/60000]
loss: 0.253876  [51200/60000]
loss: 0.152417  [57600/60000]
Test Error: 
 Accuracy: 88.6%, Avg loss: 0.297284 

Backprop time:
0.0012804947315135631
Final  epoch:
9 88.57000000000001 0.2972836361094645 tensor(0.3601, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[7, 88.83, 0.2914484177899968, tensor(0.3601, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 2 hidden layer Signal Perceptron
Epoch 1
-------------------------------
loss: 2.306890  [    0/60000]
loss: 1.638395  [ 6400/60000]
loss: 2.009937  [12800/60000]
loss: 1.451800  [19200/60000]
loss: 1.625439  [25600/60000]
loss: 1.676308  [32000/60000]
loss: 1.822805  [38400/60000]
loss: 1.883647  [44800/60000]
loss: 1.730348  [51200/60000]
loss: 1.841982  [57600/60000]
Test Error: 
 Accuracy: 28.6%, Avg loss: 1.692785 

Epoch 2
-------------------------------
loss: 1.521637  [    0/60000]
loss: 1.443560  [ 6400/60000]
loss: 1.981017  [12800/60000]
loss: 1.407931  [19200/60000]
loss: 1.591619  [25600/60000]
loss: 1.653992  [32000/60000]
loss: 1.802997  [38400/60000]
loss: 1.838724  [44800/60000]
loss: 1.718033  [51200/60000]
loss: 1.797810  [57600/60000]
Test Error: 
 Accuracy: 29.0%, Avg loss: 1.682561 

Epoch 3
-------------------------------
loss: 1.505732  [    0/60000]
loss: 1.406673  [ 6400/60000]
loss: 1.978808  [12800/60000]
loss: 1.404629  [19200/60000]
loss: 1.564756  [25600/60000]
loss: 1.635230  [32000/60000]
loss: 1.806594  [38400/60000]
loss: 1.843472  [44800/60000]
loss: 1.742197  [51200/60000]
loss: 1.764916  [57600/60000]
Test Error: 
 Accuracy: 29.4%, Avg loss: 1.656770 

Epoch 4
-------------------------------
loss: 1.521590  [    0/60000]
loss: 1.400104  [ 6400/60000]
loss: 1.942559  [12800/60000]
loss: 1.404529  [19200/60000]
loss: 1.596926  [25600/60000]
loss: 1.585925  [32000/60000]
loss: 1.802250  [38400/60000]
loss: 1.818710  [44800/60000]
loss: 1.693094  [51200/60000]
loss: 1.782571  [57600/60000]
Test Error: 
 Accuracy: 28.8%, Avg loss: 1.664765 

Epoch 5
-------------------------------
loss: 1.478153  [    0/60000]
loss: 1.398440  [ 6400/60000]
loss: 1.930821  [12800/60000]
loss: 1.416522  [19200/60000]
loss: 1.518484  [25600/60000]
loss: 1.583045  [32000/60000]
loss: 1.804063  [38400/60000]
loss: 1.821049  [44800/60000]
loss: 1.638535  [51200/60000]
loss: 1.763834  [57600/60000]
Test Error: 
 Accuracy: 29.4%, Avg loss: 1.653335 

Epoch 6
-------------------------------
loss: 1.484521  [    0/60000]
loss: 1.411277  [ 6400/60000]
loss: 1.915884  [12800/60000]
loss: 1.403193  [19200/60000]
loss: 1.511081  [25600/60000]
loss: 1.590004  [32000/60000]
loss: 1.798896  [38400/60000]
loss: 1.821460  [44800/60000]
loss: 1.696484  [51200/60000]
loss: 1.777216  [57600/60000]
Test Error: 
 Accuracy: 29.4%, Avg loss: 1.652080 

Epoch 7
-------------------------------
loss: 1.489850  [    0/60000]
loss: 1.398212  [ 6400/60000]
loss: 1.887844  [12800/60000]
loss: 1.403958  [19200/60000]
loss: 1.530217  [25600/60000]
loss: 1.583192  [32000/60000]
loss: 1.798906  [38400/60000]
loss: 1.811754  [44800/60000]
loss: 1.656731  [51200/60000]
loss: 1.763434  [57600/60000]
Test Error: 
 Accuracy: 29.3%, Avg loss: 1.662508 

Epoch 8
-------------------------------
loss: 1.502743  [    0/60000]
loss: 1.368580  [ 6400/60000]
loss: 1.889668  [12800/60000]
loss: 1.403236  [19200/60000]
loss: 1.522803  [25600/60000]
loss: 1.583052  [32000/60000]
loss: 1.798905  [38400/60000]
loss: 1.801519  [44800/60000]
loss: 1.691803  [51200/60000]
loss: 1.777064  [57600/60000]
Test Error: 
 Accuracy: 29.4%, Avg loss: 1.654679 

Epoch 9
-------------------------------
loss: 1.475243  [    0/60000]
loss: 1.370266  [ 6400/60000]
loss: 1.907590  [12800/60000]
loss: 1.403178  [19200/60000]
loss: 1.520736  [25600/60000]
loss: 1.583028  [32000/60000]
loss: 1.798898  [38400/60000]
loss: 1.856408  [44800/60000]
loss: 1.618445  [51200/60000]
loss: 1.762954  [57600/60000]
Test Error: 
 Accuracy: 29.4%, Avg loss: 1.653974 

Backprop time:
0.0015847997285833406
Final  epoch:
9 29.39 1.653973735821475 tensor(1.8709, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 29.39, 1.6520797090165933, tensor(1.8709, device='cuda:0', grad_fn=<NllLossBackward>)]
Training models with FashionMNIST DATASET :
Fourier Signal Perceptron 128
Epoch 1
-------------------------------
loss: 2.350501  [    0/60000]
loss: 0.574348  [ 6400/60000]
loss: 0.377981  [12800/60000]
loss: 0.551894  [19200/60000]
loss: 0.482035  [25600/60000]
loss: 0.410056  [32000/60000]
loss: 0.364441  [38400/60000]
loss: 0.522016  [44800/60000]
loss: 0.529529  [51200/60000]
loss: 0.498932  [57600/60000]
Test Error: 
 Accuracy: 84.9%, Avg loss: 0.416134 

Epoch 2
-------------------------------
loss: 0.286001  [    0/60000]
loss: 0.402948  [ 6400/60000]
loss: 0.302089  [12800/60000]
loss: 0.404797  [19200/60000]
loss: 0.372158  [25600/60000]
loss: 0.345717  [32000/60000]
loss: 0.304234  [38400/60000]
loss: 0.413588  [44800/60000]
loss: 0.427108  [51200/60000]
loss: 0.433928  [57600/60000]
Test Error: 
 Accuracy: 85.4%, Avg loss: 0.401217 

Epoch 3
-------------------------------
loss: 0.237349  [    0/60000]
loss: 0.339398  [ 6400/60000]
loss: 0.300929  [12800/60000]
loss: 0.339962  [19200/60000]
loss: 0.304698  [25600/60000]
loss: 0.305811  [32000/60000]
loss: 0.286170  [38400/60000]
loss: 0.368753  [44800/60000]
loss: 0.332725  [51200/60000]
loss: 0.373969  [57600/60000]
Test Error: 
 Accuracy: 85.4%, Avg loss: 0.399792 

Epoch 4
-------------------------------
loss: 0.234573  [    0/60000]
loss: 0.314023  [ 6400/60000]
loss: 0.275224  [12800/60000]
loss: 0.283104  [19200/60000]
loss: 0.272410  [25600/60000]
loss: 0.286191  [32000/60000]
loss: 0.245004  [38400/60000]
loss: 0.347436  [44800/60000]
loss: 0.328842  [51200/60000]
loss: 0.387424  [57600/60000]
Test Error: 
 Accuracy: 85.8%, Avg loss: 0.394884 

Epoch 5
-------------------------------
loss: 0.226427  [    0/60000]
loss: 0.318928  [ 6400/60000]
loss: 0.253136  [12800/60000]
loss: 0.252229  [19200/60000]
loss: 0.284665  [25600/60000]
loss: 0.298886  [32000/60000]
loss: 0.262651  [38400/60000]
loss: 0.318913  [44800/60000]
loss: 0.281204  [51200/60000]
loss: 0.337152  [57600/60000]
Test Error: 
 Accuracy: 86.0%, Avg loss: 0.391809 

Epoch 6
-------------------------------
loss: 0.201568  [    0/60000]
loss: 0.311962  [ 6400/60000]
loss: 0.247721  [12800/60000]
loss: 0.215269  [19200/60000]
loss: 0.236768  [25600/60000]
loss: 0.307492  [32000/60000]
loss: 0.239028  [38400/60000]
loss: 0.281715  [44800/60000]
loss: 0.245194  [51200/60000]
loss: 0.312049  [57600/60000]
Test Error: 
 Accuracy: 86.3%, Avg loss: 0.387820 

Epoch 7
-------------------------------
loss: 0.193992  [    0/60000]
loss: 0.306492  [ 6400/60000]
loss: 0.238617  [12800/60000]
loss: 0.190916  [19200/60000]
loss: 0.218106  [25600/60000]
loss: 0.318749  [32000/60000]
loss: 0.256370  [38400/60000]
loss: 0.273471  [44800/60000]
loss: 0.261788  [51200/60000]
loss: 0.294621  [57600/60000]
Test Error: 
 Accuracy: 86.5%, Avg loss: 0.388532 

Epoch 8
-------------------------------
loss: 0.180827  [    0/60000]
loss: 0.328012  [ 6400/60000]
loss: 0.239524  [12800/60000]
loss: 0.187529  [19200/60000]
loss: 0.226742  [25600/60000]
loss: 0.317295  [32000/60000]
loss: 0.234039  [38400/60000]
loss: 0.283903  [44800/60000]
loss: 0.319929  [51200/60000]
loss: 0.284718  [57600/60000]
Test Error: 
 Accuracy: 86.5%, Avg loss: 0.394253 

Epoch 9
-------------------------------
loss: 0.183928  [    0/60000]
loss: 0.282171  [ 6400/60000]
loss: 0.233864  [12800/60000]
loss: 0.162525  [19200/60000]
loss: 0.226225  [25600/60000]
loss: 0.307464  [32000/60000]
loss: 0.237180  [38400/60000]
loss: 0.246368  [44800/60000]
loss: 0.310837  [51200/60000]
loss: 0.272891  [57600/60000]
Test Error: 
 Accuracy: 86.2%, Avg loss: 0.404615 

Backprop time:
0.0009353625720723832
Final  epoch:
9 86.18 0.4046145249514063 tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 86.33999999999999, 0.38782023908985647, tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)]
Fourier Signal Perceptron 512
Epoch 1
-------------------------------
loss: 2.436107  [    0/60000]
loss: 0.543339  [ 6400/60000]
loss: 0.446157  [12800/60000]
loss: 0.522856  [19200/60000]
loss: 0.462085  [25600/60000]
loss: 0.442150  [32000/60000]
loss: 0.380514  [38400/60000]
loss: 0.520277  [44800/60000]
loss: 0.492057  [51200/60000]
loss: 0.518529  [57600/60000]
Test Error: 
 Accuracy: 84.7%, Avg loss: 0.428064 

Epoch 2
-------------------------------
loss: 0.254041  [    0/60000]
loss: 0.381328  [ 6400/60000]
loss: 0.314983  [12800/60000]
loss: 0.356714  [19200/60000]
loss: 0.406443  [25600/60000]
loss: 0.335330  [32000/60000]
loss: 0.290074  [38400/60000]
loss: 0.436899  [44800/60000]
loss: 0.396165  [51200/60000]
loss: 0.492227  [57600/60000]
Test Error: 
 Accuracy: 85.5%, Avg loss: 0.411610 

Epoch 3
-------------------------------
loss: 0.200036  [    0/60000]
loss: 0.327156  [ 6400/60000]
loss: 0.319835  [12800/60000]
loss: 0.282012  [19200/60000]
loss: 0.358565  [25600/60000]
loss: 0.270617  [32000/60000]
loss: 0.214092  [38400/60000]
loss: 0.342514  [44800/60000]
loss: 0.330834  [51200/60000]
loss: 0.401187  [57600/60000]
Test Error: 
 Accuracy: 85.7%, Avg loss: 0.406277 

Epoch 4
-------------------------------
loss: 0.213908  [    0/60000]
loss: 0.302482  [ 6400/60000]
loss: 0.255703  [12800/60000]
loss: 0.252827  [19200/60000]
loss: 0.323165  [25600/60000]
loss: 0.309984  [32000/60000]
loss: 0.222678  [38400/60000]
loss: 0.310617  [44800/60000]
loss: 0.302804  [51200/60000]
loss: 0.403585  [57600/60000]
Test Error: 
 Accuracy: 86.0%, Avg loss: 0.398767 

Epoch 5
-------------------------------
loss: 0.215381  [    0/60000]
loss: 0.256858  [ 6400/60000]
loss: 0.261912  [12800/60000]
loss: 0.248045  [19200/60000]
loss: 0.292719  [25600/60000]
loss: 0.329022  [32000/60000]
loss: 0.227053  [38400/60000]
loss: 0.276430  [44800/60000]
loss: 0.334588  [51200/60000]
loss: 0.428579  [57600/60000]
Test Error: 
 Accuracy: 86.1%, Avg loss: 0.397804 

Epoch 6
-------------------------------
loss: 0.167500  [    0/60000]
loss: 0.216657  [ 6400/60000]
loss: 0.193701  [12800/60000]
loss: 0.218492  [19200/60000]
loss: 0.282987  [25600/60000]
loss: 0.271209  [32000/60000]
loss: 0.201750  [38400/60000]
loss: 0.271018  [44800/60000]
loss: 0.299247  [51200/60000]
loss: 0.362635  [57600/60000]
Test Error: 
 Accuracy: 86.4%, Avg loss: 0.388066 

Epoch 7
-------------------------------
loss: 0.184793  [    0/60000]
loss: 0.222610  [ 6400/60000]
loss: 0.223820  [12800/60000]
loss: 0.233107  [19200/60000]
loss: 0.250426  [25600/60000]
loss: 0.288715  [32000/60000]
loss: 0.154988  [38400/60000]
loss: 0.275669  [44800/60000]
loss: 0.278788  [51200/60000]
loss: 0.375464  [57600/60000]
Test Error: 
 Accuracy: 86.5%, Avg loss: 0.392264 

Epoch 8
-------------------------------
loss: 0.208683  [    0/60000]
loss: 0.190967  [ 6400/60000]
loss: 0.238372  [12800/60000]
loss: 0.253552  [19200/60000]
loss: 0.250875  [25600/60000]
loss: 0.297967  [32000/60000]
loss: 0.175012  [38400/60000]
loss: 0.225834  [44800/60000]
loss: 0.222327  [51200/60000]
loss: 0.341598  [57600/60000]
Test Error: 
 Accuracy: 87.1%, Avg loss: 0.390547 

Epoch 9
-------------------------------
loss: 0.171968  [    0/60000]
loss: 0.282571  [ 6400/60000]
loss: 0.198381  [12800/60000]
loss: 0.219524  [19200/60000]
loss: 0.237921  [25600/60000]
loss: 0.255048  [32000/60000]
loss: 0.188773  [38400/60000]
loss: 0.240750  [44800/60000]
loss: 0.217199  [51200/60000]
loss: 0.390307  [57600/60000]
Test Error: 
 Accuracy: 86.7%, Avg loss: 0.395515 

Backprop time:
0.0009975358790849742
Final  epoch:
9 86.72999999999999 0.39551487560294996 tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 86.41, 0.3880661030768589, tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 1 hidden layer
Epoch 1
-------------------------------
loss: 2.315715  [    0/60000]
loss: 0.715534  [ 6400/60000]
loss: 0.397466  [12800/60000]
loss: 0.493202  [19200/60000]
loss: 0.451300  [25600/60000]
loss: 0.442228  [32000/60000]
loss: 0.375712  [38400/60000]
loss: 0.529207  [44800/60000]
loss: 0.463062  [51200/60000]
loss: 0.546774  [57600/60000]
Test Error: 
 Accuracy: 85.0%, Avg loss: 0.415930 

Epoch 2
-------------------------------
loss: 0.245678  [    0/60000]
loss: 0.331081  [ 6400/60000]
loss: 0.277047  [12800/60000]
loss: 0.363493  [19200/60000]
loss: 0.368563  [25600/60000]
loss: 0.345466  [32000/60000]
loss: 0.314820  [38400/60000]
loss: 0.452628  [44800/60000]
loss: 0.364950  [51200/60000]
loss: 0.512981  [57600/60000]
Test Error: 
 Accuracy: 86.3%, Avg loss: 0.380550 

Epoch 3
-------------------------------
loss: 0.217677  [    0/60000]
loss: 0.277701  [ 6400/60000]
loss: 0.241611  [12800/60000]
loss: 0.309656  [19200/60000]
loss: 0.336874  [25600/60000]
loss: 0.318944  [32000/60000]
loss: 0.266239  [38400/60000]
loss: 0.381294  [44800/60000]
loss: 0.324238  [51200/60000]
loss: 0.425668  [57600/60000]
Test Error: 
 Accuracy: 86.8%, Avg loss: 0.359593 

Epoch 4
-------------------------------
loss: 0.207622  [    0/60000]
loss: 0.262548  [ 6400/60000]
loss: 0.233945  [12800/60000]
loss: 0.283575  [19200/60000]
loss: 0.341584  [25600/60000]
loss: 0.297130  [32000/60000]
loss: 0.259351  [38400/60000]
loss: 0.349018  [44800/60000]
loss: 0.277261  [51200/60000]
loss: 0.387312  [57600/60000]
Test Error: 
 Accuracy: 87.1%, Avg loss: 0.358390 

Epoch 5
-------------------------------
loss: 0.201322  [    0/60000]
loss: 0.241044  [ 6400/60000]
loss: 0.234487  [12800/60000]
loss: 0.257883  [19200/60000]
loss: 0.345364  [25600/60000]
loss: 0.319543  [32000/60000]
loss: 0.246642  [38400/60000]
loss: 0.316899  [44800/60000]
loss: 0.245624  [51200/60000]
loss: 0.347281  [57600/60000]
Test Error: 
 Accuracy: 87.5%, Avg loss: 0.342479 

Epoch 6
-------------------------------
loss: 0.190536  [    0/60000]
loss: 0.220521  [ 6400/60000]
loss: 0.212989  [12800/60000]
loss: 0.240583  [19200/60000]
loss: 0.341052  [25600/60000]
loss: 0.287756  [32000/60000]
loss: 0.222896  [38400/60000]
loss: 0.302190  [44800/60000]
loss: 0.232504  [51200/60000]
loss: 0.286456  [57600/60000]
Test Error: 
 Accuracy: 87.7%, Avg loss: 0.340739 

Epoch 7
-------------------------------
loss: 0.164942  [    0/60000]
loss: 0.176851  [ 6400/60000]
loss: 0.211744  [12800/60000]
loss: 0.242364  [19200/60000]
loss: 0.331376  [25600/60000]
loss: 0.273501  [32000/60000]
loss: 0.243057  [38400/60000]
loss: 0.272078  [44800/60000]
loss: 0.231223  [51200/60000]
loss: 0.276217  [57600/60000]
Test Error: 
 Accuracy: 87.9%, Avg loss: 0.339520 

Epoch 8
-------------------------------
loss: 0.172077  [    0/60000]
loss: 0.156004  [ 6400/60000]
loss: 0.206068  [12800/60000]
loss: 0.188938  [19200/60000]
loss: 0.372711  [25600/60000]
loss: 0.259882  [32000/60000]
loss: 0.220993  [38400/60000]
loss: 0.242480  [44800/60000]
loss: 0.204574  [51200/60000]
loss: 0.256155  [57600/60000]
Test Error: 
 Accuracy: 87.9%, Avg loss: 0.344826 

Epoch 9
-------------------------------
loss: 0.146073  [    0/60000]
loss: 0.141879  [ 6400/60000]
loss: 0.215848  [12800/60000]
loss: 0.187743  [19200/60000]
loss: 0.366878  [25600/60000]
loss: 0.243857  [32000/60000]
loss: 0.201903  [38400/60000]
loss: 0.260815  [44800/60000]
loss: 0.202331  [51200/60000]
loss: 0.270995  [57600/60000]
Test Error: 
 Accuracy: 88.2%, Avg loss: 0.343226 

Backprop time:
0.001288004888513308
Final  epoch:
9 88.25 0.34322560621295006 tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[6, 87.88, 0.3395200556820365, tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 2 hidden layer
Epoch 1
-------------------------------
loss: 2.299132  [    0/60000]
loss: 1.567881  [ 6400/60000]
loss: 1.460480  [12800/60000]
loss: 1.490894  [19200/60000]
loss: 1.647084  [25600/60000]
loss: 1.576440  [32000/60000]
loss: 1.294797  [38400/60000]
loss: 1.518562  [44800/60000]
loss: 1.453231  [51200/60000]
loss: 1.441788  [57600/60000]
Test Error: 
 Accuracy: 44.0%, Avg loss: 1.426846 

Epoch 2
-------------------------------
loss: 1.264702  [    0/60000]
loss: 1.323939  [ 6400/60000]
loss: 1.345260  [12800/60000]
loss: 1.413766  [19200/60000]
loss: 1.612299  [25600/60000]
loss: 1.544458  [32000/60000]
loss: 1.248398  [38400/60000]
loss: 1.468524  [44800/60000]
loss: 1.397582  [51200/60000]
loss: 1.368665  [57600/60000]
Test Error: 
 Accuracy: 44.4%, Avg loss: 1.402464 

Epoch 3
-------------------------------
loss: 1.222196  [    0/60000]
loss: 1.297201  [ 6400/60000]
loss: 1.314586  [12800/60000]
loss: 1.384500  [19200/60000]
loss: 1.594648  [25600/60000]
loss: 1.532617  [32000/60000]
loss: 1.218384  [38400/60000]
loss: 1.440999  [44800/60000]
loss: 1.389442  [51200/60000]
loss: 1.334945  [57600/60000]
Test Error: 
 Accuracy: 44.6%, Avg loss: 1.394405 

Epoch 4
-------------------------------
loss: 1.210150  [    0/60000]
loss: 1.300963  [ 6400/60000]
loss: 1.260419  [12800/60000]
loss: 1.345784  [19200/60000]
loss: 1.579803  [25600/60000]
loss: 1.522719  [32000/60000]
loss: 1.217577  [38400/60000]
loss: 1.391406  [44800/60000]
loss: 1.344663  [51200/60000]
loss: 1.305877  [57600/60000]
Test Error: 
 Accuracy: 44.4%, Avg loss: 1.395443 

Epoch 5
-------------------------------
loss: 1.208359  [    0/60000]
loss: 1.312490  [ 6400/60000]
loss: 1.247267  [12800/60000]
loss: 1.326131  [19200/60000]
loss: 1.594723  [25600/60000]
loss: 1.521371  [32000/60000]
loss: 1.218631  [38400/60000]
loss: 1.363015  [44800/60000]
loss: 1.366960  [51200/60000]
loss: 1.275179  [57600/60000]
Test Error: 
 Accuracy: 44.2%, Avg loss: 1.391507 

Epoch 6
-------------------------------
loss: 1.208719  [    0/60000]
loss: 1.289752  [ 6400/60000]
loss: 1.268909  [12800/60000]
loss: 1.301986  [19200/60000]
loss: 1.555796  [25600/60000]
loss: 1.543470  [32000/60000]
loss: 1.198515  [38400/60000]
loss: 1.339436  [44800/60000]
loss: 1.316007  [51200/60000]
loss: 1.252803  [57600/60000]
Test Error: 
 Accuracy: 44.5%, Avg loss: 1.389580 

Epoch 7
-------------------------------
loss: 1.185619  [    0/60000]
loss: 1.270190  [ 6400/60000]
loss: 1.293593  [12800/60000]
loss: 1.311874  [19200/60000]
loss: 1.535087  [25600/60000]
loss: 1.546654  [32000/60000]
loss: 1.179425  [38400/60000]
loss: 1.349905  [44800/60000]
loss: 1.324209  [51200/60000]
loss: 1.221625  [57600/60000]
Test Error: 
 Accuracy: 44.8%, Avg loss: 1.382614 

Epoch 8
-------------------------------
loss: 1.177475  [    0/60000]
loss: 1.302814  [ 6400/60000]
loss: 1.267583  [12800/60000]
loss: 1.313149  [19200/60000]
loss: 1.556844  [25600/60000]
loss: 1.528338  [32000/60000]
loss: 1.187612  [38400/60000]
loss: 1.371143  [44800/60000]
loss: 1.270893  [51200/60000]
loss: 1.218768  [57600/60000]
Test Error: 
 Accuracy: 44.9%, Avg loss: 1.382372 

Epoch 9
-------------------------------
loss: 1.192986  [    0/60000]
loss: 1.274363  [ 6400/60000]
loss: 1.291731  [12800/60000]
loss: 1.271803  [19200/60000]
loss: 1.528838  [25600/60000]
loss: 1.513549  [32000/60000]
loss: 1.188092  [38400/60000]
loss: 1.324548  [44800/60000]
loss: 1.291997  [51200/60000]
loss: 1.181041  [57600/60000]
Test Error: 
 Accuracy: 45.1%, Avg loss: 1.383473 

Backprop time:
0.0017705625675493324
Final  epoch:
9 45.06 1.3834731912916634 tensor(1.2919, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[7, 44.87, 1.382371965487292, tensor(1.3175, device='cuda:0', grad_fn=<NllLossBackward>)]
