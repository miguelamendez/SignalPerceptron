Using cuda device
Learnable Parameters for MNIST models:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
101632 	 406528 	 407050 	 669706
torch.Size([64, 1, 28, 28])
torch.Size([1, 28, 28])
Forward time for MNIST models:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
0.00010776519775390625 	 9.298324584960938e-05 	 0.00014090538024902344 	 0.00021600723266601562
Forward time for MNIST models FS Timer class:
FSP128 	 FSP512 	 MLP 1 hidden  	 MLP 2 hidden
8.897018432617188e-05 	 8.943939208984376e-05 	 0.00014683914184570313 	 0.00021424221992492675
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                   model_inference mlp1         5.76%     134.000us        99.10%       2.304ms       2.304ms       0.000us         0.00%      18.000us      18.000us             1  
                                           aten::linear         0.77%      18.000us        86.28%       2.006ms       1.003ms       0.000us         0.00%      17.000us       8.500us             2  
                                            aten::addmm        10.88%     253.000us        84.00%       1.953ms     976.500us      17.000us        94.44%      17.000us       8.500us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      15.000us        83.33%      15.000us       7.500us             2  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us        11.11%       2.000us       1.000us             2  
                                        aten::clamp_min         2.28%      53.000us         7.53%     175.000us      43.750us       1.000us         5.56%       2.000us       0.500us             4  
                                             aten::relu         1.89%      44.000us         6.37%     148.000us      74.000us       0.000us         0.00%       1.000us       0.500us             2  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         5.56%       1.000us       0.500us             2  
                                            aten::zeros         0.30%       7.000us         0.69%      16.000us      16.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         0.90%      21.000us         0.90%      21.000us       5.250us       0.000us         0.00%       0.000us       0.000us             4  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.325ms
Self CUDA time total: 18.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                   model_inference mlp2         6.33%     159.000us        99.12%       2.489ms       2.489ms       0.000us         0.00%      25.000us      25.000us             1  
                                           aten::linear         1.00%      25.000us        84.71%       2.127ms     709.000us       0.000us         0.00%      23.000us       7.667us             3  
                                            aten::addmm        11.47%     288.000us        81.72%       2.052ms     684.000us      23.000us        92.00%      23.000us       7.667us             3  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      21.000us        84.00%      21.000us       7.000us             3  
                                        aten::clamp_min         2.75%      69.000us         9.52%     239.000us      39.833us       2.000us         8.00%       4.000us       0.667us             6  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         8.00%       2.000us       0.667us             3  
                                             aten::relu         1.79%      45.000us         7.41%     186.000us      62.000us       0.000us         0.00%       2.000us       0.667us             3  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         8.00%       2.000us       0.667us             3  
                                            aten::zeros         0.32%       8.000us         0.68%      17.000us      17.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         1.04%      26.000us         1.04%      26.000us       5.200us       0.000us         0.00%       0.000us       0.000us             5  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.511ms
Self CUDA time total: 25.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                 model_inference fsp128         5.51%     122.000us        98.78%       2.188ms       2.188ms       0.000us         0.00%       7.000us       7.000us             1  
                                           aten::linear         0.81%      18.000us        89.98%       1.993ms     996.500us       0.000us         0.00%       6.000us       3.000us             2  
                                           aten::matmul         0.77%      17.000us        87.31%       1.934ms     967.000us       0.000us         0.00%       6.000us       3.000us             2  
                                               aten::mm        10.34%     229.000us        86.55%       1.917ms     958.500us       6.000us        85.71%       6.000us       3.000us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us        85.71%       6.000us       3.000us             2  
                                              aten::cos         1.85%      41.000us         2.57%      57.000us      57.000us       1.000us        14.29%       1.000us       1.000us             1  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us        14.29%       1.000us       1.000us             1  
                                            aten::zeros         0.36%       8.000us         0.99%      22.000us      22.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         1.53%      34.000us         1.53%      34.000us       8.500us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.09%       2.000us         0.09%       2.000us       2.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.215ms
Self CUDA time total: 7.000us

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                 model_inference fsp512         5.11%     116.000us        98.85%       2.243ms       2.243ms       0.000us         0.00%      15.000us      15.000us             1  
                                           aten::linear         0.79%      18.000us        90.88%       2.062ms       1.031ms       0.000us         0.00%      14.000us       7.000us             2  
                                           aten::matmul         0.62%      14.000us        88.32%       2.004ms       1.002ms       0.000us         0.00%      14.000us       7.000us             2  
                                               aten::mm        10.67%     242.000us        87.70%       1.990ms     995.000us      14.000us        93.33%      14.000us       7.000us             2  
void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      14.000us        93.33%      14.000us       7.000us             2  
                                              aten::cos         1.67%      38.000us         2.29%      52.000us      52.000us       1.000us         6.67%       1.000us       1.000us             1  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         6.67%       1.000us       1.000us             1  
                                            aten::zeros         0.40%       9.000us         0.93%      21.000us      21.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::empty         1.37%      31.000us         1.37%      31.000us       7.750us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.13%       3.000us         0.13%       3.000us       3.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.269ms
Self CUDA time total: 15.000us

Optimizer: Adam ,lr: 0.001
Training models with MNIST DATASET :
Fourier Signal Perceptron 128
Epoch 1
-------------------------------
loss: 2.338919  [    0/60000]
loss: 0.440134  [ 6400/60000]
loss: 0.322344  [12800/60000]
loss: 0.328876  [19200/60000]
loss: 0.211097  [25600/60000]
loss: 0.296152  [32000/60000]
loss: 0.112504  [38400/60000]
loss: 0.301098  [44800/60000]
loss: 0.240941  [51200/60000]
loss: 0.237841  [57600/60000]
Test Error: 
 Accuracy: 95.8%, Avg loss: 0.154768 

Epoch 2
-------------------------------
loss: 0.129830  [    0/60000]
loss: 0.149542  [ 6400/60000]
loss: 0.110623  [12800/60000]
loss: 0.127604  [19200/60000]
loss: 0.120257  [25600/60000]
loss: 0.161469  [32000/60000]
loss: 0.052476  [38400/60000]
loss: 0.177213  [44800/60000]
loss: 0.142592  [51200/60000]
loss: 0.116176  [57600/60000]
Test Error: 
 Accuracy: 96.9%, Avg loss: 0.105132 

Epoch 3
-------------------------------
loss: 0.063184  [    0/60000]
loss: 0.116062  [ 6400/60000]
loss: 0.097275  [12800/60000]
loss: 0.077003  [19200/60000]
loss: 0.076720  [25600/60000]
loss: 0.102212  [32000/60000]
loss: 0.037958  [38400/60000]
loss: 0.102023  [44800/60000]
loss: 0.106432  [51200/60000]
loss: 0.049511  [57600/60000]
Test Error: 
 Accuracy: 97.2%, Avg loss: 0.090386 

Epoch 4
-------------------------------
loss: 0.048457  [    0/60000]
loss: 0.100926  [ 6400/60000]
loss: 0.094578  [12800/60000]
loss: 0.045648  [19200/60000]
loss: 0.054508  [25600/60000]
loss: 0.075648  [32000/60000]
loss: 0.027912  [38400/60000]
loss: 0.066484  [44800/60000]
loss: 0.094918  [51200/60000]
loss: 0.024121  [57600/60000]
Test Error: 
 Accuracy: 97.3%, Avg loss: 0.083721 

Epoch 5
-------------------------------
loss: 0.046953  [    0/60000]
loss: 0.074940  [ 6400/60000]
loss: 0.093399  [12800/60000]
loss: 0.028012  [19200/60000]
loss: 0.037567  [25600/60000]
loss: 0.053772  [32000/60000]
loss: 0.015028  [38400/60000]
loss: 0.047670  [44800/60000]
loss: 0.087855  [51200/60000]
loss: 0.015688  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.081424 

Epoch 6
-------------------------------
loss: 0.039670  [    0/60000]
loss: 0.051544  [ 6400/60000]
loss: 0.085816  [12800/60000]
loss: 0.024303  [19200/60000]
loss: 0.023991  [25600/60000]
loss: 0.034647  [32000/60000]
loss: 0.010262  [38400/60000]
loss: 0.030386  [44800/60000]
loss: 0.080107  [51200/60000]
loss: 0.012564  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.080594 

Epoch 7
-------------------------------
loss: 0.026024  [    0/60000]
loss: 0.025191  [ 6400/60000]
loss: 0.078565  [12800/60000]
loss: 0.019515  [19200/60000]
loss: 0.012140  [25600/60000]
loss: 0.020137  [32000/60000]
loss: 0.010970  [38400/60000]
loss: 0.021579  [44800/60000]
loss: 0.063889  [51200/60000]
loss: 0.011453  [57600/60000]
Test Error: 
 Accuracy: 97.6%, Avg loss: 0.083467 

Epoch 8
-------------------------------
loss: 0.015836  [    0/60000]
loss: 0.011258  [ 6400/60000]
loss: 0.072366  [12800/60000]
loss: 0.011337  [19200/60000]
loss: 0.008673  [25600/60000]
loss: 0.012737  [32000/60000]
loss: 0.003538  [38400/60000]
loss: 0.013844  [44800/60000]
loss: 0.049061  [51200/60000]
loss: 0.013412  [57600/60000]
Test Error: 
 Accuracy: 97.5%, Avg loss: 0.087405 

Epoch 9
-------------------------------
loss: 0.010102  [    0/60000]
loss: 0.005630  [ 6400/60000]
loss: 0.056739  [12800/60000]
loss: 0.006888  [19200/60000]
loss: 0.006842  [25600/60000]
loss: 0.006810  [32000/60000]
loss: 0.001897  [38400/60000]
loss: 0.009559  [44800/60000]
loss: 0.013423  [51200/60000]
loss: 0.005678  [57600/60000]
Test Error: 
 Accuracy: 97.4%, Avg loss: 0.094179 

Backprop time:
0.0011061578247677627
Final  epoch:
9 97.39 0.09417904282874506 tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 97.54, 0.08059424144374562, tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)]
Fourier Signal Perceptron 512
Epoch 1
-------------------------------
loss: 2.567894  [    0/60000]
loss: 0.331403  [ 6400/60000]
loss: 0.253962  [12800/60000]
loss: 0.263580  [19200/60000]
loss: 0.156316  [25600/60000]
loss: 0.262115  [32000/60000]
loss: 0.087420  [38400/60000]
loss: 0.244658  [44800/60000]
loss: 0.233967  [51200/60000]
loss: 0.195214  [57600/60000]
Test Error: 
 Accuracy: 96.1%, Avg loss: 0.122896 

Epoch 2
-------------------------------
loss: 0.087392  [    0/60000]
loss: 0.163802  [ 6400/60000]
loss: 0.088006  [12800/60000]
loss: 0.075229  [19200/60000]
loss: 0.072352  [25600/60000]
loss: 0.141073  [32000/60000]
loss: 0.048343  [38400/60000]
loss: 0.076938  [44800/60000]
loss: 0.112926  [51200/60000]
loss: 0.077073  [57600/60000]
Test Error: 
 Accuracy: 97.3%, Avg loss: 0.081051 

Epoch 3
-------------------------------
loss: 0.033934  [    0/60000]
loss: 0.099649  [ 6400/60000]
loss: 0.053815  [12800/60000]
loss: 0.026599  [19200/60000]
loss: 0.060759  [25600/60000]
loss: 0.098359  [32000/60000]
loss: 0.032489  [38400/60000]
loss: 0.025991  [44800/60000]
loss: 0.083661  [51200/60000]
loss: 0.020143  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.067665 

Epoch 4
-------------------------------
loss: 0.028072  [    0/60000]
loss: 0.059809  [ 6400/60000]
loss: 0.036540  [12800/60000]
loss: 0.018370  [19200/60000]
loss: 0.035259  [25600/60000]
loss: 0.052581  [32000/60000]
loss: 0.008020  [38400/60000]
loss: 0.023943  [44800/60000]
loss: 0.057467  [51200/60000]
loss: 0.010829  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.064472 

Epoch 5
-------------------------------
loss: 0.010970  [    0/60000]
loss: 0.013925  [ 6400/60000]
loss: 0.028701  [12800/60000]
loss: 0.010850  [19200/60000]
loss: 0.011833  [25600/60000]
loss: 0.023082  [32000/60000]
loss: 0.011437  [38400/60000]
loss: 0.017345  [44800/60000]
loss: 0.058783  [51200/60000]
loss: 0.006906  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.064696 

Epoch 6
-------------------------------
loss: 0.008628  [    0/60000]
loss: 0.004604  [ 6400/60000]
loss: 0.020887  [12800/60000]
loss: 0.008974  [19200/60000]
loss: 0.005901  [25600/60000]
loss: 0.003504  [32000/60000]
loss: 0.001625  [38400/60000]
loss: 0.005586  [44800/60000]
loss: 0.020402  [51200/60000]
loss: 0.004963  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.067907 

Epoch 7
-------------------------------
loss: 0.009054  [    0/60000]
loss: 0.008154  [ 6400/60000]
loss: 0.018806  [12800/60000]
loss: 0.011032  [19200/60000]
loss: 0.003779  [25600/60000]
loss: 0.053769  [32000/60000]
loss: 0.004558  [38400/60000]
loss: 0.009255  [44800/60000]
loss: 0.029953  [51200/60000]
loss: 0.011768  [57600/60000]
Test Error: 
 Accuracy: 97.3%, Avg loss: 0.086863 

Epoch 8
-------------------------------
loss: 0.032288  [    0/60000]
loss: 0.015024  [ 6400/60000]
loss: 0.015680  [12800/60000]
loss: 0.008086  [19200/60000]
loss: 0.011972  [25600/60000]
loss: 0.002190  [32000/60000]
loss: 0.001260  [38400/60000]
loss: 0.014364  [44800/60000]
loss: 0.019286  [51200/60000]
loss: 0.002128  [57600/60000]
Test Error: 
 Accuracy: 97.9%, Avg loss: 0.073761 

Epoch 9
-------------------------------
loss: 0.010755  [    0/60000]
loss: 0.004956  [ 6400/60000]
loss: 0.002425  [12800/60000]
loss: 0.004793  [19200/60000]
loss: 0.002984  [25600/60000]
loss: 0.001545  [32000/60000]
loss: 0.000378  [38400/60000]
loss: 0.002723  [44800/60000]
loss: 0.012460  [51200/60000]
loss: 0.018212  [57600/60000]
Test Error: 
 Accuracy: 97.8%, Avg loss: 0.084931 

Backprop time:
0.000950588092112592
Final  epoch:
9 97.83 0.0849313909182682 tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[3, 97.86, 0.06447168190649913]
MLP 1 hidden layer Signal Perceptron
Epoch 1
-------------------------------
loss: 2.307549  [    0/60000]
loss: 0.775661  [ 6400/60000]
loss: 0.699505  [12800/60000]
loss: 0.458927  [19200/60000]
loss: 0.811413  [25600/60000]
loss: 0.931193  [32000/60000]
loss: 0.342575  [38400/60000]
loss: 0.834685  [44800/60000]
loss: 0.687392  [51200/60000]
loss: 0.367167  [57600/60000]
Test Error: 
 Accuracy: 78.2%, Avg loss: 0.573132 

Epoch 2
-------------------------------
loss: 0.517106  [    0/60000]
loss: 0.605405  [ 6400/60000]
loss: 0.575765  [12800/60000]
loss: 0.386588  [19200/60000]
loss: 0.743839  [25600/60000]
loss: 0.827239  [32000/60000]
loss: 0.303726  [38400/60000]
loss: 0.734777  [44800/60000]
loss: 0.594292  [51200/60000]
loss: 0.296539  [57600/60000]
Test Error: 
 Accuracy: 79.2%, Avg loss: 0.525352 

Epoch 3
-------------------------------
loss: 0.492002  [    0/60000]
loss: 0.559230  [ 6400/60000]
loss: 0.553408  [12800/60000]
loss: 0.381948  [19200/60000]
loss: 0.719948  [25600/60000]
loss: 0.748856  [32000/60000]
loss: 0.302180  [38400/60000]
loss: 0.645609  [44800/60000]
loss: 0.519497  [51200/60000]
loss: 0.263414  [57600/60000]
Test Error: 
 Accuracy: 79.5%, Avg loss: 0.507762 

Epoch 4
-------------------------------
loss: 0.473886  [    0/60000]
loss: 0.537561  [ 6400/60000]
loss: 0.523235  [12800/60000]
loss: 0.350567  [19200/60000]
loss: 0.705357  [25600/60000]
loss: 0.725078  [32000/60000]
loss: 0.297646  [38400/60000]
loss: 0.594336  [44800/60000]
loss: 0.484222  [51200/60000]
loss: 0.220604  [57600/60000]
Test Error: 
 Accuracy: 79.7%, Avg loss: 0.499363 

Epoch 5
-------------------------------
loss: 0.469257  [    0/60000]
loss: 0.531181  [ 6400/60000]
loss: 0.513308  [12800/60000]
loss: 0.355975  [19200/60000]
loss: 0.695790  [25600/60000]
loss: 0.688604  [32000/60000]
loss: 0.294928  [38400/60000]
loss: 0.568362  [44800/60000]
loss: 0.464836  [51200/60000]
loss: 0.204865  [57600/60000]
Test Error: 
 Accuracy: 79.8%, Avg loss: 0.496583 

Epoch 6
-------------------------------
loss: 0.472017  [    0/60000]
loss: 0.520775  [ 6400/60000]
loss: 0.488548  [12800/60000]
loss: 0.345174  [19200/60000]
loss: 0.672132  [25600/60000]
loss: 0.656535  [32000/60000]
loss: 0.290702  [38400/60000]
loss: 0.553933  [44800/60000]
loss: 0.448717  [51200/60000]
loss: 0.190159  [57600/60000]
Test Error: 
 Accuracy: 79.9%, Avg loss: 0.496155 

Epoch 7
-------------------------------
loss: 0.449899  [    0/60000]
loss: 0.511180  [ 6400/60000]
loss: 0.493108  [12800/60000]
loss: 0.332639  [19200/60000]
loss: 0.667084  [25600/60000]
loss: 0.655823  [32000/60000]
loss: 0.291354  [38400/60000]
loss: 0.554444  [44800/60000]
loss: 0.456159  [51200/60000]
loss: 0.186729  [57600/60000]
Test Error: 
 Accuracy: 79.9%, Avg loss: 0.498747 

Epoch 8
-------------------------------
loss: 0.453868  [    0/60000]
loss: 0.508349  [ 6400/60000]
loss: 0.485105  [12800/60000]
loss: 0.293257  [19200/60000]
loss: 0.652519  [25600/60000]
loss: 0.654358  [32000/60000]
loss: 0.288855  [38400/60000]
loss: 0.548952  [44800/60000]
loss: 0.442239  [51200/60000]
loss: 0.189753  [57600/60000]
Test Error: 
 Accuracy: 79.8%, Avg loss: 0.504230 

Epoch 9
-------------------------------
loss: 0.447437  [    0/60000]
loss: 0.509118  [ 6400/60000]
loss: 0.497592  [12800/60000]
loss: 0.294769  [19200/60000]
loss: 0.200188  [25600/60000]
loss: 0.294668  [32000/60000]
loss: 0.189288  [38400/60000]
loss: 0.406502  [44800/60000]
loss: 0.254899  [51200/60000]
loss: 0.121864  [57600/60000]
Test Error: 
 Accuracy: 89.3%, Avg loss: 0.287040 

Backprop time:
0.001299093668281792
Final  epoch:
9 89.28 0.28704013789345506 tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[8, 89.28, 0.28704013789345506, tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 2 hidden layer Signal Perceptron
Epoch 1
-------------------------------
loss: 2.292605  [    0/60000]
loss: 0.749563  [ 6400/60000]
loss: 1.297604  [12800/60000]
loss: 0.999655  [19200/60000]
loss: 0.994487  [25600/60000]
loss: 0.885745  [32000/60000]
loss: 0.747148  [38400/60000]
loss: 0.969350  [44800/60000]
loss: 0.854491  [51200/60000]
loss: 1.156338  [57600/60000]
Test Error: 
 Accuracy: 66.7%, Avg loss: 0.836287 

Epoch 2
-------------------------------
loss: 0.706212  [    0/60000]
loss: 0.566571  [ 6400/60000]
loss: 1.028442  [12800/60000]
loss: 0.804039  [19200/60000]
loss: 0.901131  [25600/60000]
loss: 0.693349  [32000/60000]
loss: 0.687864  [38400/60000]
loss: 0.843661  [44800/60000]
loss: 0.799225  [51200/60000]
loss: 1.088940  [57600/60000]
Test Error: 
 Accuracy: 67.0%, Avg loss: 0.810408 

Epoch 3
-------------------------------
loss: 0.712133  [    0/60000]
loss: 0.491751  [ 6400/60000]
loss: 0.991680  [12800/60000]
loss: 0.827192  [19200/60000]
loss: 0.912114  [25600/60000]
loss: 0.619519  [32000/60000]
loss: 0.687783  [38400/60000]
loss: 0.812621  [44800/60000]
loss: 0.841533  [51200/60000]
loss: 1.012017  [57600/60000]
Test Error: 
 Accuracy: 66.7%, Avg loss: 0.826023 

Epoch 4
-------------------------------
loss: 0.688740  [    0/60000]
loss: 0.465337  [ 6400/60000]
loss: 0.980406  [12800/60000]
loss: 0.835765  [19200/60000]
loss: 0.840165  [25600/60000]
loss: 0.602698  [32000/60000]
loss: 0.688027  [38400/60000]
loss: 0.797697  [44800/60000]
loss: 0.759874  [51200/60000]
loss: 0.987623  [57600/60000]
Test Error: 
 Accuracy: 66.0%, Avg loss: 0.865563 

Epoch 5
-------------------------------
loss: 0.665164  [    0/60000]
loss: 0.471486  [ 6400/60000]
loss: 0.988936  [12800/60000]
loss: 0.768811  [19200/60000]
loss: 0.888893  [25600/60000]
loss: 0.607047  [32000/60000]
loss: 0.685655  [38400/60000]
loss: 0.829119  [44800/60000]
loss: 0.824093  [51200/60000]
loss: 1.005922  [57600/60000]
Test Error: 
 Accuracy: 67.7%, Avg loss: 0.794477 

Epoch 6
-------------------------------
loss: 0.705592  [    0/60000]
loss: 0.568580  [ 6400/60000]
loss: 0.987928  [12800/60000]
loss: 0.842632  [19200/60000]
loss: 0.878293  [25600/60000]
loss: 0.583241  [32000/60000]
loss: 0.684669  [38400/60000]
loss: 0.798119  [44800/60000]
loss: 0.732184  [51200/60000]
loss: 1.005035  [57600/60000]
Test Error: 
 Accuracy: 67.5%, Avg loss: 0.792374 

Epoch 7
-------------------------------
loss: 0.654918  [    0/60000]
loss: 0.435387  [ 6400/60000]
loss: 1.070020  [12800/60000]
loss: 0.757887  [19200/60000]
loss: 0.839173  [25600/60000]
loss: 0.566862  [32000/60000]
loss: 0.683653  [38400/60000]
loss: 0.826028  [44800/60000]
loss: 0.751959  [51200/60000]
loss: 1.017778  [57600/60000]
Test Error: 
 Accuracy: 66.9%, Avg loss: 0.842920 

Epoch 8
-------------------------------
loss: 0.662605  [    0/60000]
loss: 0.480667  [ 6400/60000]
loss: 0.943169  [12800/60000]
loss: 0.758529  [19200/60000]
loss: 0.797336  [25600/60000]
loss: 0.540369  [32000/60000]
loss: 0.691313  [38400/60000]
loss: 0.795961  [44800/60000]
loss: 0.816330  [51200/60000]
loss: 0.987991  [57600/60000]
Test Error: 
 Accuracy: 67.9%, Avg loss: 0.775795 

Epoch 9
-------------------------------
loss: 0.653897  [    0/60000]
loss: 0.482533  [ 6400/60000]
loss: 0.939334  [12800/60000]
loss: 0.816849  [19200/60000]
loss: 0.793442  [25600/60000]
loss: 0.577240  [32000/60000]
loss: 0.684777  [38400/60000]
loss: 0.792106  [44800/60000]
loss: 0.712255  [51200/60000]
loss: 0.972947  [57600/60000]
Test Error: 
 Accuracy: 66.8%, Avg loss: 0.819950 

Backprop time:
0.0017821506368297729
Final  epoch:
9 66.78 0.8199504718279383 tensor(0.8635, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[7, 67.94, 0.7757952972582192, tensor(0.8635, device='cuda:0', grad_fn=<NllLossBackward>)]
Training models with FashionMNIST DATASET :
Fourier Signal Perceptron 128
Epoch 1
-------------------------------
loss: 2.379455  [    0/60000]
loss: 0.583248  [ 6400/60000]
loss: 0.389857  [12800/60000]
loss: 0.554215  [19200/60000]
loss: 0.543647  [25600/60000]
loss: 0.431874  [32000/60000]
loss: 0.381077  [38400/60000]
loss: 0.506871  [44800/60000]
loss: 0.498023  [51200/60000]
loss: 0.521814  [57600/60000]
Test Error: 
 Accuracy: 84.9%, Avg loss: 0.428982 

Epoch 2
-------------------------------
loss: 0.263935  [    0/60000]
loss: 0.412299  [ 6400/60000]
loss: 0.288613  [12800/60000]
loss: 0.441461  [19200/60000]
loss: 0.370013  [25600/60000]
loss: 0.331097  [32000/60000]
loss: 0.315573  [38400/60000]
loss: 0.449527  [44800/60000]
loss: 0.392327  [51200/60000]
loss: 0.447868  [57600/60000]
Test Error: 
 Accuracy: 85.8%, Avg loss: 0.408977 

Epoch 3
-------------------------------
loss: 0.201706  [    0/60000]
loss: 0.346051  [ 6400/60000]
loss: 0.255047  [12800/60000]
loss: 0.375807  [19200/60000]
loss: 0.350113  [25600/60000]
loss: 0.287064  [32000/60000]
loss: 0.270445  [38400/60000]
loss: 0.404502  [44800/60000]
loss: 0.353212  [51200/60000]
loss: 0.390791  [57600/60000]
Test Error: 
 Accuracy: 85.9%, Avg loss: 0.403518 

Epoch 4
-------------------------------
loss: 0.190629  [    0/60000]
loss: 0.314973  [ 6400/60000]
loss: 0.244047  [12800/60000]
loss: 0.341681  [19200/60000]
loss: 0.319135  [25600/60000]
loss: 0.264214  [32000/60000]
loss: 0.255627  [38400/60000]
loss: 0.365698  [44800/60000]
loss: 0.316724  [51200/60000]
loss: 0.353424  [57600/60000]
Test Error: 
 Accuracy: 85.8%, Avg loss: 0.404799 

Epoch 5
-------------------------------
loss: 0.198636  [    0/60000]
loss: 0.283674  [ 6400/60000]
loss: 0.231959  [12800/60000]
loss: 0.289043  [19200/60000]
loss: 0.301049  [25600/60000]
loss: 0.268111  [32000/60000]
loss: 0.233943  [38400/60000]
loss: 0.334332  [44800/60000]
loss: 0.338378  [51200/60000]
loss: 0.318866  [57600/60000]
Test Error: 
 Accuracy: 85.8%, Avg loss: 0.407981 

Epoch 6
-------------------------------
loss: 0.198431  [    0/60000]
loss: 0.279608  [ 6400/60000]
loss: 0.221667  [12800/60000]
loss: 0.264770  [19200/60000]
loss: 0.304772  [25600/60000]
loss: 0.291397  [32000/60000]
loss: 0.238713  [38400/60000]
loss: 0.299526  [44800/60000]
loss: 0.314916  [51200/60000]
loss: 0.292393  [57600/60000]
Test Error: 
 Accuracy: 86.1%, Avg loss: 0.403419 

Epoch 7
-------------------------------
loss: 0.178683  [    0/60000]
loss: 0.278732  [ 6400/60000]
loss: 0.196480  [12800/60000]
loss: 0.235462  [19200/60000]
loss: 0.332367  [25600/60000]
loss: 0.281013  [32000/60000]
loss: 0.241474  [38400/60000]
loss: 0.300840  [44800/60000]
loss: 0.325417  [51200/60000]
loss: 0.295226  [57600/60000]
Test Error: 
 Accuracy: 85.9%, Avg loss: 0.413583 

Epoch 8
-------------------------------
loss: 0.181616  [    0/60000]
loss: 0.256510  [ 6400/60000]
loss: 0.213638  [12800/60000]
loss: 0.219106  [19200/60000]
loss: 0.297492  [25600/60000]
loss: 0.272219  [32000/60000]
loss: 0.264882  [38400/60000]
loss: 0.238368  [44800/60000]
loss: 0.313177  [51200/60000]
loss: 0.249344  [57600/60000]
Test Error: 
 Accuracy: 86.0%, Avg loss: 0.412071 

Epoch 9
-------------------------------
loss: 0.139637  [    0/60000]
loss: 0.230873  [ 6400/60000]
loss: 0.219482  [12800/60000]
loss: 0.220251  [19200/60000]
loss: 0.300346  [25600/60000]
loss: 0.278227  [32000/60000]
loss: 0.280879  [38400/60000]
loss: 0.229358  [44800/60000]
loss: 0.284826  [51200/60000]
loss: 0.233654  [57600/60000]
Test Error: 
 Accuracy: 86.5%, Avg loss: 0.405987 

Backprop time:
0.0008469899664894435
Final  epoch:
9 86.53999999999999 0.4059868584013289 tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[5, 86.05000000000001, 0.40341875649941195, tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward>)]
Fourier Signal Perceptron 512
Epoch 1
-------------------------------
loss: 2.364289  [    0/60000]
loss: 0.570348  [ 6400/60000]
loss: 0.433643  [12800/60000]
loss: 0.570534  [19200/60000]
loss: 0.461110  [25600/60000]
loss: 0.390461  [32000/60000]
loss: 0.360236  [38400/60000]
loss: 0.505986  [44800/60000]
loss: 0.482752  [51200/60000]
loss: 0.519189  [57600/60000]
Test Error: 
 Accuracy: 84.3%, Avg loss: 0.434509 

Epoch 2
-------------------------------
loss: 0.300140  [    0/60000]
loss: 0.326651  [ 6400/60000]
loss: 0.321051  [12800/60000]
loss: 0.385994  [19200/60000]
loss: 0.432794  [25600/60000]
loss: 0.315499  [32000/60000]
loss: 0.276835  [38400/60000]
loss: 0.385003  [44800/60000]
loss: 0.361156  [51200/60000]
loss: 0.476498  [57600/60000]
Test Error: 
 Accuracy: 85.1%, Avg loss: 0.419704 

Epoch 3
-------------------------------
loss: 0.272715  [    0/60000]
loss: 0.312299  [ 6400/60000]
loss: 0.278479  [12800/60000]
loss: 0.290907  [19200/60000]
loss: 0.350323  [25600/60000]
loss: 0.314445  [32000/60000]
loss: 0.272268  [38400/60000]
loss: 0.320889  [44800/60000]
loss: 0.286811  [51200/60000]
loss: 0.397905  [57600/60000]
Test Error: 
 Accuracy: 85.3%, Avg loss: 0.413696 

Epoch 4
-------------------------------
loss: 0.204088  [    0/60000]
loss: 0.287669  [ 6400/60000]
loss: 0.277734  [12800/60000]
loss: 0.301868  [19200/60000]
loss: 0.339767  [25600/60000]
loss: 0.294986  [32000/60000]
loss: 0.258988  [38400/60000]
loss: 0.338094  [44800/60000]
loss: 0.269673  [51200/60000]
loss: 0.341109  [57600/60000]
Test Error: 
 Accuracy: 85.8%, Avg loss: 0.404147 

Epoch 5
-------------------------------
loss: 0.132048  [    0/60000]
loss: 0.281410  [ 6400/60000]
loss: 0.266099  [12800/60000]
loss: 0.260866  [19200/60000]
loss: 0.303336  [25600/60000]
loss: 0.282871  [32000/60000]
loss: 0.205890  [38400/60000]
loss: 0.348449  [44800/60000]
loss: 0.242966  [51200/60000]
loss: 0.339222  [57600/60000]
Test Error: 
 Accuracy: 86.3%, Avg loss: 0.390207 

Epoch 6
-------------------------------
loss: 0.110225  [    0/60000]
loss: 0.304664  [ 6400/60000]
loss: 0.247145  [12800/60000]
loss: 0.234766  [19200/60000]
loss: 0.350440  [25600/60000]
loss: 0.263371  [32000/60000]
loss: 0.249778  [38400/60000]
loss: 0.298479  [44800/60000]
loss: 0.256659  [51200/60000]
loss: 0.278728  [57600/60000]
Test Error: 
 Accuracy: 86.2%, Avg loss: 0.404111 

Epoch 7
-------------------------------
loss: 0.107897  [    0/60000]
loss: 0.296620  [ 6400/60000]
loss: 0.236408  [12800/60000]
loss: 0.253555  [19200/60000]
loss: 0.239963  [25600/60000]
loss: 0.254464  [32000/60000]
loss: 0.242388  [38400/60000]
loss: 0.282076  [44800/60000]
loss: 0.220646  [51200/60000]
loss: 0.271214  [57600/60000]
Test Error: 
 Accuracy: 86.3%, Avg loss: 0.409101 

Epoch 8
-------------------------------
loss: 0.125314  [    0/60000]
loss: 0.254550  [ 6400/60000]
loss: 0.230142  [12800/60000]
loss: 0.193964  [19200/60000]
loss: 0.320252  [25600/60000]
loss: 0.255640  [32000/60000]
loss: 0.231361  [38400/60000]
loss: 0.239345  [44800/60000]
loss: 0.189607  [51200/60000]
loss: 0.274078  [57600/60000]
Test Error: 
 Accuracy: 86.3%, Avg loss: 0.404643 

Epoch 9
-------------------------------
loss: 0.135490  [    0/60000]
loss: 0.229761  [ 6400/60000]
loss: 0.208538  [12800/60000]
loss: 0.264026  [19200/60000]
loss: 0.248150  [25600/60000]
loss: 0.274689  [32000/60000]
loss: 0.252436  [38400/60000]
loss: 0.274409  [44800/60000]
loss: 0.165843  [51200/60000]
loss: 0.218520  [57600/60000]
Test Error: 
 Accuracy: 86.5%, Avg loss: 0.402666 

Backprop time:
0.0010108535138952368
Final  epoch:
9 86.52 0.4026658942175519 tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[4, 86.28, 0.3902066948877019, tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 1 hidden layer
Epoch 1
-------------------------------
loss: 2.318517  [    0/60000]
loss: 0.995676  [ 6400/60000]
loss: 0.742903  [12800/60000]
loss: 0.918595  [19200/60000]
loss: 0.961685  [25600/60000]
loss: 0.737977  [32000/60000]
loss: 0.976827  [38400/60000]
loss: 0.938987  [44800/60000]
loss: 1.040377  [51200/60000]
loss: 0.992128  [57600/60000]
Test Error: 
 Accuracy: 66.8%, Avg loss: 0.845534 

Epoch 2
-------------------------------
loss: 0.804674  [    0/60000]
loss: 0.793489  [ 6400/60000]
loss: 0.641618  [12800/60000]
loss: 0.797900  [19200/60000]
loss: 0.890273  [25600/60000]
loss: 0.672433  [32000/60000]
loss: 0.906728  [38400/60000]
loss: 0.887079  [44800/60000]
loss: 0.946031  [51200/60000]
loss: 0.933290  [57600/60000]
Test Error: 
 Accuracy: 67.6%, Avg loss: 0.823868 

Epoch 3
-------------------------------
loss: 0.776562  [    0/60000]
loss: 0.742748  [ 6400/60000]
loss: 0.608563  [12800/60000]
loss: 0.735325  [19200/60000]
loss: 0.891495  [25600/60000]
loss: 0.653345  [32000/60000]
loss: 0.873120  [38400/60000]
loss: 0.832070  [44800/60000]
loss: 0.882588  [51200/60000]
loss: 0.895326  [57600/60000]
Test Error: 
 Accuracy: 67.7%, Avg loss: 0.824946 

Epoch 4
-------------------------------
loss: 0.774654  [    0/60000]
loss: 0.715835  [ 6400/60000]
loss: 0.598031  [12800/60000]
loss: 0.712795  [19200/60000]
loss: 0.875961  [25600/60000]
loss: 0.654953  [32000/60000]
loss: 0.856737  [38400/60000]
loss: 0.804412  [44800/60000]
loss: 0.865983  [51200/60000]
loss: 0.850571  [57600/60000]
Test Error: 
 Accuracy: 68.0%, Avg loss: 0.815814 

Epoch 5
-------------------------------
loss: 0.753363  [    0/60000]
loss: 0.697497  [ 6400/60000]
loss: 0.601944  [12800/60000]
loss: 0.685808  [19200/60000]
loss: 0.881938  [25600/60000]
loss: 0.617210  [32000/60000]
loss: 0.845421  [38400/60000]
loss: 0.760026  [44800/60000]
loss: 0.851228  [51200/60000]
loss: 0.826469  [57600/60000]
Test Error: 
 Accuracy: 68.5%, Avg loss: 0.804127 

Epoch 6
-------------------------------
loss: 0.747786  [    0/60000]
loss: 0.696240  [ 6400/60000]
loss: 0.595943  [12800/60000]
loss: 0.681025  [19200/60000]
loss: 0.887979  [25600/60000]
loss: 0.604598  [32000/60000]
loss: 0.833758  [38400/60000]
loss: 0.707279  [44800/60000]
loss: 0.829088  [51200/60000]
loss: 0.799616  [57600/60000]
Test Error: 
 Accuracy: 68.4%, Avg loss: 0.812341 

Epoch 7
-------------------------------
loss: 0.761918  [    0/60000]
loss: 0.674338  [ 6400/60000]
loss: 0.607830  [12800/60000]
loss: 0.673781  [19200/60000]
loss: 0.862633  [25600/60000]
loss: 0.601554  [32000/60000]
loss: 0.814649  [38400/60000]
loss: 0.679270  [44800/60000]
loss: 0.814982  [51200/60000]
loss: 0.790238  [57600/60000]
Test Error: 
 Accuracy: 68.5%, Avg loss: 0.809498 

Epoch 8
-------------------------------
loss: 0.751631  [    0/60000]
loss: 0.663425  [ 6400/60000]
loss: 0.607156  [12800/60000]
loss: 0.683054  [19200/60000]
loss: 0.862307  [25600/60000]
loss: 0.589470  [32000/60000]
loss: 0.812635  [38400/60000]
loss: 0.695108  [44800/60000]
loss: 0.805996  [51200/60000]
loss: 0.772047  [57600/60000]
Test Error: 
 Accuracy: 68.3%, Avg loss: 0.815291 

Epoch 9
-------------------------------
loss: 0.750168  [    0/60000]
loss: 0.659365  [ 6400/60000]
loss: 0.603928  [12800/60000]
loss: 0.660778  [19200/60000]
loss: 0.845270  [25600/60000]
loss: 0.597092  [32000/60000]
loss: 0.807970  [38400/60000]
loss: 0.649407  [44800/60000]
loss: 0.798161  [51200/60000]
loss: 0.767628  [57600/60000]
Test Error: 
 Accuracy: 68.7%, Avg loss: 0.806594 

Backprop time:
0.001303188412767314
Final  epoch:
9 68.72 0.8065944532300257 tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[4, 68.55, 0.8041272049496888, tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward>)]
MLP 2 hidden layer
Epoch 1
-------------------------------
loss: 2.302292  [    0/60000]
loss: 1.312284  [ 6400/60000]
loss: 1.137978  [12800/60000]
loss: 1.372739  [19200/60000]
loss: 1.467602  [25600/60000]
loss: 1.275129  [32000/60000]
loss: 1.370262  [38400/60000]
loss: 1.439690  [44800/60000]
loss: 1.237114  [51200/60000]
loss: 1.412936  [57600/60000]
Test Error: 
 Accuracy: 51.8%, Avg loss: 1.250977 

Epoch 2
-------------------------------
loss: 0.967639  [    0/60000]
loss: 1.119362  [ 6400/60000]
loss: 1.032470  [12800/60000]
loss: 1.288596  [19200/60000]
loss: 1.318479  [25600/60000]
loss: 1.262487  [32000/60000]
loss: 1.338209  [38400/60000]
loss: 1.359802  [44800/60000]
loss: 1.160082  [51200/60000]
loss: 1.417003  [57600/60000]
Test Error: 
 Accuracy: 52.5%, Avg loss: 1.225370 

Epoch 3
-------------------------------
loss: 0.918145  [    0/60000]
loss: 1.091227  [ 6400/60000]
loss: 0.981403  [12800/60000]
loss: 1.214762  [19200/60000]
loss: 1.269803  [25600/60000]
loss: 1.250574  [32000/60000]
loss: 1.333597  [38400/60000]
loss: 1.302414  [44800/60000]
loss: 1.094027  [51200/60000]
loss: 1.351864  [57600/60000]
Test Error: 
 Accuracy: 52.4%, Avg loss: 1.214923 

Epoch 4
-------------------------------
loss: 0.896532  [    0/60000]
loss: 1.088332  [ 6400/60000]
loss: 0.969392  [12800/60000]
loss: 1.188185  [19200/60000]
loss: 1.303220  [25600/60000]
loss: 1.254917  [32000/60000]
loss: 1.318599  [38400/60000]
loss: 1.285251  [44800/60000]
loss: 1.044160  [51200/60000]
loss: 1.312721  [57600/60000]
Test Error: 
 Accuracy: 52.8%, Avg loss: 1.208354 

Epoch 5
-------------------------------
loss: 0.909339  [    0/60000]
loss: 1.079126  [ 6400/60000]
loss: 0.995182  [12800/60000]
loss: 1.190109  [19200/60000]
loss: 1.301627  [25600/60000]
loss: 1.221282  [32000/60000]
loss: 1.307971  [38400/60000]
loss: 1.243801  [44800/60000]
loss: 0.997945  [51200/60000]
loss: 1.373216  [57600/60000]
Test Error: 
 Accuracy: 52.6%, Avg loss: 1.207289 

Epoch 6
-------------------------------
loss: 0.880169  [    0/60000]
loss: 1.094706  [ 6400/60000]
loss: 0.987330  [12800/60000]
loss: 1.165413  [19200/60000]
loss: 1.293738  [25600/60000]
loss: 1.203153  [32000/60000]
loss: 1.308087  [38400/60000]
loss: 1.227905  [44800/60000]
loss: 0.986883  [51200/60000]
loss: 1.339602  [57600/60000]
Test Error: 
 Accuracy: 52.8%, Avg loss: 1.204590 

Epoch 7
-------------------------------
loss: 0.885625  [    0/60000]
loss: 1.036183  [ 6400/60000]
loss: 1.002507  [12800/60000]
loss: 1.151556  [19200/60000]
loss: 1.338916  [25600/60000]
loss: 1.177776  [32000/60000]
loss: 1.332665  [38400/60000]
loss: 1.195178  [44800/60000]
loss: 0.967536  [51200/60000]
loss: 1.318555  [57600/60000]
Test Error: 
 Accuracy: 53.1%, Avg loss: 1.200073 

Epoch 8
-------------------------------
loss: 0.897708  [    0/60000]
loss: 1.025588  [ 6400/60000]
loss: 1.006878  [12800/60000]
loss: 1.173175  [19200/60000]
loss: 1.306540  [25600/60000]
loss: 1.184098  [32000/60000]
loss: 1.277961  [38400/60000]
loss: 1.199482  [44800/60000]
loss: 0.974411  [51200/60000]
loss: 1.253594  [57600/60000]
Test Error: 
 Accuracy: 53.1%, Avg loss: 1.207422 

Epoch 9
-------------------------------
loss: 0.871761  [    0/60000]
loss: 1.016689  [ 6400/60000]
loss: 1.003863  [12800/60000]
loss: 1.146211  [19200/60000]
loss: 1.306011  [25600/60000]
loss: 1.139483  [32000/60000]
loss: 1.330354  [38400/60000]
loss: 1.164872  [44800/60000]
loss: 0.981906  [51200/60000]
loss: 1.269362  [57600/60000]
Test Error: 
 Accuracy: 52.9%, Avg loss: 1.215315 

Backprop time:
0.0017788959210361245
Final  epoch:
9 52.910000000000004 1.215315002165023 tensor(0.6816, device='cuda:0', grad_fn=<NllLossBackward>)
Optimal  epoch:
[6, 53.11, 1.2000734103713067, tensor(0.7346, device='cuda:0', grad_fn=<NllLossBackward>)]
